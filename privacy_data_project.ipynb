{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAx_vARGYbND",
        "outputId": "8b26878a-3e10-4555-ce65-dd776c6bfb44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf51eEItYkDT"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from typing import List, Dict, Tuple, Any, Optional, Union, Callable\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYEj1x7fYkFn"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
        "from transformers import LogitsProcessorList, StoppingCriteriaList, StoppingCriteria\n",
        "from transformers.generation.logits_process import (\n",
        "    LogitsProcessor,\n",
        "    RepetitionPenaltyLogitsProcessor,\n",
        "    TopKLogitsWarper,\n",
        "    TopPLogitsWarper,\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Set, Optional, Union\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc, Span, Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KblkoWvZbGQ"
      },
      "outputs": [],
      "source": [
        "PII_PATTERNS = {\n",
        "    'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "    'phone_us': r'\\b(\\+\\d{1,2}\\s?)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
        "    'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n",
        "    'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',\n",
        "    'ip_address': r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b',\n",
        "    'customer_id': r'\\bCUST-\\d{4}-\\d{4}\\b',\n",
        "    'url': r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[/\\w\\.-]*\\??[-\\w&=%]*',\n",
        "    'date_of_birth': r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b',\n",
        "    'zip_code': r'\\b\\d{5}(?:-\\d{4})?\\b',\n",
        "}\n",
        "\n",
        "# Compiled regex patterns for faster matching\n",
        "COMPILED_PATTERNS = {name: re.compile(pattern) for name, pattern in PII_PATTERNS.items()}\n",
        "\n",
        "# Entity types to detect with NER\n",
        "NER_ENTITY_TYPES = [\n",
        "    'PERSON',\n",
        "    'ORG',\n",
        "    'GPE',  # Geo-political entities (countries, cities)\n",
        "    'LOC',  # Non-GPE locations\n",
        "    'FACILITY',\n",
        "    'PRODUCT',\n",
        "    'EVENT',\n",
        "    'WORK_OF_ART',\n",
        "    'LAW',\n",
        "    'LANGUAGE',\n",
        "    'FAC',  # Buildings, airports\n",
        "    'NORP'  # Nationalities, religious groups\n",
        "]\n",
        "\n",
        "# Placeholder formats\n",
        "def get_placeholder(entity_type, index=None):\n",
        "    \"\"\"Generate a placeholder for the detected entity type.\"\"\"\n",
        "    if index is not None:\n",
        "        return f\"[{entity_type}_{index}]\"\n",
        "    return f\"[{entity_type}]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoBIBcvdZT3R"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PreGenerationFilter:\n",
        "    \"\"\"\n",
        "    Pre-Generation Filter that combines regex-based and NER-based PII detection\n",
        "    with token-level redaction capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spacy_model: str = \"en_core_web_sm\",\n",
        "        use_token_level: bool = True,\n",
        "        preserve_context: bool = True,\n",
        "        redaction_char: str = \"█\",\n",
        "        default_placeholder: str = \"[REDACTED]\",\n",
        "        type_specific_placeholders: bool = True,\n",
        "        min_entity_length: int = 2,\n",
        "        custom_patterns: Dict[str, str] = None,\n",
        "        excluded_entity_types: Set[str] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Pre-Generation Filter.\n",
        "\n",
        "        Args:\n",
        "            spacy_model: The spaCy model to use for NER.\n",
        "            use_token_level: Whether to use token-level redaction (vs. whole entity).\n",
        "            preserve_context: Whether to preserve context around redacted entities.\n",
        "            redaction_char: Character used for inline redaction.\n",
        "            default_placeholder: Default placeholder for redacted entities.\n",
        "            type_specific_placeholders: Whether to use type-specific placeholders.\n",
        "            min_entity_length: Minimum length for entities to be redacted.\n",
        "            custom_patterns: Additional regex patterns to use for detection.\n",
        "            excluded_entity_types: Entity types to exclude from redaction.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(spacy_model)\n",
        "            logger.info(f\"Loaded spaCy model: {spacy_model}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load spaCy model: {str(e)}\")\n",
        "            logger.warning(\"Downloading spaCy model...\")\n",
        "            spacy.cli.download(spacy_model)\n",
        "            self.nlp = spacy.load(spacy_model)\n",
        "\n",
        "        self.use_token_level = use_token_level\n",
        "        self.preserve_context = preserve_context\n",
        "        self.redaction_char = redaction_char\n",
        "        self.default_placeholder = default_placeholder\n",
        "        self.type_specific_placeholders = type_specific_placeholders\n",
        "        self.min_entity_length = min_entity_length\n",
        "        self.excluded_entity_types = excluded_entity_types or set()\n",
        "\n",
        "        # Use default patterns and add any custom ones\n",
        "        self.patterns = dict(COMPILED_PATTERNS)\n",
        "        if custom_patterns:\n",
        "            for name, pattern in custom_patterns.items():\n",
        "                self.patterns[name] = re.compile(pattern)\n",
        "\n",
        "        # Track replacement counts for reporting\n",
        "        self.replacements = {\n",
        "            \"regex\": 0,\n",
        "            \"ner\": 0,\n",
        "            \"total\": 0\n",
        "        }\n",
        "\n",
        "    def reset_counts(self):\n",
        "        \"\"\"Reset the replacement counters.\"\"\"\n",
        "        self.replacements = {key: 0 for key in self.replacements}\n",
        "\n",
        "    def process(self, text: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        Process the input text to detect and redact PII.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to process.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - The processed text with redacted PII\n",
        "            - Statistics about the redaction process\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\", {\"error\": \"Empty text provided\"}\n",
        "\n",
        "        self.reset_counts()\n",
        "\n",
        "        # Process text with spaCy for NER\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Token-level tracking for entities to redact\n",
        "        tokens_to_redact = []\n",
        "        entity_tracking = {}  # Track entity replacements\n",
        "\n",
        "        # First pass: NER-based detection\n",
        "        for ent in doc.ents:\n",
        "            if (ent.label_ in NER_ENTITY_TYPES and\n",
        "                ent.label_ not in self.excluded_entity_types and\n",
        "                len(ent.text) >= self.min_entity_length):\n",
        "\n",
        "                # Generate placeholder based on entity type\n",
        "                entity_type = ent.label_\n",
        "                count = entity_tracking.get(entity_type, 0) + 1\n",
        "                entity_tracking[entity_type] = count\n",
        "\n",
        "                if self.type_specific_placeholders:\n",
        "                    placeholder = get_placeholder(entity_type, count)\n",
        "                else:\n",
        "                    placeholder = self.default_placeholder\n",
        "\n",
        "                if self.use_token_level:\n",
        "                    # Mark all tokens in this entity for redaction\n",
        "                    for token in ent:\n",
        "                        tokens_to_redact.append((token.i, token.text, placeholder))\n",
        "                else:\n",
        "                    # Replace the whole entity span\n",
        "                    tokens_to_redact.extend([(token.i, token.text, placeholder) for token in ent])\n",
        "\n",
        "                self.replacements[\"ner\"] += 1\n",
        "\n",
        "        # Second pass: regex-based detection for structured PII\n",
        "        for token in doc:\n",
        "            # Skip tokens already marked for redaction\n",
        "            if any(token.i == t[0] for t in tokens_to_redact):\n",
        "                continue\n",
        "\n",
        "            for pattern_name, pattern in self.patterns.items():\n",
        "                if pattern.search(token.text):\n",
        "                    placeholder = get_placeholder(pattern_name.upper()) if self.type_specific_placeholders else self.default_placeholder\n",
        "                    tokens_to_redact.append((token.i, token.text, placeholder))\n",
        "                    self.replacements[\"regex\"] += 1\n",
        "                    break  # Once a token is matched by one pattern, no need to check others\n",
        "\n",
        "        # Apply redactions\n",
        "        result = self._apply_token_redactions(doc, tokens_to_redact)\n",
        "\n",
        "        # Update total replacements count\n",
        "        self.replacements[\"total\"] = self.replacements[\"regex\"] + self.replacements[\"ner\"]\n",
        "\n",
        "        stats = {\n",
        "            \"input_length\": len(text),\n",
        "            \"output_length\": len(result),\n",
        "            \"replacements\": dict(self.replacements),\n",
        "            \"entity_types\": list(entity_tracking.keys())\n",
        "        }\n",
        "\n",
        "        return result, stats\n",
        "\n",
        "    def _apply_token_redactions(self, doc: Doc, tokens_to_redact: List[Tuple[int, str, str]]) -> str:\n",
        "        \"\"\"\n",
        "        Apply token-level redactions to the document.\n",
        "\n",
        "        Args:\n",
        "            doc: The spaCy Doc object.\n",
        "            tokens_to_redact: List of (token_idx, original_text, placeholder) tuples.\n",
        "\n",
        "        Returns:\n",
        "            The redacted text.\n",
        "        \"\"\"\n",
        "        if not tokens_to_redact:\n",
        "            return doc.text\n",
        "\n",
        "        # Sort by token index to process in order\n",
        "        tokens_to_redact.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Group consecutive tokens with the same placeholder\n",
        "        grouped_tokens = []\n",
        "        current_group = [tokens_to_redact[0]]\n",
        "\n",
        "        for i in range(1, len(tokens_to_redact)):\n",
        "            prev_token = tokens_to_redact[i-1]\n",
        "            curr_token = tokens_to_redact[i]\n",
        "\n",
        "            # If consecutive and same placeholder, add to group\n",
        "            if curr_token[0] == prev_token[0] + 1 and curr_token[2] == prev_token[2]:\n",
        "                current_group.append(curr_token)\n",
        "            else:\n",
        "                grouped_tokens.append(current_group)\n",
        "                current_group = [curr_token]\n",
        "\n",
        "        # Add the last group\n",
        "        if current_group:\n",
        "            grouped_tokens.append(current_group)\n",
        "\n",
        "        # Create the redacted text\n",
        "        result = []\n",
        "        last_idx = 0\n",
        "\n",
        "        for group in grouped_tokens:\n",
        "            start_token_idx = group[0][0]\n",
        "            end_token_idx = group[-1][0]\n",
        "            placeholder = group[0][2]\n",
        "\n",
        "            # Add text before this redaction\n",
        "            if start_token_idx > 0 and last_idx < start_token_idx:\n",
        "                result.append(doc[last_idx:start_token_idx].text_with_ws.rstrip())\n",
        "\n",
        "            # Add the placeholder\n",
        "            result.append(placeholder)\n",
        "\n",
        "            # Update last index\n",
        "            last_idx = end_token_idx + 1\n",
        "\n",
        "        # Add any remaining text\n",
        "        if last_idx < len(doc):\n",
        "            result.append(doc[last_idx:].text)\n",
        "\n",
        "        return \" \".join(result)\n",
        "\n",
        "    def redact_entities(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Simple interface to redact PII from text.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to redact.\n",
        "\n",
        "        Returns:\n",
        "            The redacted text.\n",
        "        \"\"\"\n",
        "        redacted_text, _ = self.process(text)\n",
        "        return redacted_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8Nz5wGa9Ahx",
        "outputId": "931223d5-870f-4aa7-975f-56b2eb495e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElHq7qSSYkIB"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BlockedWordsLogitsProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    LogitsProcessor that blocks specific tokens or sequences from being generated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, blocked_words: List[str], min_probability: float = -100.0):\n",
        "        \"\"\"\n",
        "        Initialize the processor.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: The tokenizer used by the model.\n",
        "            blocked_words: List of words or phrases to block.\n",
        "            min_probability: Log probability assigned to blocked tokens (default: -100.0).\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.min_probability = min_probability\n",
        "        self.blocked_token_ids = []\n",
        "\n",
        "        # Convert words to token ids\n",
        "        for word in blocked_words:\n",
        "            token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            if token_ids:  # Ensure we have valid token IDs\n",
        "                self.blocked_token_ids.append(token_ids)\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Process logits to block specific tokens.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs already generated.\n",
        "            scores: Current token scores/logits.\n",
        "\n",
        "        Returns:\n",
        "            Modified scores with blocked tokens penalized.\n",
        "        \"\"\"\n",
        "        # For single token blocks, directly modify the scores\n",
        "        for token_ids in self.blocked_token_ids:\n",
        "            if len(token_ids) == 1:\n",
        "                scores[:, token_ids[0]] = self.min_probability\n",
        "\n",
        "        # For multi-token sequences, check if we're at the start of a blocked sequence\n",
        "        batch_size = input_ids.shape[0]\n",
        "        for b in range(batch_size):\n",
        "            for token_seq in self.blocked_token_ids:\n",
        "                if len(token_seq) > 1:\n",
        "                    # Get the current sequence length to compare\n",
        "                    seq_len = min(len(token_seq) - 1, input_ids.shape[1])\n",
        "\n",
        "                    # Check if the last tokens match the beginning of the blocked sequence\n",
        "                    if seq_len > 0 and input_ids[b, -seq_len:].tolist() == token_seq[:seq_len]:\n",
        "                        # If it matches, block the next token in the sequence\n",
        "                        next_token = token_seq[seq_len]\n",
        "                        scores[b, next_token] = self.min_probability\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "class GemmaTextGenerator:\n",
        "    \"\"\"\n",
        "    Text generation with Gemma 2B model with privacy-focused generation controls.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"google/gemma-2b-it\",\n",
        "        max_new_tokens: int = 512,\n",
        "        temperature: float = 0.8,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.95,\n",
        "        repetition_penalty: float = 1.2,\n",
        "        blocked_words: List[str] = None,\n",
        "        cache_dir: str = None,\n",
        "        load_in_8bit: bool = False,\n",
        "        hf_auth_token: str = \"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\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Gemma text generator with privacy controls.\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model identifier.\n",
        "            device: Device to use (cuda, cpu, mps). If None, will auto-detect.\n",
        "            max_new_tokens: Maximum number of new tokens to generate.\n",
        "            temperature: Sampling temperature. Higher = more random.\n",
        "            top_k: Keep only top k tokens with highest probability.\n",
        "            top_p: Keep tokens with cumulative probability >= top_p.\n",
        "            repetition_penalty: Penalty for repeating tokens. Higher = less repetition.\n",
        "            blocked_words: List of words or phrases to block from generation.\n",
        "            cache_dir: Directory for caching models.\n",
        "            load_in_8bit: Whether to load the model in 8-bit precision.\n",
        "            hf_auth_token: HuggingFace API token for accessing gated models.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "        self.temperature = temperature\n",
        "        self.top_k = top_k\n",
        "        self.top_p = top_p\n",
        "        self.repetition_penalty = repetition_penalty\n",
        "        self.blocked_words = blocked_words or []\n",
        "\n",
        "        # Set HF_AUTH_TOKEN if provided\n",
        "        if hf_auth_token:\n",
        "            os.environ[\"HF_TOKEN\"] = hf_auth_token\n",
        "            logger.info(\"Using provided HuggingFace auth token\")\n",
        "\n",
        "        # Determine device1\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            cache_dir=cache_dir,\n",
        "            use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
        "        )\n",
        "\n",
        "        # Set padding side to left for better handling of context sizes\n",
        "        self.tokenizer.padding_side = \"left\"\n",
        "\n",
        "        # Load model\n",
        "        model_kwargs = {\n",
        "            \"cache_dir\": cache_dir,\n",
        "            \"use_auth_token\": os.environ.get(\"HF_TOKEN\")\n",
        "        }\n",
        "\n",
        "        if load_in_8bit:\n",
        "            logger.info(\"Loading model in 8-bit precision\")\n",
        "            model_kwargs[\"load_in_8bit\"] = True\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else None,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None,\n",
        "            **model_kwargs\n",
        "        )\n",
        "\n",
        "        if self.device != \"cuda\":\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        logger.info(f\"Loaded model: {model_name}\")\n",
        "        accelerator = Accelerator()\n",
        "\n",
        "        # Create pipeline for easy use\n",
        "        self.pipeline = TextGenerationPipeline(model=self.model, tokenizer=self.tokenizer)\n",
        "\n",
        "        # Initialize logits processors list\n",
        "        self.custom_logits_processors = []\n",
        "\n",
        "        if self.blocked_words:\n",
        "            self.custom_logits_processors.append(\n",
        "                BlockedWordsLogitsProcessor(self.tokenizer, self.blocked_words)\n",
        "            )\n",
        "\n",
        "    def _get_generation_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get the generation configuration based on current settings.\n",
        "\n",
        "        Returns:\n",
        "            Dict containing generation parameters.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            \"max_new_tokens\": self.max_new_tokens,\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"repetition_penalty\": self.repetition_penalty,\n",
        "            \"pad_token_id\": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "        return config\n",
        "\n",
        "    def generate(self, prompt: str, **kwargs) -> Tuple[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Generate text based on the prompt with privacy controls applied.\n",
        "\n",
        "        Args:\n",
        "            prompt: The input prompt.\n",
        "            **kwargs: Additional generation parameters to override defaults.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - The generated text\n",
        "            - Dict with metadata about the generation process\n",
        "        \"\"\"\n",
        "        # Get base generation config and update with any overrides\n",
        "        gen_config = self._get_generation_config()\n",
        "        gen_config.update(kwargs)\n",
        "\n",
        "        # Build logits processors list\n",
        "        logits_processors = LogitsProcessorList(self.custom_logits_processors)\n",
        "\n",
        "        if gen_config.get(\"repetition_penalty\", 1.0) > 1.0:\n",
        "            logits_processors.append(\n",
        "                RepetitionPenaltyLogitsProcessor(gen_config[\"repetition_penalty\"])\n",
        "            )\n",
        "\n",
        "        if gen_config.get(\"top_k\", 0) > 0:\n",
        "            logits_processors.append(\n",
        "                TopKLogitsWarper(gen_config[\"top_k\"])\n",
        "            )\n",
        "\n",
        "        if gen_config.get(\"top_p\", 1.0) < 1.0:\n",
        "            logits_processors.append(\n",
        "                TopPLogitsWarper(gen_config[\"top_p\"])\n",
        "            )\n",
        "\n",
        "        # Start generation timer\n",
        "        start_time = torch.cuda.Event(enable_timing=True) if self.device == \"cuda\" else None\n",
        "        end_time = torch.cuda.Event(enable_timing=True) if self.device == \"cuda\" else None\n",
        "\n",
        "        if start_time:\n",
        "            start_time.record()\n",
        "\n",
        "        # Encode the prompt\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        # Generate text with custom processors\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                gen_config[\"logits_processor\"] = logits_processors\n",
        "\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    **gen_config\n",
        "                )\n",
        "\n",
        "            # Extract only the newly generated tokens\n",
        "            generated_tokens = outputs[0, input_length:]\n",
        "            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "            if end_time:\n",
        "                end_time.record()\n",
        "                torch.cuda.synchronize()\n",
        "                gen_time_ms = start_time.elapsed_time(end_time)\n",
        "            else:\n",
        "                gen_time_ms = None\n",
        "\n",
        "            # Prepare metadata\n",
        "            metadata = {\n",
        "                \"model\": self.model_name,\n",
        "                \"input_tokens\": input_length,\n",
        "                \"generated_tokens\": len(generated_tokens),\n",
        "                \"generation_time_ms\": gen_time_ms,\n",
        "                \"generation_config\": {k: v for k, v in gen_config.items() if k != \"logits_processor\"}\n",
        "            }\n",
        "\n",
        "            return generated_text, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Generation error: {str(e)}\")\n",
        "            return f\"Error generating text: {str(e)}\", {\"error\": str(e)}\n",
        "\n",
        "    def update_blocked_words(self, blocked_words: List[str]):\n",
        "        \"\"\"\n",
        "        Update the list of blocked words.\n",
        "\n",
        "        Args:\n",
        "            blocked_words: New list of words to block.\n",
        "        \"\"\"\n",
        "        self.blocked_words = blocked_words\n",
        "\n",
        "        # Rebuild logits processors\n",
        "        self.custom_logits_processors = []\n",
        "\n",
        "        if self.blocked_words:\n",
        "            self.custom_logits_processors.append(\n",
        "                BlockedWordsLogitsProcessor(self.tokenizer, self.blocked_words)\n",
        "            )\n",
        "\n",
        "    def update_generation_params(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Update generation parameters.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Parameters to update (temperature, top_k, top_p, etc).\n",
        "        \"\"\"\n",
        "        valid_params = [\n",
        "            \"max_new_tokens\", \"temperature\", \"top_k\", \"top_p\", \"repetition_penalty\"\n",
        "        ]\n",
        "\n",
        "        for param, value in kwargs.items():\n",
        "            if param in valid_params:\n",
        "                setattr(self, param, value)\n",
        "                logger.info(f\"Updated {param} to {value}\")\n",
        "            else:\n",
        "                logger.warning(f\"Ignored invalid parameter: {param}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_konsj_YkKd"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PostGenerationFilter:\n",
        "    \"\"\"\n",
        "    Post-Generation Filter that uses similarity-based detection, with adaptive thresholds\n",
        "    and optional re-application of PII detection from the Pre-Generation filter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sensitive_items: List[str] = None,\n",
        "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        base_threshold: float = 0.92,\n",
        "        adaptive_thresholds: Dict[str, float] = None,\n",
        "        content_type_classifier: Optional[Callable[[str], str]] = None,\n",
        "        reapply_pii_detection: bool = True,\n",
        "        pii_filter: Optional[PreGenerationFilter] = None,\n",
        "        replacement_text: str = \"[REDACTED_CONTENT_HIGH_SIMILARITY]\",\n",
        "        use_token_level_redaction: bool = True,\n",
        "        batch_size: int = 32,\n",
        "        device: str = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Post-Generation Filter.\n",
        "\n",
        "        Args:\n",
        "            sensitive_items: List of sensitive text items to check against.\n",
        "            embedding_model: SentenceTransformer model to use for embeddings.\n",
        "            base_threshold: Base similarity threshold (0.0-1.0).\n",
        "            adaptive_thresholds: Dict mapping content types to thresholds.\n",
        "            content_type_classifier: Function that classifies text into content types.\n",
        "            reapply_pii_detection: Whether to reapply PII detection on outputs.\n",
        "            pii_filter: Pre-configured PreGenerationFilter instance to use.\n",
        "            replacement_text: Text to use for replacing highly similar content.\n",
        "            use_token_level_redaction: Use token-level redaction for PII.\n",
        "            batch_size: Batch size for computing embeddings.\n",
        "            device: Device to use for embeddings (cuda, cpu, mps).\n",
        "        \"\"\"\n",
        "        # Initialize sensitive items list\n",
        "        self.sensitive_items = sensitive_items or []\n",
        "        self.sensitive_embeddings = None\n",
        "\n",
        "        # Similarity settings\n",
        "        self.base_threshold = base_threshold\n",
        "        self.adaptive_thresholds = adaptive_thresholds or {\n",
        "            \"default\": base_threshold,\n",
        "            \"technical\": 0.95,  # Higher threshold for technical content\n",
        "            \"general\": 0.9,     # Lower threshold for general content\n",
        "            \"personal\": 0.85    # Even lower threshold for personal info\n",
        "        }\n",
        "\n",
        "        # Content classification function\n",
        "        self.content_type_classifier = content_type_classifier or self._simple_classifier\n",
        "\n",
        "        # Text replacement settings\n",
        "        self.replacement_text = replacement_text\n",
        "        self.use_token_level_redaction = use_token_level_redaction\n",
        "\n",
        "        # PII detection\n",
        "        self.reapply_pii_detection = reapply_pii_detection\n",
        "        self.pii_filter = pii_filter or PreGenerationFilter(\n",
        "            use_token_level=use_token_level_redaction,\n",
        "            type_specific_placeholders=True\n",
        "        )\n",
        "\n",
        "        # Load embedding model\n",
        "        try:\n",
        "            logger.info(f\"Loading embedding model: {embedding_model}\")\n",
        "            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
        "            self.batch_size = batch_size\n",
        "            self._precompute_embeddings()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load embedding model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Tracking stats\n",
        "        self.stats = {\n",
        "            \"total_processed\": 0,\n",
        "            \"similarity_redactions\": 0,\n",
        "            \"pii_redactions\": 0,\n",
        "            \"content_types\": {}\n",
        "        }\n",
        "\n",
        "    def reset_stats(self):\n",
        "        \"\"\"Reset the filtering statistics.\"\"\"\n",
        "        self.stats = {\n",
        "            \"total_processed\": 0,\n",
        "            \"similarity_redactions\": 0,\n",
        "            \"pii_redactions\": 0,\n",
        "            \"content_types\": {}\n",
        "        }\n",
        "\n",
        "    def _precompute_embeddings(self):\n",
        "        \"\"\"Precompute embeddings for sensitive items.\"\"\"\n",
        "        if not self.sensitive_items:\n",
        "            self.sensitive_embeddings = None\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Precomputing embeddings for {len(self.sensitive_items)} sensitive items\")\n",
        "        self.sensitive_embeddings = self.embedding_model.encode(\n",
        "            self.sensitive_items,\n",
        "            batch_size=self.batch_size,\n",
        "            show_progress_bar=len(self.sensitive_items) > 10\n",
        "        )\n",
        "\n",
        "    def _simple_classifier(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Simple rule-based content type classifier.\n",
        "\n",
        "        Args:\n",
        "            text: Text to classify.\n",
        "\n",
        "        Returns:\n",
        "            Content type classification (technical, personal, general).\n",
        "        \"\"\"\n",
        "        # Check for patterns indicating personal information\n",
        "        personal_indicators = [\n",
        "            r'\\b(?:I|my|me|mine|myself)\\b',\n",
        "            r'\\b(?:you|your|yours|yourself)\\b',\n",
        "            r'\\b(?:he|him|his|himself|she|her|hers|herself)\\b',\n",
        "            r'\\b(?:we|us|our|ours|ourselves)\\b',\n",
        "            r'\\b(?:they|them|their|theirs|themselves)\\b',\n",
        "            r'\\b(?:address|phone|email|contact|birth|birthday|age|gender|identity|family)\\b'\n",
        "        ]\n",
        "\n",
        "        personal_count = sum(1 for pattern in personal_indicators if re.search(pattern, text, re.IGNORECASE))\n",
        "\n",
        "        # Check for patterns indicating technical content\n",
        "        technical_indicators = [\n",
        "            r'\\b(?:code|function|algorithm|data|technical|system|software|hardware|server|database|API|protocol)\\b',\n",
        "            r'\\b(?:network|security|cybersecurity|encryption|programming|developer|architecture|framework)\\b',\n",
        "            r'\\b(?:implementation|design|pattern|interface|class|object|procedure|method|structure)\\b'\n",
        "        ]\n",
        "\n",
        "        technical_count = sum(1 for pattern in technical_indicators if re.search(pattern, text, re.IGNORECASE))\n",
        "\n",
        "        # Determine content type based on matches\n",
        "        if personal_count > technical_count and personal_count > 2:\n",
        "            return \"personal\"\n",
        "        elif technical_count > personal_count and technical_count > 2:\n",
        "            return \"technical\"\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def get_threshold_for_content(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Get the appropriate similarity threshold for the given content.\n",
        "\n",
        "        Args:\n",
        "            text: Text to get threshold for.\n",
        "\n",
        "        Returns:\n",
        "            Similarity threshold value.\n",
        "        \"\"\"\n",
        "        content_type = self.content_type_classifier(text)\n",
        "\n",
        "        # Update stats\n",
        "        self.stats[\"content_types\"][content_type] = self.stats[\"content_types\"].get(content_type, 0) + 1\n",
        "\n",
        "        # Get threshold for this content type\n",
        "        threshold = self.adaptive_thresholds.get(content_type, self.base_threshold)\n",
        "\n",
        "        return threshold\n",
        "\n",
        "    def add_sensitive_items(self, items: List[str]):\n",
        "        \"\"\"\n",
        "        Add new items to the sensitive items list.\n",
        "\n",
        "        Args:\n",
        "            items: List of sensitive text items to add.\n",
        "        \"\"\"\n",
        "        if not items:\n",
        "            return\n",
        "\n",
        "        # Add items\n",
        "        self.sensitive_items.extend(items)\n",
        "\n",
        "        # Recompute embeddings\n",
        "        self._precompute_embeddings()\n",
        "\n",
        "    def check_similarity(self, text: str, threshold: Optional[float] = None) -> Tuple[float, int]:\n",
        "        \"\"\"\n",
        "        Check the similarity of the text against sensitive items.\n",
        "\n",
        "        Args:\n",
        "            text: Text to check.\n",
        "            threshold: Optional override threshold. If None, uses content-dependent threshold.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - Maximum similarity score (float)\n",
        "            - Index of the most similar item (-1 if no items)\n",
        "        \"\"\"\n",
        "        if not self.sensitive_items or not self.sensitive_embeddings is not None:\n",
        "            return 0.0, -1\n",
        "\n",
        "        # Get text embedding\n",
        "        text_embedding = self.embedding_model.encode(text, batch_size=1)\n",
        "\n",
        "        # Reshape for similarity calculation\n",
        "        text_embedding = text_embedding.reshape(1, -1)\n",
        "\n",
        "        # Calculate similarity to all sensitive items\n",
        "        similarities = cosine_similarity(text_embedding, self.sensitive_embeddings)[0]\n",
        "\n",
        "        # Get max similarity and index\n",
        "        max_similarity = similarities.max()\n",
        "        max_index = similarities.argmax()\n",
        "\n",
        "        return float(max_similarity), int(max_index)\n",
        "\n",
        "    def process(self, text: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        Process the generated text to detect and redact sensitive content.\n",
        "\n",
        "        Args:\n",
        "            text: The text to process.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - The processed text\n",
        "            - Statistics about the processing\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\", {\"error\": \"Empty text provided\"}\n",
        "\n",
        "        self.stats[\"total_processed\"] += 1\n",
        "\n",
        "        # Get content-dependent threshold\n",
        "        threshold = self.get_threshold_for_content(text)\n",
        "\n",
        "        # Check similarity against sensitive items\n",
        "        max_similarity, max_index = self.check_similarity(text, threshold)\n",
        "\n",
        "        result_stats = {\n",
        "            \"content_type\": self.content_type_classifier(text),\n",
        "            \"threshold\": threshold,\n",
        "            \"max_similarity\": max_similarity,\n",
        "            \"similarity_redacted\": False,\n",
        "            \"pii_redacted\": False\n",
        "        }\n",
        "\n",
        "        # If similarity exceeds threshold, redact the entire text\n",
        "        if max_similarity >= threshold and max_index >= 0:\n",
        "            result = self.replacement_text\n",
        "            result_stats[\"similarity_redacted\"] = True\n",
        "            self.stats[\"similarity_redactions\"] += 1\n",
        "            return result, result_stats\n",
        "\n",
        "        # If similarity check passes, optionally apply PII detection\n",
        "        if self.reapply_pii_detection:\n",
        "            redacted_text, pii_stats = self.pii_filter.process(text)\n",
        "\n",
        "            # If PII was found, update result and stats\n",
        "            if pii_stats[\"replacements\"][\"total\"] > 0:\n",
        "                result = redacted_text\n",
        "                result_stats[\"pii_redacted\"] = True\n",
        "                result_stats[\"pii_stats\"] = pii_stats\n",
        "                self.stats[\"pii_redactions\"] += 1\n",
        "                return result, result_stats\n",
        "\n",
        "        # If all checks pass, return the original text\n",
        "        return text, result_stats\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Get current filtering statistics.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with filtering statistics.\n",
        "        \"\"\"\n",
        "        if self.stats[\"total_processed\"] > 0:\n",
        "            similarity_rate = self.stats[\"similarity_redactions\"] / self.stats[\"total_processed\"]\n",
        "            pii_rate = self.stats[\"pii_redactions\"] / self.stats[\"total_processed\"]\n",
        "            safe_rate = 1.0 - (similarity_rate + pii_rate)\n",
        "        else:\n",
        "            similarity_rate = 0.0\n",
        "            pii_rate = 0.0\n",
        "            safe_rate = 0.0\n",
        "\n",
        "        return {\n",
        "            **self.stats,\n",
        "            \"similarity_redaction_rate\": similarity_rate,\n",
        "            \"pii_redaction_rate\": pii_rate,\n",
        "            \"safe_rate\": safe_rate\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPYF9BttYkR3"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PrivacyFilterPipeline:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline for synthetic data generation with privacy filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pre_filter_config: Optional[Dict[str, Any]] = None,\n",
        "        generator_config: Optional[Dict[str, Any]] = None,\n",
        "        post_filter_config: Optional[Dict[str, Any]] = None,\n",
        "        model_name: str = \"google/gemma-2b-it\",\n",
        "        sensitive_items: List[str] = None,\n",
        "        device: str = None,\n",
        "        hf_auth_token: str = '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'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the privacy filter pipeline.\n",
        "\n",
        "        Args:\n",
        "            pre_filter_config: Configuration for the pre-generation filter.\n",
        "            generator_config: Configuration for the text generator.\n",
        "            post_filter_config: Configuration for the post-generation filter.\n",
        "            model_name: Name of the model to use for text generation.\n",
        "            sensitive_items: List of sensitive items to check against.\n",
        "            device: Device to use for models (cuda, cpu, mps).\n",
        "            hf_auth_token: HuggingFace API token.\n",
        "        \"\"\"\n",
        "        # Set default configurations\n",
        "        pre_filter_config = pre_filter_config or {\n",
        "            \"use_token_level\": True,\n",
        "            \"type_specific_placeholders\": True\n",
        "        }\n",
        "\n",
        "        generator_config = generator_config or {\n",
        "            \"temperature\": 0.8,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.95,\n",
        "            \"repetition_penalty\": 1.2,\n",
        "            \"max_new_tokens\": 256,\n",
        "            \"blocked_words\": [\"password\", \"social security\", \"credit card\", \"address\", \"phone number\", \"email\"]\n",
        "        }\n",
        "\n",
        "        post_filter_config = post_filter_config or {\n",
        "            \"base_threshold\": 0.92,\n",
        "            \"adaptive_thresholds\": {\n",
        "                \"technical\": 0.95,\n",
        "                \"general\": 0.9,\n",
        "                \"personal\": 0.85\n",
        "            },\n",
        "            \"reapply_pii_detection\": True,\n",
        "            \"use_token_level_redaction\": True\n",
        "        }\n",
        "\n",
        "        # Initialize components\n",
        "        logger.info(\"Initializing Pre-Generation Filter...\")\n",
        "        self.pre_filter = PreGenerationFilter(**pre_filter_config)\n",
        "\n",
        "        logger.info(f\"Initializing Text Generator with model: {model_name}...\")\n",
        "        # Set HuggingFace token if provided\n",
        "        if hf_auth_token:\n",
        "            os.environ[\"HF_TOKEN\"] = hf_auth_token\n",
        "\n",
        "        # Extract generator-specific configs\n",
        "        generator_kwargs = dict(generator_config)\n",
        "        self.generator = GemmaTextGenerator(\n",
        "            model_name=model_name,\n",
        "            device=device,\n",
        "            hf_auth_token=hf_auth_token,\n",
        "            **generator_kwargs\n",
        "        )\n",
        "\n",
        "        logger.info(\"Initializing Post-Generation Filter...\")\n",
        "        # Add sensitive items to post filter config\n",
        "        if sensitive_items:\n",
        "            post_filter_config[\"sensitive_items\"] = sensitive_items\n",
        "        self.post_filter = PostGenerationFilter(**post_filter_config)\n",
        "\n",
        "        # Pipeline configuration\n",
        "        self.config = {\n",
        "            \"pre_filter\": pre_filter_config,\n",
        "            \"generator\": generator_config,\n",
        "            \"post_filter\": post_filter_config,\n",
        "            \"model_name\": model_name\n",
        "        }\n",
        "\n",
        "        # Pipeline stats\n",
        "        self.stats = {\n",
        "            \"total_processed\": 0,\n",
        "            \"pre_filter_applied\": 0,\n",
        "            \"post_filter_applied\": 0,\n",
        "            \"total_replacements\": 0\n",
        "        }\n",
        "\n",
        "        logger.info(\"Pipeline initialized successfully\")\n",
        "\n",
        "    def reset_stats(self):\n",
        "        \"\"\"Reset the pipeline statistics.\"\"\"\n",
        "        self.stats = {\n",
        "            \"total_processed\": 0,\n",
        "            \"pre_filter_applied\": 0,\n",
        "            \"post_filter_applied\": 0,\n",
        "            \"total_replacements\": 0\n",
        "        }\n",
        "\n",
        "    def process(\n",
        "        self,\n",
        "        text: str,\n",
        "        prompt_template: str = None,\n",
        "        max_new_tokens: int = None,\n",
        "        return_all_stages: bool = False\n",
        "    ) -> Union[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process a text through the entire pipeline.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to process.\n",
        "            prompt_template: Template for the generation prompt. Use {text} as placeholder.\n",
        "            max_new_tokens: Override for max tokens to generate.\n",
        "            return_all_stages: Whether to return outputs from all pipeline stages.\n",
        "\n",
        "        Returns:\n",
        "            If return_all_stages is False: The final processed text.\n",
        "            If return_all_stages is True: Dict with outputs from all stages.\n",
        "        \"\"\"\n",
        "        self.stats[\"total_processed\"] += 1\n",
        "\n",
        "        # 1. Apply pre-generation filter\n",
        "        logger.info(\"Applying pre-generation filter...\")\n",
        "        pre_filtered_text, pre_stats = self.pre_filter.process(text)\n",
        "\n",
        "        if pre_stats[\"replacements\"][\"total\"] > 0:\n",
        "            self.stats[\"pre_filter_applied\"] += 1\n",
        "            self.stats[\"total_replacements\"] += pre_stats[\"replacements\"][\"total\"]\n",
        "\n",
        "        # 2. Prepare prompt for generation\n",
        "        if prompt_template is None:\n",
        "            prompt_template = \"Based on the following text, generate a variation that preserves the meaning but uses different wording:\\n\\n{text}\"\n",
        "\n",
        "        prompt = prompt_template.format(text=pre_filtered_text)\n",
        "\n",
        "        # 3. Generate text\n",
        "        logger.info(\"Generating text...\")\n",
        "        gen_kwargs = {}\n",
        "        if max_new_tokens is not None:\n",
        "            gen_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "\n",
        "        generated_text, gen_stats = self.generator.generate(prompt, **gen_kwargs)\n",
        "\n",
        "        # 4. Apply post-generation filter\n",
        "        logger.info(\"Applying post-generation filter...\")\n",
        "        post_filtered_text, post_stats = self.post_filter.process(generated_text)\n",
        "\n",
        "        if post_stats[\"similarity_redacted\"] or post_stats[\"pii_redacted\"]:\n",
        "            self.stats[\"post_filter_applied\"] += 1\n",
        "\n",
        "        # 5. Return result\n",
        "        if return_all_stages:\n",
        "            return {\n",
        "                \"original_text\": text,\n",
        "                \"pre_filtered_text\": pre_filtered_text,\n",
        "                \"prompt\": prompt,\n",
        "                \"generated_text\": generated_text,\n",
        "                \"final_text\": post_filtered_text,\n",
        "                \"stats\": {\n",
        "                    \"pre_filter\": pre_stats,\n",
        "                    \"generation\": gen_stats,\n",
        "                    \"post_filter\": post_stats,\n",
        "                    \"pipeline\": dict(self.stats)\n",
        "                }\n",
        "            }\n",
        "        else:\n",
        "            return post_filtered_text\n",
        "\n",
        "    def process_batch(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        prompt_template: str = None,\n",
        "        max_new_tokens: int = None,\n",
        "        return_all_stages: bool = False,\n",
        "        progress_callback: callable = None\n",
        "    ) -> Union[List[str], List[Dict[str, Any]]]:\n",
        "        \"\"\"\n",
        "        Process a batch of texts through the pipeline.\n",
        "\n",
        "        Args:\n",
        "            texts: List of input texts to process.\n",
        "            prompt_template: Template for the generation prompt. Use {text} as placeholder.\n",
        "            max_new_tokens: Override for max tokens to generate.\n",
        "            return_all_stages: Whether to return outputs from all pipeline stages.\n",
        "            progress_callback: Optional callback function to report progress.\n",
        "\n",
        "        Returns:\n",
        "            List of processed outputs (strings or dicts depending on return_all_stages).\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        total = len(texts)\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            # Report progress if callback provided\n",
        "            if progress_callback and callable(progress_callback):\n",
        "                progress_callback(i, total)\n",
        "\n",
        "            # Process text\n",
        "            result = self.process(\n",
        "                text=text,\n",
        "                prompt_template=prompt_template,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                return_all_stages=return_all_stages\n",
        "            )\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "            # Log progress periodically\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == total:\n",
        "                logger.info(f\"Processed {i + 1}/{total} texts\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_dataframe(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        text_column: str,\n",
        "        output_column: str = \"processed_text\",\n",
        "        all_stages_column: str = None,\n",
        "        prompt_template: str = None,\n",
        "        max_new_tokens: int = None,\n",
        "        inplace: bool = False,\n",
        "        progress_callback: callable = None\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Process texts in a DataFrame through the pipeline.\n",
        "\n",
        "        Args:\n",
        "            df: Input DataFrame.\n",
        "            text_column: Column containing input texts.\n",
        "            output_column: Column to store final outputs.\n",
        "            all_stages_column: Column to store all stage outputs (if desired).\n",
        "            prompt_template: Template for the generation prompt. Use {text} as placeholder.\n",
        "            max_new_tokens: Override for max tokens to generate.\n",
        "            inplace: Whether to modify the input DataFrame in-place.\n",
        "            progress_callback: Optional callback function to report progress.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with processed results.\n",
        "        \"\"\"\n",
        "        if text_column not in df.columns:\n",
        "            raise ValueError(f\"Text column '{text_column}' not found in DataFrame\")\n",
        "\n",
        "        # Get texts to process\n",
        "        texts = df[text_column].tolist()\n",
        "\n",
        "        # Determine if we need to return all stages\n",
        "        return_all_stages = all_stages_column is not None\n",
        "\n",
        "        # Process texts\n",
        "        results = self.process_batch(\n",
        "            texts=texts,\n",
        "            prompt_template=prompt_template,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            return_all_stages=return_all_stages,\n",
        "            progress_callback=progress_callback\n",
        "        )\n",
        "\n",
        "        # Create output DataFrame\n",
        "        if not inplace:\n",
        "            output_df = df.copy()\n",
        "        else:\n",
        "            output_df = df\n",
        "\n",
        "        # Add results to DataFrame\n",
        "        if return_all_stages:\n",
        "            # Extract final texts for the output column\n",
        "            output_df[output_column] = [r[\"final_text\"] for r in results]\n",
        "            # Store full results in the all stages column\n",
        "            output_df[all_stages_column] = results\n",
        "        else:\n",
        "            output_df[output_column] = results\n",
        "\n",
        "        return output_df\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get the pipeline configuration.\"\"\"\n",
        "        return self.config\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get the pipeline statistics.\"\"\"\n",
        "        return self.stats\n",
        "\n",
        "    def update_pre_filter_config(self, **kwargs):\n",
        "        \"\"\"Update the pre-generation filter configuration.\"\"\"\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(self.pre_filter, key):\n",
        "                setattr(self.pre_filter, key, value)\n",
        "                self.config[\"pre_filter\"][key] = value\n",
        "                logger.info(f\"Updated pre-filter parameter: {key} = {value}\")\n",
        "\n",
        "    def update_generator_config(self, **kwargs):\n",
        "        \"\"\"Update the text generator configuration.\"\"\"\n",
        "        for key, value in kwargs.items():\n",
        "            if key in [\"temperature\", \"top_k\", \"top_p\", \"repetition_penalty\", \"max_new_tokens\"]:\n",
        "                # These are parameters that can be updated directly\n",
        "                self.generator.update_generation_params(**{key: value})\n",
        "                self.config[\"generator\"][key] = value\n",
        "                logger.info(f\"Updated generator parameter: {key} = {value}\")\n",
        "            elif key == \"blocked_words\":\n",
        "                # Update blocked words list\n",
        "                self.generator.update_blocked_words(value)\n",
        "                self.config[\"generator\"][key] = value\n",
        "                logger.info(f\"Updated blocked words list with {len(value)} words\")\n",
        "\n",
        "    def update_post_filter_config(self, **kwargs):\n",
        "        \"\"\"Update the post-generation filter configuration.\"\"\"\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(self.post_filter, key):\n",
        "                setattr(self.post_filter, key, value)\n",
        "                self.config[\"post_filter\"][key] = value\n",
        "                logger.info(f\"Updated post-filter parameter: {key} = {value}\")\n",
        "\n",
        "        # Special handling for sensitive items\n",
        "        if \"sensitive_items\" in kwargs:\n",
        "            self.post_filter.add_sensitive_items(kwargs[\"sensitive_items\"])\n",
        "            logger.info(f\"Added {len(kwargs['sensitive_items'])} sensitive items to post-filter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804,
          "referenced_widgets": [
            "f2fc1522525649a38f1ae96410b4170b",
            "90503c2141844d1a943870f42d0d1564",
            "79ef355c55ff4279b0b6a56e27eac41e",
            "03356978a19d48958a6b987823f8a8a1",
            "607747b751f94cb197571e3a9d269dc6",
            "dfb2823eeee14209adefb4a381a5b5b1",
            "9873724a000a458a8f8c05390a3d9fab",
            "388a6135d5cd47b7b3dee1352b55860e",
            "f00447dcb5914a3d9dec1f4ab620d847",
            "440c652c57984ef59ff51d89b60d43ad",
            "c970aef5ce254d32b112046b381ce2fe",
            "a92d259f60d84704961a728d75fc1801",
            "dfba185289cd4bfebc5541469a845148",
            "ff58d8fe78724421a55e43c4d0dc64b7",
            "7414cc2c1fa548a78736924f25703c29",
            "da941b949bd641f2b26b0111a710443a",
            "2e9c10e3e0f7410c8da75e4c500f930e",
            "83156b4fa27e475f90bf6f788c95d708",
            "2c4465bcdba94819823207975bfc5f57",
            "d50b49387e414898839b2e383a1f6e4a",
            "9f6d7074c27a4437b63daf603341e141",
            "96f1c80da31046b5ada7569afff99a24",
            "9af107f45eb44ff09cd2e9b8cad22c34",
            "5e03d96c6bd6401b808894669f791f7e",
            "099d969d3d2b48cd8f9758206c1352e0",
            "ca3d62f9c04b405384d32806e05407f5",
            "57f6295cd9af49a9b80bae8c6223f09d",
            "485c429733014b31ae427cf99bdc8f03",
            "5d45b220b9154e2f86393729bec59909",
            "8d668eef4ecf46b681c4cb6ca11591c3",
            "0cd9d3d4740346b0a005e023aaf68c82",
            "f65199e23b9b4429935e6eceffc77679",
            "d7958c18d9cd456bb3dae566452fd820",
            "5d7d0fc7c16d4ab88e99aab2f1f10186",
            "a417bd961f844badb4506af79a7f6c35",
            "6cc01642394c462699ff75c0f39a1ada",
            "e1f3c3f6f4604c78b83dad6df261d7c7",
            "97d184f36e294400b3edb5e928d6d2cd",
            "3e2984f5d3974e159d00c5a89d4e9091",
            "c1df11c8a17d4c849256023faa6cc2c9",
            "f6b2feed82a54e15b4347bf86f06f4b9",
            "be4397ec9ae94d66aae46b0c6c97dc6a",
            "da5bb40ef4af480fb34764153e388759",
            "24012eae2f6f4b67b3db4434efc97a35",
            "abdc9915a4c74a4186eea2d420ca22e0",
            "bbb102d20a79497d9ee300a8ca95419b",
            "b88ed5b93d284a938bdb052ac41ea64b",
            "f19975c82c1540e99befd3ed16dc06b4",
            "57920ad91570446e84ae32fdab7624e3",
            "4a3f3e3406984854bb9e9f14aee024da",
            "5e940f0f819141899d75dabd2d5cb523",
            "625ce3cd8b0d45f2826bad30876a1e04",
            "348671d759f640869d23ba6efe1c1557",
            "facabc990703422d8900a4d61644b524",
            "84a4093337314610818c42a6da6174f3",
            "bae13fe72c5e439485597e8e80e8465c",
            "b808af808de249bbb222ce87f70f9920",
            "ee4dd76746544ba6bcd43c34532fd788",
            "c5cf90fceed547c09a2c54f87a04c70e",
            "2babb7d63f734e0bbe2ee2e96fcfe46f",
            "c0040399ab6641eca288c2b458e820ab",
            "77860de1af984dd5b44aadf8eb3dfb9a",
            "02b5544f4ec6476d823518b605260c4e",
            "7e6bcfd849fd41b98b0e605ff47d759e",
            "3f113c193b654ac6b7589f8eb1ecf1ea",
            "5e38841727814fa8bf8823a2b294a9c3",
            "0048fccd8af0429f94c3a505cfdd4d24",
            "cc47a786e6cc406e99d7549771ebbccc",
            "785818fd7d014b51bd532fa58f5c713f",
            "4751b20c139649a991a2df43be4a0066",
            "91e920c0015a4704a3b0b9fcd4b88c64",
            "b56e976bcc8446069657752224ec20fc",
            "409fa822e6a0439f8c20bd57241fa963",
            "cb60ca90db75486cab237033d5e6d059",
            "dfdfdd112038482192716dfada8ec586",
            "2cadf2520b29451bb7780d9984b4334c",
            "aa0d33f9b15e44ffa4398c23db91f0c1",
            "93f46fbca6b54f408dbccbf7ded2a21c",
            "cda5ac508e5d48c1a8463debcce03786",
            "4b71f36bf67d4cf2a94dcba10ad21833",
            "1229e2b8a1a649ebb79ac66430042aab",
            "4ba750060b0d4cd8a13368998e8aa220",
            "faa666d2337e412a9ca1ce3dddd0634f",
            "776f554abdf647e7b62bd4d1ca7c73e7",
            "aa9fe8c0380e45219e237cd2668a1a2b",
            "d0502159dc034a6695bfc3ce456d9726",
            "cd27b4d597044f39bda39e582c6844d1",
            "8f0a78c968ca441b9bbcc7dfff7591f1",
            "44a677416e3b4a8593de047e9bc968b8",
            "d6fcf854e7314f9f9d7ad4e2ef327ae2",
            "9313390c7a694df893ed46e13efcc87e",
            "e3c48cc8caf941b3babf0a5816681d28",
            "18029dc47f1543918662b76d778dbfd4",
            "dcfcd2b735884616aafba383d3cfb98c",
            "d9b24d3eb10448c597f98b81faeb2dbe",
            "e503f73bd6f840ccb645bf9994ad13f7",
            "2016665e4b574a868d348e070d91e22f",
            "64d52abdb6c1426c8a2c8d371068685f",
            "ee773f37046d4844abdc79288b91c836",
            "7ff03200d5ee49d0a485437d584218b2",
            "67a3f934d1d843a7984398a505e3a0d9",
            "3d5b199d914d42b183bf704c892f13da",
            "d9fba5e602cb414fa5ce5a6f5f10852f",
            "a29120c8a07b41429f8b2a3d7e1fb4ed",
            "1adcd96c77a241e58ccc2c044e5dd01a",
            "2f9864116bd44f9c87d624800ccf0a38",
            "75193da6a3574a7baafb0be5a19e54f2",
            "1ebadc395d6547f994fb0afdc1d97aff",
            "1bdaf75525dc41fda8f7687143519bd5",
            "8d1ea9ef26f14430b899c516cf6a51b9",
            "d9d41a01f0a44fd6a32428af44b4f6c7",
            "dfa5807dda1b4c49932879beb41aac22",
            "f6f22d8615714892977d42fde0f0fc85",
            "c61d361bf2fd41a98ae8b8f57e68dd88",
            "82e63d7f21d7490989fe46b789ff3c7b",
            "8fb59ae445274d06a066103f04cbaaf3",
            "a06c6055296c4e4eb1efb5d1434df129",
            "9da37f1b672d4d1d8fd033dff1ad7ef0",
            "03210a6ad76b40a8bbbbf3325d3ba733",
            "fe09c6c497a74e85a52de5cfc1e2840e",
            "c95d93f3c0b64bca9bc1c3fede0dca42"
          ]
        },
        "id": "JWRV4JTKYkUc",
        "outputId": "281d518c-280b-4c6f-b72c-f9fa145fff61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing pre-filter only...\n",
            "\n",
            "Original text:\n",
            "My name is Jane Doe and my email is jane.doe@example.com\n",
            "\n",
            "Redacted text:\n",
            "My name is [PERSON_1] and my email is [EMAIL]\n",
            "\n",
            "Stats:\n",
            "{'input_length': 56, 'output_length': 45, 'replacements': {'regex': 1, 'ner': 1, 'total': 2}, 'entity_types': ['PERSON']}\n",
            "\n",
            "\n",
            "Testing simplified pipeline (preprocessing only)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2fc1522525649a38f1ae96410b4170b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a92d259f60d84704961a728d75fc1801"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9af107f45eb44ff09cd2e9b8cad22c34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d7d0fc7c16d4ab88e99aab2f1f10186"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abdc9915a4c74a4186eea2d420ca22e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bae13fe72c5e439485597e8e80e8465c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0048fccd8af0429f94c3a505cfdd4d24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93f46fbca6b54f408dbccbf7ded2a21c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44a677416e3b4a8593de047e9bc968b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ff03200d5ee49d0a485437d584218b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9d41a01f0a44fd6a32428af44b4f6c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not working\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "with open('simple_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Initialize the pipeline with configuration\n",
        "try:\n",
        "    # First test only the pre-filter to ensure it works\n",
        "    print(\"Testing pre-filter only...\")\n",
        "    pre_filter = PreGenerationFilter(\n",
        "        use_token_level=True,\n",
        "        type_specific_placeholders=True\n",
        "    )\n",
        "\n",
        "    text = \"My name is Jane Doe and my email is jane.doe@example.com\"\n",
        "    redacted_text, stats = pre_filter.process(text)\n",
        "    print(\"\\nOriginal text:\")\n",
        "    print(text)\n",
        "    print(\"\\nRedacted text:\")\n",
        "    print(redacted_text)\n",
        "    print(\"\\nStats:\")\n",
        "    print(stats)\n",
        "\n",
        "    # Set up a simplified pipeline with just preprocessing (no LLM generation)\n",
        "    print(\"\\n\\nTesting simplified pipeline (preprocessing only)...\")\n",
        "    pipeline_args = {}\n",
        "\n",
        "    if \"pre_filter_config\" in config:\n",
        "        pipeline_args[\"pre_filter_config\"] = config[\"pre_filter_config\"]\n",
        "    if \"post_filter_config\" in config:\n",
        "        pipeline_args[\"post_filter_config\"] = config[\"post_filter_config\"]\n",
        "\n",
        "    # Add sensitive items for post-filter\n",
        "    sensitive_items = [\n",
        "        \"My SSN is 123-45-6789\",\n",
        "        \"My credit card number is 4111-1111-1111-1111\"\n",
        "    ]\n",
        "    pipeline_args[\"sensitive_items\"] = sensitive_items\n",
        "\n",
        "\n",
        "    # Create the pipeline\n",
        "    pipeline = PrivacyFilterPipeline(**pipeline_args)\n",
        "\n",
        "    # Process a text\n",
        "    test_text = \"\"\"\n",
        "    Hello, my name is John Smith and I work at Acme Corporation.\n",
        "    You can reach me at john.smith@example.com or call me at (555) 123-4567.\n",
        "    My address is 123 Main St, New York, NY 10001.\n",
        "    \"\"\"\n",
        "\n",
        "    result = pipeline.process(\n",
        "        text=test_text,\n",
        "        return_all_stages=True\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nOriginal Text:\")\n",
        "    print(result[\"original_text\"])\n",
        "    print(\"\\nPre-Filtered Text:\")\n",
        "    print(result[\"pre_filtered_text\"])\n",
        "    print(\"\\nFinal Text:\")\n",
        "    print(result[\"final_text\"])\n",
        "    print(\"\\nPipeline Stats:\")\n",
        "    print(json.dumps(result[\"stats\"], indent=2))\n",
        "except:\n",
        "    print(\"not working\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40KAwkH6Zxts"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dataset preparation utilities for creating datasets with injected sensitive information.\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Set, Optional, Union, Any, Callable\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SensitiveDataInjector:\n",
        "    \"\"\"\n",
        "    Utility for injecting synthetic sensitive data into datasets for testing privacy filters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        seed: int = 42,\n",
        "        name_list_path: Optional[str] = None,\n",
        "        canary_phrases: List[str] = None,\n",
        "        pii_injection_rate: float = 0.3,\n",
        "        canary_injection_rate: float = 0.1,\n",
        "        max_injections_per_sample: int = 2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the sensitive data injector.\n",
        "\n",
        "        Args:\n",
        "            seed: Random seed for reproducibility.\n",
        "            name_list_path: Path to a file containing names for injection (one per line).\n",
        "            canary_phrases: List of custom \"secret\" canary phrases to inject.\n",
        "            pii_injection_rate: Rate at which to inject PII (0.0-1.0).\n",
        "            canary_injection_rate: Rate at which to inject canary phrases (0.0-1.0).\n",
        "            max_injections_per_sample: Maximum number of injections per text sample.\n",
        "        \"\"\"\n",
        "        self.rng = random.Random(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.pii_injection_rate = pii_injection_rate\n",
        "        self.canary_injection_rate = canary_injection_rate\n",
        "        self.max_injections_per_sample = max_injections_per_sample\n",
        "\n",
        "        # Load names if path provided\n",
        "        self.names = []\n",
        "        if name_list_path:\n",
        "            try:\n",
        "                with open(name_list_path, 'r', encoding='utf-8') as f:\n",
        "                    self.names = [line.strip() for line in f if line.strip()]\n",
        "                logger.info(f\"Loaded {len(self.names)} names from {name_list_path}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load names from {name_list_path}: {str(e)}\")\n",
        "                self._generate_fallback_names()\n",
        "        else:\n",
        "            self._generate_fallback_names()\n",
        "\n",
        "        # Setup canary phrases\n",
        "        self.canary_phrases = canary_phrases or [\n",
        "            \"The purple elephant dances at midnight under the silver moon\",\n",
        "            \"Golden keys unlock the ancient wooden chest of forgotten memories\",\n",
        "            \"Whispers of crystal snowflakes tell secrets to winter shadows\",\n",
        "            \"Emerald dragons soar through clouds of rainbow silk\",\n",
        "            \"The captain's compass always points to dreams of lost islands\"\n",
        "        ]\n",
        "\n",
        "        # Setup PII generators\n",
        "        self.pii_generators = {\n",
        "            'name': self._generate_name,\n",
        "            'email': self._generate_email,\n",
        "            'phone': self._generate_phone,\n",
        "            'ssn': self._generate_ssn,\n",
        "            'credit_card': self._generate_credit_card,\n",
        "            'address': self._generate_address,\n",
        "        }\n",
        "\n",
        "    def _generate_fallback_names(self):\n",
        "        \"\"\"Generate a list of fake names when no name file is provided.\"\"\"\n",
        "        first_names = [\"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\",\n",
        "                     \"Linda\", \"William\", \"Elizabeth\", \"David\", \"Susan\", \"Joseph\", \"Jessica\",\n",
        "                     \"Charles\", \"Sarah\", \"Thomas\", \"Karen\", \"Daniel\", \"Lisa\"]\n",
        "\n",
        "        last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Miller\", \"Davis\",\n",
        "                     \"Garcia\", \"Rodriguez\", \"Wilson\", \"Martinez\", \"Anderson\", \"Taylor\",\n",
        "                     \"Thomas\", \"Hernandez\", \"Moore\", \"Martin\", \"Jackson\", \"Thompson\", \"White\"]\n",
        "\n",
        "        self.names = [f\"{first} {last}\" for first in first_names for last in last_names]\n",
        "        self.rng.shuffle(self.names)\n",
        "        logger.info(f\"Generated {len(self.names)} fallback names\")\n",
        "\n",
        "    def _generate_name(self) -> str:\n",
        "        \"\"\"Generate a random name.\"\"\"\n",
        "        return self.rng.choice(self.names)\n",
        "\n",
        "    def _generate_email(self) -> str:\n",
        "        \"\"\"Generate a random email address.\"\"\"\n",
        "        name = self.rng.choice(self.names).lower().replace(\" \", \".\")\n",
        "        domains = [\"gmail.com\", \"yahoo.com\", \"outlook.com\", \"example.com\", \"company.org\"]\n",
        "        domain = self.rng.choice(domains)\n",
        "        return f\"{name}@{domain}\"\n",
        "\n",
        "    def _generate_phone(self) -> str:\n",
        "        \"\"\"Generate a random US phone number.\"\"\"\n",
        "        area_code = self.rng.randint(200, 999)\n",
        "        prefix = self.rng.randint(200, 999)\n",
        "        line = self.rng.randint(1000, 9999)\n",
        "        formats = [\n",
        "            f\"({area_code}) {prefix}-{line}\",\n",
        "            f\"{area_code}-{prefix}-{line}\",\n",
        "            f\"+1 {area_code} {prefix} {line}\"\n",
        "        ]\n",
        "        return self.rng.choice(formats)\n",
        "\n",
        "    def _generate_ssn(self) -> str:\n",
        "        \"\"\"Generate a random Social Security Number.\"\"\"\n",
        "        area = self.rng.randint(100, 999)\n",
        "        group = self.rng.randint(10, 99)\n",
        "        serial = self.rng.randint(1000, 9999)\n",
        "        return f\"{area}-{group}-{serial}\"\n",
        "\n",
        "    def _generate_credit_card(self) -> str:\n",
        "        \"\"\"Generate a random credit card number.\"\"\"\n",
        "        prefixes = [\"4\", \"51\", \"36\", \"34\", \"6011\"]\n",
        "        prefix = self.rng.choice(prefixes)\n",
        "\n",
        "        # Generate the rest of the digits\n",
        "        remaining_length = 16 - len(prefix)\n",
        "        digits = [str(self.rng.randint(0, 9)) for _ in range(remaining_length)]\n",
        "        number = prefix + \"\".join(digits)\n",
        "\n",
        "        # Format with dashes\n",
        "        formatted = \"-\".join([number[i:i+4] for i in range(0, len(number), 4)])\n",
        "        return formatted\n",
        "\n",
        "    def _generate_address(self) -> str:\n",
        "        \"\"\"Generate a random US address.\"\"\"\n",
        "        number = self.rng.randint(1, 9999)\n",
        "        street_types = [\"St\", \"Ave\", \"Blvd\", \"Dr\", \"Ln\", \"Rd\", \"Way\", \"Place\", \"Court\"]\n",
        "        street_names = [\"Main\", \"Oak\", \"Maple\", \"Washington\", \"Park\", \"Elm\", \"Lake\", \"Hill\",\n",
        "                      \"Pine\", \"Cedar\", \"Sunset\", \"River\", \"Ocean\", \"Meadow\", \"Forest\"]\n",
        "\n",
        "        street = f\"{number} {self.rng.choice(street_names)} {self.rng.choice(street_types)}\"\n",
        "\n",
        "        cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n",
        "                \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\", \"Austin\", \"Jacksonville\",\n",
        "                \"Fort Worth\", \"Columbus\", \"Charlotte\", \"Seattle\", \"Denver\", \"Boston\"]\n",
        "\n",
        "        states = {\n",
        "            \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\",\n",
        "            \"CA\": \"California\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\",\n",
        "            \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
        "            \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\",\n",
        "            \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n",
        "            \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
        "            \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\",\n",
        "            \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\",\n",
        "            \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
        "            \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n",
        "            \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\",\n",
        "            \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\",\n",
        "            \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"\n",
        "        }\n",
        "\n",
        "        state_abbr = self.rng.choice(list(states.keys()))\n",
        "\n",
        "        city = self.rng.choice(cities)\n",
        "        zip_code = self.rng.randint(10000, 99999)\n",
        "\n",
        "        # Decide on format (full state name or abbreviation)\n",
        "        if self.rng.random() > 0.5:\n",
        "            return f\"{street}, {city}, {state_abbr} {zip_code}\"\n",
        "        else:\n",
        "            return f\"{street}, {city}, {states[state_abbr]} {zip_code}\"\n",
        "\n",
        "    def _select_injection_points(self, text: str, num_injections: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        Select positions in the text to inject sensitive data.\n",
        "\n",
        "        Args:\n",
        "            text: The text to inject into.\n",
        "            num_injections: Number of injection points to select.\n",
        "\n",
        "        Returns:\n",
        "            List of character positions for injections.\n",
        "        \"\"\"\n",
        "        # Split text into sentences\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        if not sentences:\n",
        "            return []\n",
        "\n",
        "        # Remove empty sentences\n",
        "        sentences = [s for s in sentences if s.strip()]\n",
        "        if not sentences:\n",
        "            return []\n",
        "\n",
        "        # Select random sentences for injection\n",
        "        selected_indices = self.rng.sample(range(len(sentences)), min(num_injections, len(sentences)))\n",
        "\n",
        "        # Find the character positions of the selected sentences\n",
        "        positions = []\n",
        "        current_pos = 0\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i in selected_indices:\n",
        "                # Insert at the end of the sentence\n",
        "                end_pos = current_pos + len(sentence)\n",
        "                positions.append(end_pos)\n",
        "            current_pos += len(sentence) + 1  # +1 for the space after the sentence\n",
        "\n",
        "        return positions\n",
        "\n",
        "    def inject_pii(self, text: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        Inject PII into the given text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to inject PII into.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - Text with injected PII\n",
        "            - Dictionary with metadata about injections\n",
        "        \"\"\"\n",
        "        if not text or len(text.strip()) < 10:\n",
        "            return text, {\"error\": \"Text too short for injection\"}\n",
        "\n",
        "        # Determine if we should inject PII in this sample\n",
        "        if self.rng.random() > self.pii_injection_rate:\n",
        "            return text, {\"injected\": False}\n",
        "\n",
        "        # Determine how many PIIs to inject\n",
        "        num_injections = self.rng.randint(1, self.max_injections_per_sample)\n",
        "\n",
        "        # Select injection points\n",
        "        injection_points = self._select_injection_points(text, num_injections)\n",
        "        if not injection_points:\n",
        "            return text, {\"injected\": False, \"reason\": \"No suitable injection points\"}\n",
        "\n",
        "        # Select PII types to inject\n",
        "        pii_types = self.rng.choices(list(self.pii_generators.keys()), k=len(injection_points))\n",
        "\n",
        "        # Inject PII\n",
        "        injected_text = text\n",
        "        offset = 0\n",
        "        injections = []\n",
        "\n",
        "        for point, pii_type in zip(injection_points, pii_types):\n",
        "            # Generate PII\n",
        "            pii_value = self.pii_generators[pii_type]()\n",
        "\n",
        "            # Create injection string\n",
        "            injection_templates = [\n",
        "                f\" My {pii_type} is {pii_value}.\",\n",
        "                f\" By the way, my {pii_type} is {pii_value}.\",\n",
        "                f\" For reference, my {pii_type} is {pii_value}.\",\n",
        "                f\" Just so you know, my {pii_type} is {pii_value}.\"\n",
        "            ]\n",
        "            injection_str = self.rng.choice(injection_templates)\n",
        "\n",
        "            # Insert at the adjusted point\n",
        "            adjusted_point = point + offset\n",
        "            injected_text = injected_text[:adjusted_point] + injection_str + injected_text[adjusted_point:]\n",
        "\n",
        "            # Update offset\n",
        "            offset += len(injection_str)\n",
        "\n",
        "            # Record injection metadata\n",
        "            injections.append({\n",
        "                \"type\": pii_type,\n",
        "                \"value\": pii_value,\n",
        "                \"position\": adjusted_point,\n",
        "                \"injection_text\": injection_str\n",
        "            })\n",
        "\n",
        "        metadata = {\n",
        "            \"injected\": True,\n",
        "            \"num_injections\": len(injections),\n",
        "            \"injections\": injections\n",
        "        }\n",
        "\n",
        "        return injected_text, metadata\n",
        "\n",
        "    def inject_canary(self, text: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"\n",
        "        Inject a canary phrase into the given text.\n",
        "\n",
        "        Args:\n",
        "            text: Text to inject canary into.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - Text with injected canary\n",
        "            - Dictionary with metadata about the injection\n",
        "        \"\"\"\n",
        "        if not text or len(text.strip()) < 10:\n",
        "            return text, {\"error\": \"Text too short for injection\"}\n",
        "\n",
        "        # Determine if we should inject a canary in this sample\n",
        "        if self.rng.random() > self.canary_injection_rate:\n",
        "            return text, {\"injected\": False}\n",
        "\n",
        "        # Select a canary phrase\n",
        "        canary = self.rng.choice(self.canary_phrases)\n",
        "\n",
        "        # Select an injection point\n",
        "        injection_points = self._select_injection_points(text, 1)\n",
        "        if not injection_points:\n",
        "            return text, {\"injected\": False, \"reason\": \"No suitable injection points\"}\n",
        "\n",
        "        # Create injection string\n",
        "        injection_templates = [\n",
        "            f\" {canary}.\",\n",
        "            f\" Remember that \\\"{canary}\\\".\",\n",
        "            f\" As they say, \\\"{canary}\\\".\",\n",
        "            f\" I once heard that \\\"{canary}\\\".\"\n",
        "        ]\n",
        "        injection_str = self.rng.choice(injection_templates)\n",
        "\n",
        "        # Insert at the point\n",
        "        point = injection_points[0]\n",
        "        injected_text = text[:point] + injection_str + text[point:]\n",
        "\n",
        "        metadata = {\n",
        "            \"injected\": True,\n",
        "            \"canary\": canary,\n",
        "            \"position\": point,\n",
        "            \"injection_text\": injection_str\n",
        "        }\n",
        "\n",
        "        return injected_text, metadata\n",
        "\n",
        "    def process_dataset(\n",
        "        self,\n",
        "        input_data: Union[str, pd.DataFrame, List[str]],\n",
        "        text_column: str = None,\n",
        "        output_path: Optional[str] = None,\n",
        "        output_format: str = \"json\",\n",
        "        id_column: str = None\n",
        "    ) -> Tuple[pd.DataFrame, Dict]:\n",
        "        \"\"\"\n",
        "        Process a dataset by injecting PII and canary phrases.\n",
        "\n",
        "        Args:\n",
        "            input_data: File path, DataFrame, or list of strings.\n",
        "            text_column: Column name containing text (if DataFrame).\n",
        "            output_path: Path to save the processed dataset.\n",
        "            output_format: Format to save the output (json, csv, pickle).\n",
        "            id_column: Column to use as ID (if DataFrame).\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - Processed DataFrame\n",
        "            - Dictionary with processing statistics\n",
        "        \"\"\"\n",
        "        # Load the dataset\n",
        "        df = None\n",
        "        if isinstance(input_data, str):\n",
        "            # Input is a file path\n",
        "            file_path = input_data\n",
        "            extension = Path(file_path).suffix.lower()\n",
        "\n",
        "            try:\n",
        "                if extension == \".csv\":\n",
        "                    df = pd.read_csv(file_path)\n",
        "                elif extension == \".json\":\n",
        "                    df = pd.read_json(file_path)\n",
        "                elif extension in [\".pkl\", \".pickle\"]:\n",
        "                    df = pd.read_pickle(file_path)\n",
        "                elif extension in [\".txt\", \".text\"]:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        lines = [line.strip() for line in f if line.strip()]\n",
        "                    df = pd.DataFrame({\"text\": lines})\n",
        "                    text_column = \"text\"\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported file extension: {extension}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load dataset from {file_path}: {str(e)}\")\n",
        "                raise\n",
        "        elif isinstance(input_data, pd.DataFrame):\n",
        "            # Input is already a DataFrame\n",
        "            df = input_data.copy()\n",
        "        elif isinstance(input_data, list):\n",
        "            # Input is a list of strings\n",
        "            df = pd.DataFrame({\"text\": input_data})\n",
        "            text_column = \"text\"\n",
        "        else:\n",
        "            raise ValueError(\"input_data must be a file path, DataFrame, or list of strings\")\n",
        "\n",
        "        # Validate text column\n",
        "        if text_column is None:\n",
        "            # Try to guess the text column\n",
        "            potential_text_cols = [\"text\", \"content\", \"message\", \"body\", \"description\"]\n",
        "            for col in potential_text_cols:\n",
        "                if col in df.columns:\n",
        "                    text_column = col\n",
        "                    logger.info(f\"Using guessed text column: {text_column}\")\n",
        "                    break\n",
        "\n",
        "            if text_column is None:\n",
        "                raise ValueError(\"text_column must be specified if it cannot be guessed\")\n",
        "\n",
        "        if text_column not in df.columns:\n",
        "            raise ValueError(f\"Text column '{text_column}' not found in the dataset\")\n",
        "\n",
        "        # Add ID column if needed\n",
        "        if id_column is None or id_column not in df.columns:\n",
        "            df[\"id\"] = [f\"sample_{i}\" for i in range(len(df))]\n",
        "            id_column = \"id\"\n",
        "\n",
        "        # Create clean copies of the dataset\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        # Initialize tracking columns\n",
        "        df[\"has_pii\"] = False\n",
        "        df[\"has_canary\"] = False\n",
        "        df[\"pii_metadata\"] = [{}] * len(df)\n",
        "        df[\"canary_metadata\"] = [{}] * len(df)\n",
        "\n",
        "        # Process each row\n",
        "        stats = {\n",
        "            \"total_samples\": len(df),\n",
        "            \"pii_injections\": 0,\n",
        "            \"canary_injections\": 0,\n",
        "            \"pii_types\": {},\n",
        "            \"canaries_used\": {}\n",
        "        }\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row[text_column]\n",
        "\n",
        "            # Skip empty or very short texts\n",
        "            if not isinstance(text, str) or len(text.strip()) < 10:\n",
        "                continue\n",
        "\n",
        "            # Inject PII\n",
        "            text_with_pii, pii_metadata = self.inject_pii(text)\n",
        "            if pii_metadata.get(\"injected\", False):\n",
        "                df.at[idx, text_column] = text_with_pii\n",
        "                df.at[idx, \"has_pii\"] = True\n",
        "                df.at[idx, \"pii_metadata\"] = pii_metadata\n",
        "                stats[\"pii_injections\"] += 1\n",
        "\n",
        "                # Track PII types\n",
        "                for injection in pii_metadata.get(\"injections\", []):\n",
        "                    pii_type = injection[\"type\"]\n",
        "                    stats[\"pii_types\"][pii_type] = stats[\"pii_types\"].get(pii_type, 0) + 1\n",
        "\n",
        "            # Inject canary phrase (possibly on top of the PII)\n",
        "            current_text = df.at[idx, text_column]\n",
        "            text_with_canary, canary_metadata = self.inject_canary(current_text)\n",
        "            if canary_metadata.get(\"injected\", False):\n",
        "                df.at[idx, text_column] = text_with_canary\n",
        "                df.at[idx, \"has_canary\"] = True\n",
        "                df.at[idx, \"canary_metadata\"] = canary_metadata\n",
        "                stats[\"canary_injections\"] += 1\n",
        "\n",
        "                # Track canaries used\n",
        "                canary = canary_metadata.get(\"canary\", \"\")\n",
        "                stats[\"canaries_used\"][canary] = stats[\"canaries_used\"].get(canary, 0) + 1\n",
        "\n",
        "        # Calculate percentages\n",
        "        stats[\"pii_injection_rate\"] = stats[\"pii_injections\"] / stats[\"total_samples\"]\n",
        "        stats[\"canary_injection_rate\"] = stats[\"canary_injections\"] / stats[\"total_samples\"]\n",
        "\n",
        "        # Save the processed dataset if requested\n",
        "        if output_path:\n",
        "            output_dir = os.path.dirname(output_path)\n",
        "            if output_dir:\n",
        "                os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            if output_format == \"json\":\n",
        "                df.to_json(output_path, orient=\"records\", indent=2)\n",
        "            elif output_format == \"csv\":\n",
        "                df.to_csv(output_path, index=False)\n",
        "            elif output_format == \"pickle\":\n",
        "                df.to_pickle(output_path)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "            # Save clean dataset if applicable\n",
        "            clean_path = Path(output_path)\n",
        "            clean_stem = clean_path.stem + \"_clean\"\n",
        "            clean_path = clean_path.with_stem(clean_stem)\n",
        "\n",
        "            if output_format == \"json\":\n",
        "                df_clean.to_json(clean_path, orient=\"records\", indent=2)\n",
        "            elif output_format == \"csv\":\n",
        "                df_clean.to_csv(clean_path, index=False)\n",
        "            elif output_format == \"pickle\":\n",
        "                df_clean.to_pickle(clean_path)\n",
        "\n",
        "            logger.info(f\"Saved processed dataset to {output_path}\")\n",
        "            logger.info(f\"Saved clean dataset to {clean_path}\")\n",
        "\n",
        "        return df, stats\n",
        "\n",
        "\n",
        "def load_enron_sample(num_samples: int = 1000, seed: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a sample of the Enron email dataset.\n",
        "\n",
        "    Args:\n",
        "        num_samples: Number of samples to load.\n",
        "        seed: Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with the loaded samples.\n",
        "    \"\"\"\n",
        "    # This is a placeholder. In a real implementation, you would download or load the actual dataset.\n",
        "    # For demo purposes, we'll generate some fake emails that resemble the Enron format.\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    subjects = [\n",
        "        \"Meeting Agenda\", \"Project Update\", \"Quarterly Financials\", \"Team Lunch\",\n",
        "        \"Conference Call\", \"Budget Review\", \"New Policy\", \"HR Announcement\",\n",
        "        \"System Maintenance\", \"Weekly Report\", \"Training Session\", \"Client Meeting\",\n",
        "        \"Contract Negotiation\", \"Marketing Plan\", \"Sales Targets\", \"Product Launch\"\n",
        "    ]\n",
        "\n",
        "    senders = [\n",
        "        \"john.doe@enron.com\", \"jane.smith@enron.com\", \"robert.johnson@enron.com\",\n",
        "        \"emily.williams@enron.com\", \"michael.brown@enron.com\", \"sarah.miller@enron.com\",\n",
        "        \"david.wilson@enron.com\", \"jennifer.moore@enron.com\", \"richard.taylor@enron.com\",\n",
        "        \"jessica.anderson@enron.com\", \"william.jackson@enron.com\", \"lisa.white@enron.com\"\n",
        "    ]\n",
        "\n",
        "    recipients = [\n",
        "        \"sales-team@enron.com\", \"executive-committee@enron.com\", \"it-support@enron.com\",\n",
        "        \"finance-department@enron.com\", \"hr-staff@enron.com\", \"marketing-team@enron.com\",\n",
        "        \"legal-department@enron.com\", \"customer-service@enron.com\", \"research-team@enron.com\"\n",
        "    ]\n",
        "\n",
        "    body_templates = [\n",
        "        \"Hi team,\\n\\n{content}\\n\\nRegards,\\n{sender_name}\",\n",
        "        \"Dear colleagues,\\n\\n{content}\\n\\nThank you,\\n{sender_name}\",\n",
        "        \"Hello,\\n\\n{content}\\n\\nBest,\\n{sender_name}\",\n",
        "        \"Team,\\n\\n{content}\\n\\nCheers,\\n{sender_name}\",\n",
        "        \"Greetings,\\n\\n{content}\\n\\nBest wishes,\\n{sender_name}\"\n",
        "    ]\n",
        "\n",
        "    content_snippets = [\n",
        "        \"Please find attached the latest financial reports for Q3. We need to review these before our meeting next week.\",\n",
        "        \"I wanted to update you on the progress of Project Alpha. We're currently on track to meet our deadlines, but there are some resource constraints we need to address.\",\n",
        "        \"The client meeting scheduled for Thursday has been moved to Friday at 2pm. Please adjust your calendars accordingly.\",\n",
        "        \"We need to finalize the budget for the next fiscal year. Please send me your department's projections by end of day tomorrow.\",\n",
        "        \"The new HR policy regarding remote work will take effect starting next month. More details will be provided in the upcoming town hall.\",\n",
        "        \"Our team has exceeded sales targets for this quarter. I'd like to thank everyone for their hard work and dedication.\",\n",
        "        \"There will be system maintenance this weekend. Please save your work and log out before leaving on Friday.\",\n",
        "        \"The marketing campaign for Product X has been very successful. We've seen a 30% increase in leads since launch.\",\n",
        "        \"We're planning a team building event for next month. Please fill out the survey to indicate your preferences.\",\n",
        "        \"The legal team has reviewed the contract and has some concerns about clauses 3.2 and 5.7. We need to discuss this further.\",\n",
        "        \"I'll be out of office next week attending the industry conference. Please direct urgent matters to my assistant.\",\n",
        "        \"We need volunteers for the company charity event. If you're interested, please let me know by Wednesday.\",\n",
        "        \"The quarterly performance reviews are coming up. Managers should schedule meetings with their team members soon.\",\n",
        "        \"We're considering new vendors for our IT services. If you have any recommendations, please share them with the procurement team.\",\n",
        "        \"The customer satisfaction survey results are in, and I'm pleased to say we've improved by 15% compared to last year.\"\n",
        "    ]\n",
        "\n",
        "    # Generate random emails\n",
        "    emails = []\n",
        "    for i in range(num_samples):\n",
        "        sender = rng.choice(senders)\n",
        "        sender_name = sender.split('@')[0].replace('.', ' ').title()\n",
        "\n",
        "        recipient = rng.choice(recipients)\n",
        "        subject = rng.choice(subjects)\n",
        "\n",
        "        # Create email body\n",
        "        content = rng.choice(content_snippets)\n",
        "        template = rng.choice(body_templates)\n",
        "        body = template.format(content=content, sender_name=sender_name)\n",
        "\n",
        "        # Add random additional content\n",
        "        if rng.random() < 0.7:\n",
        "            additional_content = rng.choice(content_snippets)\n",
        "            body += f\"\\n\\nP.S. {additional_content}\"\n",
        "\n",
        "        # Create email entry\n",
        "        email = {\n",
        "            \"id\": f\"email_{i}\",\n",
        "            \"sender\": sender,\n",
        "            \"recipient\": recipient,\n",
        "            \"subject\": subject,\n",
        "            \"body\": body,\n",
        "            \"date\": f\"2023-{rng.randint(1, 12):02d}-{rng.randint(1, 28):02d}\"\n",
        "        }\n",
        "\n",
        "        emails.append(email)\n",
        "\n",
        "    return pd.DataFrame(emails)\n",
        "\n",
        "\n",
        "def load_stackoverflow_sample(num_samples: int = 1000, seed: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a sample of the Stack Overflow questions dataset.\n",
        "\n",
        "    Args:\n",
        "        num_samples: Number of samples to load.\n",
        "        seed: Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with the loaded samples.\n",
        "    \"\"\"\n",
        "    # This is a placeholder. In a real implementation, you would download or load the actual dataset.\n",
        "    # For demo purposes, we'll generate some fake Stack Overflow questions.\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "    # Sample question titles\n",
        "    titles = [\n",
        "        \"How to implement authentication using JWT in React\",\n",
        "        \"Python pandas groupby with multiple columns\",\n",
        "        \"Best practices for error handling in asynchronous JavaScript\",\n",
        "        \"How to optimize SQL queries for large databases\",\n",
        "        \"Understanding Docker container networking\",\n",
        "        \"Converting DataFrame to dictionary in pandas\",\n",
        "        \"Efficient ways to handle state in React components\",\n",
        "        \"Using LSTM networks for time series prediction\",\n",
        "        \"How to implement pagination in a REST API\",\n",
        "        \"Setting up CI/CD pipeline with GitHub Actions\",\n",
        "        \"Optimizing performance in Django applications\",\n",
        "        \"Best way to handle form validation in Angular\",\n",
        "        \"Understanding memory management in Node.js\",\n",
        "        \"How to implement websockets in a Flask application\",\n",
        "        \"Strategies for database indexing in PostgreSQL\",\n",
        "        \"Implementing role-based access control in Express.js\",\n",
        "        \"Understanding async/await in JavaScript\",\n",
        "        \"Best practices for microservice architecture\",\n",
        "        \"How to use Redux with React hooks\",\n",
        "        \"Implementing full-text search in MongoDB\"\n",
        "    ]\n",
        "\n",
        "    # Sample tags\n",
        "    all_tags = [\n",
        "        \"python\", \"javascript\", \"java\", \"c#\", \"php\", \"android\", \"html\", \"jquery\",\n",
        "        \"css\", \"ios\", \"sql\", \"mysql\", \"r\", \"node.js\", \"arrays\", \"c++\", \"json\",\n",
        "        \"ruby-on-rails\", \"c\", \"sql-server\", \"swift\", \"objective-c\", \"django\",\n",
        "        \"angular\", \"excel\", \"regex\", \"ruby\", \"iphone\", \"ajax\", \"xml\", \"asp.net\",\n",
        "        \"linux\", \"react.js\", \"database\", \"wpf\", \"spring\", \"algorithm\", \"python-3.x\",\n",
        "        \"vba\", \"dataframe\", \"windows\", \"pandas\", \"mongodb\", \"xcode\", \"multithreading\",\n",
        "        \"string\", \"typescript\", \"laravel\", \"oracle\", \"bash\", \"git\", \"firebase\",\n",
        "        \"tensorflow\", \"api\", \"docker\", \"rest\", \"kubernetes\", \"web-development\"\n",
        "    ]\n",
        "\n",
        "    # Sample question templates\n",
        "    question_templates = [\n",
        "        \"I'm trying to {action} but I'm running into an issue where {problem}. Here's my code:\\n\\n```{language}\\n{code_snippet}\\n```\\n\\nWhat am I doing wrong?\",\n",
        "        \"What's the best way to {action} in {technology}? I've tried {approach} but it {problem}.\",\n",
        "        \"I need to {action} and I'm not sure how to approach it. My current solution is:\\n\\n```{language}\\n{code_snippet}\\n```\\n\\nIs there a more efficient way?\",\n",
        "        \"Can someone explain how to {action} in {technology}? The documentation is unclear on this point.\",\n",
        "        \"I'm having trouble understanding how {concept} works in {technology}. Specifically, I don't understand {problem}.\"\n",
        "    ]\n",
        "\n",
        "    # Sample actions\n",
        "    actions = [\n",
        "        \"implement authentication\",\n",
        "        \"optimize database queries\",\n",
        "        \"handle form validation\",\n",
        "        \"set up a CI/CD pipeline\",\n",
        "        \"debug memory leaks\",\n",
        "        \"implement pagination\",\n",
        "        \"handle state management\",\n",
        "        \"optimize performance\",\n",
        "        \"implement websockets\",\n",
        "        \"create a RESTful API\",\n",
        "        \"deploy to production\",\n",
        "        \"implement caching\",\n",
        "        \"handle error logging\",\n",
        "        \"set up unit tests\",\n",
        "        \"implement file uploads\",\n",
        "        \"create a responsive design\",\n",
        "        \"handle concurrent requests\",\n",
        "        \"implement search functionality\",\n",
        "        \"secure API endpoints\",\n",
        "        \"migrate to a new framework\"\n",
        "    ]\n",
        "\n",
        "    # Sample problems\n",
        "    problems = [\n",
        "        \"I get an error message\",\n",
        "        \"it's not working as expected\",\n",
        "        \"performance is very slow\",\n",
        "        \"the application crashes\",\n",
        "        \"the data isn't being saved correctly\",\n",
        "        \"it works in development but not in production\",\n",
        "        \"it's not compatible with older browsers\",\n",
        "        \"it works inconsistently\",\n",
        "        \"I'm getting undefined behavior\",\n",
        "        \"it's causing memory leaks\",\n",
        "        \"I'm getting strange side effects\",\n",
        "        \"it conflicts with other parts of my code\",\n",
        "        \"it's not secure\",\n",
        "        \"I'm getting timeout errors\",\n",
        "        \"it's not scalable\"\n",
        "    ]\n",
        "\n",
        "    # Sample technologies\n",
        "    technologies = [\n",
        "        \"React\", \"Angular\", \"Vue.js\", \"Node.js\", \"Django\", \"Flask\", \"Rails\",\n",
        "        \"Spring Boot\", \"Express.js\", \"Laravel\", \"ASP.NET Core\", \"Symfony\",\n",
        "        \"jQuery\", \"Bootstrap\", \"Tailwind CSS\", \"PostgreSQL\", \"MongoDB\", \"MySQL\",\n",
        "        \"Redis\", \"ElasticSearch\", \"Docker\", \"Kubernetes\", \"AWS\", \"Azure\",\n",
        "        \"Google Cloud\", \"Firebase\", \"TensorFlow\", \"PyTorch\", \"Pandas\", \"NumPy\"\n",
        "    ]\n",
        "\n",
        "    # Sample code snippets for different languages\n",
        "    code_snippets = {\n",
        "        \"python\": [\n",
        "            \"def process_data(data):\\n    result = []\\n    for item in data:\\n        if item['valid']:\\n            result.append(item['value'] * 2)\\n    return result\",\n",
        "            \"class User:\\n    def __init__(self, username, email):\\n        self.username = username\\n        self.email = email\\n        \\n    def validate(self):\\n        if '@' not in self.email:\\n            raise ValueError('Invalid email')\",\n",
        "            \"import pandas as pd\\n\\ndf = pd.read_csv('data.csv')\\ngrouped = df.groupby(['category', 'region']).agg({'sales': 'sum'})\",\n",
        "            \"from flask import Flask, request\\n\\napp = Flask(__name__)\\n\\n@app.route('/api/users', methods=['GET'])\\ndef get_users():\\n    page = request.args.get('page', 1, type=int)\\n    limit = request.args.get('limit', 10, type=int)\\n    # pagination logic here\"\n",
        "        ],\n",
        "        \"javascript\": [\n",
        "            \"function fetchData() {\\n  return fetch('https://api.example.com/data')\\n    .then(response => response.json())\\n    .then(data => {\\n      return data.items.map(item => {\\n        return {\\n          id: item.id,\\n          name: item.name,\\n          value: item.value * 2\\n        };\\n      });\\n    })\\n    .catch(error => console.error('Error:', error));\\n}\",\n",
        "            \"class AuthService {\\n  constructor(apiClient) {\\n    this.apiClient = apiClient;\\n    this.token = localStorage.getItem('token');\\n  }\\n\\n  isAuthenticated() {\\n    return !!this.token;\\n  }\\n\\n  login(credentials) {\\n    return this.apiClient.post('/login', credentials)\\n      .then(response => {\\n        this.token = response.data.token;\\n        localStorage.setItem('token', this.token);\\n        return response;\\n      });\\n  }\\n}\",\n",
        "            \"function debounce(func, wait) {\\n  let timeout;\\n  return function(...args) {\\n    clearTimeout(timeout);\\n    timeout = setTimeout(() => func.apply(this, args), wait);\\n  };\\n}\"\n",
        "        ],\n",
        "        \"java\": [\n",
        "            \"public class UserService {\\n    private final UserRepository userRepository;\\n    \\n    public UserService(UserRepository userRepository) {\\n        this.userRepository = userRepository;\\n    }\\n    \\n    public User createUser(String username, String email, String password) {\\n        if (userRepository.findByEmail(email).isPresent()) {\\n            throw new UserAlreadyExistsException(\\\"Email already in use\\\");\\n        }\\n        \\n        User user = new User(username, email, password);\\n        return userRepository.save(user);\\n    }\\n}\",\n",
        "            \"import java.util.List;\\nimport java.util.stream.Collectors;\\n\\npublic List<String> processItems(List<Item> items) {\\n    return items.stream()\\n            .filter(item -> item.isActive())\\n            .map(Item::getName)\\n            .collect(Collectors.toList());\\n}\"\n",
        "        ],\n",
        "        \"sql\": [\n",
        "            \"SELECT\\n    c.customer_id,\\n    c.name,\\n    COUNT(o.order_id) as total_orders,\\n    SUM(o.total_amount) as total_spent\\nFROM\\n    customers c\\nLEFT JOIN\\n    orders o ON c.customer_id = o.customer_id\\nWHERE\\n    o.order_date BETWEEN '2022-01-01' AND '2022-12-31'\\nGROUP BY\\n    c.customer_id, c.name\\nHAVING\\n    COUNT(o.order_id) > 5\\nORDER BY\\n    total_spent DESC;\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Generate random questions\n",
        "    questions = []\n",
        "    for i in range(num_samples):\n",
        "        title = rng.choice(titles)\n",
        "\n",
        "        # Generate 2-5 random tags\n",
        "        num_tags = rng.randint(2, 5)\n",
        "        tags = rng.sample(all_tags, num_tags)\n",
        "\n",
        "        # Generate question body\n",
        "        template = rng.choice(question_templates)\n",
        "        action = rng.choice(actions)\n",
        "        problem = rng.choice(problems)\n",
        "        technology = rng.choice(technologies)\n",
        "\n",
        "        # Choose a language and code snippet if needed\n",
        "        language = rng.choice(list(code_snippets.keys()))\n",
        "        code_snippet = rng.choice(code_snippets[language])\n",
        "\n",
        "        # Format the question\n",
        "        body = template.format(\n",
        "            action=action,\n",
        "            problem=problem,\n",
        "            technology=technology,\n",
        "            language=language,\n",
        "            code_snippet=code_snippet,\n",
        "            approach=f\"using {rng.choice(technologies)}\",\n",
        "            concept=rng.choice([\n",
        "                \"asynchronous programming\", \"dependency injection\", \"state management\",\n",
        "                \"object-relational mapping\", \"middleware\", \"event propagation\"\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        # Create question entry\n",
        "        question = {\n",
        "            \"id\": f\"question_{i}\",\n",
        "            \"title\": title,\n",
        "            \"body\": body,\n",
        "            \"tags\": tags,\n",
        "            \"score\": rng.randint(-2, 50),\n",
        "            \"view_count\": rng.randint(10, 5000),\n",
        "            \"answer_count\": rng.randint(0, 10),\n",
        "            \"creation_date\": f\"2023-{rng.randint(1, 12):02d}-{rng.randint(1, 28):02d}\"\n",
        "        }\n",
        "\n",
        "        questions.append(question)\n",
        "\n",
        "    return pd.DataFrame(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqdtmo8HcTwN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_configuration(config_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load pipeline configuration from a JSON file.\"\"\"\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "def save_results(results, output_path: str):\n",
        "    \"\"\"Save results to a file.\"\"\"\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if output_dir:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        if isinstance(results, dict):\n",
        "            json.dump(results, f, indent=2)\n",
        "        else:\n",
        "            f.write(results)\n",
        "\n",
        "    logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    *,\n",
        "    text: Optional[str] = None,\n",
        "    file_path: Optional[str] = None,\n",
        "    dataset: Optional[str] = None,\n",
        "    num_samples: int = 5,\n",
        "    config_path: Optional[str] = None,\n",
        "    hf_token: str = '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',\n",
        "    max_tokens: int = 256,\n",
        "    prompt_template: Optional[str] = None,\n",
        "    output_path: str = \"pipeline_output.json\",\n",
        "    all_stages: bool = False,\n",
        "):\n",
        "    \"\"\"Run the Privacy Filter Pipeline without relying on argparse.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, optional\n",
        "        Direct text to process through the pipeline.\n",
        "    file_path : str, optional\n",
        "        Path to a file whose contents will be processed.\n",
        "    dataset : {\"enron\", \"stackoverflow\"}, optional\n",
        "        Name of a sample dataset to load. Exactly one of ``text``, ``file_path``, or\n",
        "        ``dataset`` should be provided.\n",
        "    num_samples : int, default 5\n",
        "        Number of samples to draw from the dataset if ``dataset`` is chosen.\n",
        "    config_path : str, optional\n",
        "        Path to a JSON configuration file that contains pipeline settings.\n",
        "    hf_token : str, optional\n",
        "        Hugging Face token. If ``None``, the code will fall back to the\n",
        "        ``HF_TOKEN`` environment variable.\n",
        "    max_tokens : int, default 256\n",
        "        Maximum length for generated text.\n",
        "    prompt_template : str, optional\n",
        "        Custom generation prompt; must contain the ``{text}`` placeholder.\n",
        "    output_path : str, default \"pipeline_output.json\"\n",
        "        Where to write the results.\n",
        "    all_stages : bool, default False\n",
        "        If ``True``, return intermediate representations (pre‑filter, generated,\n",
        "        etc.) rather than just the final text.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Sanity checks -----------------------------------------------------\n",
        "    supplied = [arg is not None for arg in (text, file_path, dataset)]\n",
        "    if sum(supplied) != 1:\n",
        "        raise ValueError(\"Provide exactly one of `text`, `file_path`, or `dataset`.\")\n",
        "\n",
        "    # --- Token & configuration -------------------------------------------\n",
        "    if not hf_token:\n",
        "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"A HuggingFace token must be supplied via `hf_token` or the HF_TOKEN environment variable.\")\n",
        "\n",
        "    config = {}\n",
        "    if config_path:\n",
        "        config = load_configuration(config_path)\n",
        "\n",
        "    # Build the pipeline‑specific kwargs\n",
        "    pipeline_args = {\n",
        "        \"hf_auth_token\": hf_token,\n",
        "        **{k: config[k] for k in (\"pre_filter_config\", \"generator_config\", \"post_filter_config\", \"model_name\", \"sensitive_items\") if k in config}\n",
        "    }\n",
        "\n",
        "    logger.info(\"Initializing Privacy Filter Pipeline …\")\n",
        "    pipeline = PrivacyFilterPipeline(**pipeline_args)\n",
        "\n",
        "    # --- Input acquisition -------------------------------------------------\n",
        "    if text is not None:\n",
        "        input_text = text\n",
        "        process_single = True\n",
        "    elif file_path is not None:\n",
        "        with open(file_path, \"r\", encoding=\"utf‑8\") as f:\n",
        "            input_text = f.read()\n",
        "        process_single = True\n",
        "    else:  # dataset branch\n",
        "        process_single = False\n",
        "        if dataset == \"enron\":\n",
        "            logger.info(f\"Loading {num_samples} samples from Enron dataset …\")\n",
        "            data = load_enron_sample(num_samples=num_samples)\n",
        "            text_column = \"body\"\n",
        "        elif dataset == \"stackoverflow\":\n",
        "            logger.info(f\"Loading {num_samples} samples from Stack Overflow dataset …\")\n",
        "            data = load_stackoverflow_sample(num_samples=num_samples)\n",
        "            text_column = \"body\"\n",
        "        else:\n",
        "            raise ValueError(\"Dataset must be either 'enron' or 'stackoverflow'.\")\n",
        "\n",
        "    # --- Processing --------------------------------------------------------\n",
        "    if process_single:\n",
        "        logger.info(\"Processing single text through the pipeline …\")\n",
        "        result = pipeline.process(\n",
        "            text=input_text,\n",
        "            prompt_template=prompt_template,\n",
        "            max_new_tokens=max_tokens,\n",
        "            return_all_stages=all_stages,\n",
        "        )\n",
        "\n",
        "        _pretty_print_single_result(input_text, result, all_stages)\n",
        "        save_results(result, output_path)\n",
        "    else:\n",
        "        logger.info(\"Processing dataset through the pipeline …\")\n",
        "\n",
        "        injector = SensitiveDataInjector(\n",
        "            pii_injection_rate=0.8,  # High rate for demonstration\n",
        "            canary_injection_rate=0.3,\n",
        "        )\n",
        "        processed_data, stats = injector.process_dataset(\n",
        "            input_data=data,\n",
        "            text_column=text_column,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Injected PII into %d samples, canaries into %d samples\", stats[\"pii_injections\"], stats[\"canary_injections\"])\n",
        "\n",
        "        result_df = pipeline.process_dataframe(\n",
        "            df=processed_data,\n",
        "            text_column=text_column,\n",
        "            output_column=\"processed_text\",\n",
        "            all_stages_column=\"pipeline_stages\" if all_stages else None,\n",
        "            prompt_template=prompt_template,\n",
        "            max_new_tokens=max_tokens,\n",
        "        )\n",
        "\n",
        "        _pretty_print_dataframe_samples(result_df, text_column)\n",
        "        result_df.to_json(output_path, orient=\"records\", indent=2)\n",
        "        logger.info(\"Results saved to %s\", output_path)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Helper utilities\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _pretty_print_single_result(original: str, result, show_all: bool):\n",
        "    if show_all:\n",
        "        print(\"\\n--- Original Text ---\\n\", original)\n",
        "        print(\"\\n--- Pre‑Filtered Text ---\\n\", result[\"pre_filtered_text\"])\n",
        "        print(\"\\n--- Generated Text ---\\n\", result[\"generated_text\"])\n",
        "        print(\"\\n--- Final Text ---\\n\", result[\"final_text\"])\n",
        "        print(\"\\n--- Pipeline Stats ---\\n\", json.dumps(result[\"stats\"][\"pipeline\"], indent=2))\n",
        "    else:\n",
        "        print(\"\\n--- Original Text ---\\n\", original)\n",
        "        print(\"\\n--- Processed Text ---\\n\", result)\n",
        "        print(\"\\n--- Pipeline Stats ---\\n\", json.dumps(result.get(\"pipeline_stats\", {}), indent=2))\n",
        "\n",
        "\n",
        "def _pretty_print_dataframe_samples(df, text_column: str, n: int = 3):\n",
        "    print(\"\\n--- Sample Results ---\")\n",
        "    for i in range(min(n, len(df))):\n",
        "        row = df.iloc[i]\n",
        "        orig = row[text_column]\n",
        "        proc = row[\"processed_text\"]\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(\"Original:\")\n",
        "        print((orig[:500] + \"…\") if len(orig) > 500 else orig)\n",
        "        print(\"\\nProcessed:\")\n",
        "        print((proc[:500] + \"…\") if len(proc) > 500 else proc)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntlfPM69do-v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PrivacyEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluates privacy metrics for synthetic text generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        similarity_thresholds: List[float] = None,\n",
        "        device: str = None,\n",
        "        batch_size: int = 32,\n",
        "        use_spacy: bool = True,\n",
        "        spacy_model: str = \"en_core_web_sm\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the privacy evaluator.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: SentenceTransformer model to use for embeddings.\n",
        "            similarity_thresholds: List of thresholds for similarity analysis.\n",
        "            device: Device to use for embeddings (cuda, cpu, mps).\n",
        "            batch_size: Batch size for computing embeddings.\n",
        "            use_spacy: Whether to use spaCy for entity extraction.\n",
        "            spacy_model: spaCy model to use for entity extraction.\n",
        "        \"\"\"\n",
        "        self.similarity_thresholds = similarity_thresholds or [0.8, 0.85, 0.9, 0.95, 0.98]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Load embedding model\n",
        "        try:\n",
        "            logger.info(f\"Loading embedding model: {embedding_model}\")\n",
        "            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load embedding model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Load spaCy model\n",
        "        self.use_spacy = use_spacy\n",
        "        if use_spacy:\n",
        "            try:\n",
        "                self.nlp = spacy.load(spacy_model)\n",
        "                logger.info(f\"Loaded spaCy model: {spacy_model}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load spaCy model: {str(e)}\")\n",
        "                logger.warning(\"Downloading spaCy model...\")\n",
        "                spacy.cli.download(spacy_model)\n",
        "                self.nlp = spacy.load(spacy_model)\n",
        "\n",
        "    def evaluate_pii_leakage(\n",
        "        self,\n",
        "        original_data: pd.DataFrame,\n",
        "        generated_texts: List[str],\n",
        "        text_column: str,\n",
        "        pii_metadata_column: str = \"pii_metadata\",\n",
        "        ignore_case: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate PII leakage rates in generated texts.\n",
        "\n",
        "        Args:\n",
        "            original_data: DataFrame with original data and PII metadata.\n",
        "            generated_texts: List of generated texts to evaluate.\n",
        "            text_column: Column containing the text in original_data.\n",
        "            pii_metadata_column: Column containing PII metadata in original_data.\n",
        "            ignore_case: Whether to ignore case when matching PII.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with PII leakage metrics.\n",
        "        \"\"\"\n",
        "        if len(original_data) != len(generated_texts):\n",
        "            raise ValueError(\"Number of original samples and generated texts must match\")\n",
        "\n",
        "        # Track PII leakage\n",
        "        total_samples = len(original_data)\n",
        "        total_with_pii = 0\n",
        "        total_leaked = 0\n",
        "        pii_types_leaked = {}\n",
        "        leaked_samples = []\n",
        "\n",
        "        # Evaluate each sample\n",
        "        for i, (_, row) in enumerate(original_data.iterrows()):\n",
        "            # Skip samples without PII\n",
        "            pii_metadata = row.get(pii_metadata_column, {})\n",
        "            if not pii_metadata or not isinstance(pii_metadata, dict) or not pii_metadata.get(\"injected\", False):\n",
        "                continue\n",
        "\n",
        "            total_with_pii += 1\n",
        "            generated_text = generated_texts[i]\n",
        "\n",
        "            # Check for PII leakage\n",
        "            sample_leaked = False\n",
        "            leaked_pii_types = []\n",
        "            leaked_values = []\n",
        "\n",
        "            for injection in pii_metadata.get(\"injections\", []):\n",
        "                pii_type = injection.get(\"type\", \"unknown\")\n",
        "                pii_value = injection.get(\"value\", \"\")\n",
        "\n",
        "                # Skip empty values\n",
        "                if not pii_value:\n",
        "                    continue\n",
        "\n",
        "                # Check if the PII value is in the generated text\n",
        "                if ignore_case:\n",
        "                    found = pii_value.lower() in generated_text.lower()\n",
        "                else:\n",
        "                    found = pii_value in generated_text\n",
        "\n",
        "                if found:\n",
        "                    sample_leaked = True\n",
        "                    leaked_pii_types.append(pii_type)\n",
        "                    leaked_values.append(pii_value)\n",
        "\n",
        "                    # Update type-specific counts\n",
        "                    pii_types_leaked[pii_type] = pii_types_leaked.get(pii_type, 0) + 1\n",
        "\n",
        "            # Record if this sample leaked PII\n",
        "            if sample_leaked:\n",
        "                total_leaked += 1\n",
        "                leaked_samples.append({\n",
        "                    \"sample_idx\": i,\n",
        "                    \"pii_types\": leaked_pii_types,\n",
        "                    \"leaked_values\": leaked_values\n",
        "                })\n",
        "\n",
        "        # Calculate metrics\n",
        "        if total_with_pii > 0:\n",
        "            leakage_rate = total_leaked / total_with_pii\n",
        "            per_type_rate = {pii_type: count / total_with_pii for pii_type, count in pii_types_leaked.items()}\n",
        "        else:\n",
        "            leakage_rate = 0.0\n",
        "            per_type_rate = {}\n",
        "\n",
        "        return {\n",
        "            \"total_samples\": total_samples,\n",
        "            \"samples_with_pii\": total_with_pii,\n",
        "            \"samples_leaked_pii\": total_leaked,\n",
        "            \"pii_leakage_rate\": leakage_rate,\n",
        "            \"per_type_leakage_rate\": per_type_rate,\n",
        "            \"leaked_samples\": leaked_samples\n",
        "        }\n",
        "\n",
        "    def evaluate_canary_leakage(\n",
        "        self,\n",
        "        original_data: pd.DataFrame,\n",
        "        generated_texts: List[str],\n",
        "        text_column: str,\n",
        "        canary_metadata_column: str = \"canary_metadata\",\n",
        "        similarity_threshold: float = 0.95,\n",
        "        exact_match: bool = True,\n",
        "        ignore_case: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate canary phrase leakage rates in generated texts.\n",
        "\n",
        "        Args:\n",
        "            original_data: DataFrame with original data and canary metadata.\n",
        "            generated_texts: List of generated texts to evaluate.\n",
        "            text_column: Column containing the text in original_data.\n",
        "            canary_metadata_column: Column containing canary metadata in original_data.\n",
        "            similarity_threshold: Threshold for similarity-based leakage detection.\n",
        "            exact_match: Whether to require exact matches (vs. similarity).\n",
        "            ignore_case: Whether to ignore case when matching canaries.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with canary leakage metrics.\n",
        "        \"\"\"\n",
        "        if len(original_data) != len(generated_texts):\n",
        "            raise ValueError(\"Number of original samples and generated texts must match\")\n",
        "\n",
        "        # Track canary leakage\n",
        "        total_samples = len(original_data)\n",
        "        total_with_canary = 0\n",
        "        total_leaked_exact = 0\n",
        "        total_leaked_similar = 0\n",
        "        canary_phrase_counts = {}\n",
        "        leaked_samples = []\n",
        "\n",
        "        # Get all unique canary phrases for batch embedding\n",
        "        all_canaries = set()\n",
        "        for _, row in original_data.iterrows():\n",
        "            canary_metadata = row.get(canary_metadata_column, {})\n",
        "            if canary_metadata and isinstance(canary_metadata, dict) and canary_metadata.get(\"injected\", False):\n",
        "                canary = canary_metadata.get(\"canary\", \"\")\n",
        "                if canary:\n",
        "                    all_canaries.add(canary)\n",
        "\n",
        "        # Compute embeddings for all canary phrases\n",
        "        if all_canaries and not exact_match:\n",
        "            canary_phrases = list(all_canaries)\n",
        "            canary_embeddings = self.embedding_model.encode(\n",
        "                canary_phrases,\n",
        "                batch_size=self.batch_size,\n",
        "                show_progress_bar=len(canary_phrases) > 10\n",
        "            )\n",
        "            canary_embed_map = {phrase: embedding for phrase, embedding in zip(canary_phrases, canary_embeddings)}\n",
        "        else:\n",
        "            canary_embed_map = {}\n",
        "\n",
        "        # Evaluate each sample\n",
        "        for i, (_, row) in enumerate(original_data.iterrows()):\n",
        "            # Skip samples without canary phrases\n",
        "            canary_metadata = row.get(canary_metadata_column, {})\n",
        "            if not canary_metadata or not isinstance(canary_metadata, dict) or not canary_metadata.get(\"injected\", False):\n",
        "                continue\n",
        "\n",
        "            total_with_canary += 1\n",
        "            generated_text = generated_texts[i]\n",
        "            canary_phrase = canary_metadata.get(\"canary\", \"\")\n",
        "\n",
        "            # Skip empty canary phrases\n",
        "            if not canary_phrase:\n",
        "                continue\n",
        "\n",
        "            # Update canary phrase counts\n",
        "            canary_phrase_counts[canary_phrase] = canary_phrase_counts.get(canary_phrase, 0) + 1\n",
        "\n",
        "            # Check for exact match\n",
        "            leaked_exact = False\n",
        "            if ignore_case:\n",
        "                if canary_phrase.lower() in generated_text.lower():\n",
        "                    leaked_exact = True\n",
        "            else:\n",
        "                if canary_phrase in generated_text:\n",
        "                    leaked_exact = True\n",
        "\n",
        "            # Check for similarity-based leakage if exact match wasn't found\n",
        "            leaked_similar = False\n",
        "            similarity_score = 0.0\n",
        "\n",
        "            if not leaked_exact and not exact_match:\n",
        "                # Get embedding for generated text\n",
        "                gen_embedding = self.embedding_model.encode(generated_text, batch_size=1)\n",
        "\n",
        "                # Get embedding for this canary phrase\n",
        "                canary_embedding = canary_embed_map.get(canary_phrase)\n",
        "\n",
        "                if canary_embedding is not None:\n",
        "                    # Calculate similarity\n",
        "                    similarity_score = cosine_similarity(\n",
        "                        [gen_embedding],\n",
        "                        [canary_embedding]\n",
        "                    )[0][0]\n",
        "\n",
        "                    if similarity_score >= similarity_threshold:\n",
        "                        leaked_similar = True\n",
        "\n",
        "            # Record leakage\n",
        "            if leaked_exact:\n",
        "                total_leaked_exact += 1\n",
        "                leaked_samples.append({\n",
        "                    \"sample_idx\": i,\n",
        "                    \"canary_phrase\": canary_phrase,\n",
        "                    \"match_type\": \"exact\",\n",
        "                    \"similarity\": 1.0\n",
        "                })\n",
        "            elif leaked_similar:\n",
        "                total_leaked_similar += 1\n",
        "                leaked_samples.append({\n",
        "                    \"sample_idx\": i,\n",
        "                    \"canary_phrase\": canary_phrase,\n",
        "                    \"match_type\": \"similar\",\n",
        "                    \"similarity\": float(similarity_score)\n",
        "                })\n",
        "\n",
        "        # Calculate metrics\n",
        "        if total_with_canary > 0:\n",
        "            exact_leakage_rate = total_leaked_exact / total_with_canary\n",
        "            similar_leakage_rate = total_leaked_similar / total_with_canary\n",
        "            total_leakage_rate = (total_leaked_exact + total_leaked_similar) / total_with_canary\n",
        "        else:\n",
        "            exact_leakage_rate = 0.0\n",
        "            similar_leakage_rate = 0.0\n",
        "            total_leakage_rate = 0.0\n",
        "\n",
        "        return {\n",
        "            \"total_samples\": total_samples,\n",
        "            \"samples_with_canary\": total_with_canary,\n",
        "            \"samples_leaked_exact\": total_leaked_exact,\n",
        "            \"samples_leaked_similar\": total_leaked_similar,\n",
        "            \"exact_leakage_rate\": exact_leakage_rate,\n",
        "            \"similar_leakage_rate\": similar_leakage_rate,\n",
        "            \"total_leakage_rate\": total_leakage_rate,\n",
        "            \"canary_phrase_counts\": canary_phrase_counts,\n",
        "            \"leaked_samples\": leaked_samples\n",
        "        }\n",
        "\n",
        "    def evaluate_similarity_distribution(\n",
        "        self,\n",
        "        original_data: pd.DataFrame,\n",
        "        generated_texts: List[str],\n",
        "        text_column: str,\n",
        "        analyze_chunks: bool = True,\n",
        "        chunk_size: int = 100\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate similarity between original and generated texts.\n",
        "\n",
        "        Args:\n",
        "            original_data: DataFrame with original data.\n",
        "            generated_texts: List of generated texts to evaluate.\n",
        "            text_column: Column containing the text in original_data.\n",
        "            analyze_chunks: Whether to analyze text chunks for more granular similarity.\n",
        "            chunk_size: Size of text chunks for chunk-based analysis.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with similarity metrics.\n",
        "        \"\"\"\n",
        "        if len(original_data) != len(generated_texts):\n",
        "            raise ValueError(\"Number of original samples and generated texts must match\")\n",
        "\n",
        "        # Extract original texts\n",
        "        original_texts = original_data[text_column].tolist()\n",
        "\n",
        "        # Compute embeddings for whole texts\n",
        "        logger.info(\"Computing embeddings for original texts...\")\n",
        "        original_embeddings = self.embedding_model.encode(\n",
        "            original_texts,\n",
        "            batch_size=self.batch_size,\n",
        "            show_progress_bar=len(original_texts) > 10\n",
        "        )\n",
        "\n",
        "        logger.info(\"Computing embeddings for generated texts...\")\n",
        "        generated_embeddings = self.embedding_model.encode(\n",
        "            generated_texts,\n",
        "            batch_size=self.batch_size,\n",
        "            show_progress_bar=len(generated_texts) > 10\n",
        "        )\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for i in range(len(original_embeddings)):\n",
        "            similarity = cosine_similarity(\n",
        "                [original_embeddings[i]],\n",
        "                [generated_embeddings[i]]\n",
        "            )[0][0]\n",
        "            similarities.append(float(similarity))\n",
        "\n",
        "        # Calculate metrics for whole texts\n",
        "        whole_text_metrics = {\n",
        "            \"mean_similarity\": float(np.mean(similarities)),\n",
        "            \"median_similarity\": float(np.median(similarities)),\n",
        "            \"min_similarity\": float(np.min(similarities)),\n",
        "            \"max_similarity\": float(np.max(similarities)),\n",
        "            \"std_deviation\": float(np.std(similarities))\n",
        "        }\n",
        "\n",
        "        # Calculate thresholded counts\n",
        "        threshold_counts = {}\n",
        "        for threshold in self.similarity_thresholds:\n",
        "            count = sum(1 for sim in similarities if sim >= threshold)\n",
        "            rate = count / len(similarities) if similarities else 0\n",
        "            threshold_counts[f\"above_{threshold:.2f}\"] = {\n",
        "                \"count\": count,\n",
        "                \"rate\": rate\n",
        "            }\n",
        "\n",
        "        # Analyze chunks if requested\n",
        "        chunk_metrics = {}\n",
        "        if analyze_chunks:\n",
        "            chunk_similarities = []\n",
        "            high_similarity_chunks = []\n",
        "\n",
        "            for i, (orig_text, gen_text) in enumerate(zip(original_texts, generated_texts)):\n",
        "                # Skip if either text is too short\n",
        "                if len(orig_text) < chunk_size or len(gen_text) < chunk_size:\n",
        "                    continue\n",
        "\n",
        "                # Create chunks\n",
        "                orig_chunks = [orig_text[j:j+chunk_size] for j in range(0, len(orig_text), chunk_size)]\n",
        "                gen_chunks = [gen_text[j:j+chunk_size] for j in range(0, len(gen_text), chunk_size)]\n",
        "\n",
        "                # Calculate similarities between chunks\n",
        "                for orig_idx, orig_chunk in enumerate(orig_chunks):\n",
        "                    orig_embed = self.embedding_model.encode(orig_chunk, batch_size=1)\n",
        "\n",
        "                    for gen_idx, gen_chunk in enumerate(gen_chunks):\n",
        "                        gen_embed = self.embedding_model.encode(gen_chunk, batch_size=1)\n",
        "\n",
        "                        chunk_sim = cosine_similarity([orig_embed], [gen_embed])[0][0]\n",
        "                        chunk_similarities.append(float(chunk_sim))\n",
        "\n",
        "                        # Record high similarity chunks\n",
        "                        if chunk_sim >= 0.95:\n",
        "                            high_similarity_chunks.append({\n",
        "                                \"sample_idx\": i,\n",
        "                                \"orig_chunk_idx\": orig_idx,\n",
        "                                \"gen_chunk_idx\": gen_idx,\n",
        "                                \"orig_chunk\": orig_chunk[:100] + \"...\" if len(orig_chunk) > 100 else orig_chunk,\n",
        "                                \"gen_chunk\": gen_chunk[:100] + \"...\" if len(gen_chunk) > 100 else gen_chunk,\n",
        "                                \"similarity\": float(chunk_sim)\n",
        "                            })\n",
        "\n",
        "            # Calculate chunk-level metrics\n",
        "            if chunk_similarities:\n",
        "                chunk_metrics = {\n",
        "                    \"mean_chunk_similarity\": float(np.mean(chunk_similarities)),\n",
        "                    \"median_chunk_similarity\": float(np.median(chunk_similarities)),\n",
        "                    \"min_chunk_similarity\": float(np.min(chunk_similarities)),\n",
        "                    \"max_chunk_similarity\": float(np.max(chunk_similarities)),\n",
        "                    \"std_deviation\": float(np.std(chunk_similarities)),\n",
        "                    \"high_similarity_chunks\": high_similarity_chunks[:20]  # Limit to first 20\n",
        "                }\n",
        "\n",
        "                # Calculate thresholded counts for chunks\n",
        "                chunk_threshold_counts = {}\n",
        "                for threshold in self.similarity_thresholds:\n",
        "                    count = sum(1 for sim in chunk_similarities if sim >= threshold)\n",
        "                    rate = count / len(chunk_similarities)\n",
        "                    chunk_threshold_counts[f\"above_{threshold:.2f}\"] = {\n",
        "                        \"count\": count,\n",
        "                        \"rate\": rate\n",
        "                    }\n",
        "\n",
        "                chunk_metrics[\"threshold_counts\"] = chunk_threshold_counts\n",
        "\n",
        "        return {\n",
        "            \"whole_text_metrics\": whole_text_metrics,\n",
        "            \"threshold_counts\": threshold_counts,\n",
        "            \"chunk_metrics\": chunk_metrics,\n",
        "            \"all_similarities\": similarities\n",
        "        }\n",
        "\n",
        "    def run_adversarial_memorization_test(\n",
        "        self,\n",
        "        canary_phrases: List[str],\n",
        "        adversarial_prompts: List[str],\n",
        "        generated_texts: List[str],\n",
        "        exact_match: bool = True,\n",
        "        ignore_case: bool = True,\n",
        "        similarity_threshold: float = 0.98\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate resistance to adversarial prompts designed to elicit memorized content.\n",
        "\n",
        "        Args:\n",
        "            canary_phrases: List of canary phrases to test for.\n",
        "            adversarial_prompts: List of prompts designed to elicit the canaries.\n",
        "            generated_texts: List of texts generated from the adversarial prompts.\n",
        "            exact_match: Whether to require exact matches (vs. similarity).\n",
        "            ignore_case: Whether to ignore case when matching canaries.\n",
        "            similarity_threshold: Threshold for similarity-based leakage detection.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with adversarial test metrics.\n",
        "        \"\"\"\n",
        "        if len(adversarial_prompts) != len(generated_texts):\n",
        "            raise ValueError(\"Number of adversarial prompts and generated texts must match\")\n",
        "\n",
        "        # Compute embeddings for canary phrases\n",
        "        canary_embeddings = None\n",
        "        if not exact_match:\n",
        "            canary_embeddings = self.embedding_model.encode(\n",
        "                canary_phrases,\n",
        "                batch_size=self.batch_size,\n",
        "                show_progress_bar=len(canary_phrases) > 10\n",
        "            )\n",
        "\n",
        "        # Track leakage\n",
        "        total_prompts = len(adversarial_prompts)\n",
        "        leaked_canaries = []\n",
        "\n",
        "        for i, (prompt, generated_text) in enumerate(zip(adversarial_prompts, generated_texts)):\n",
        "            # Check each canary phrase\n",
        "            for j, canary in enumerate(canary_phrases):\n",
        "                # Check for exact match\n",
        "                leaked_exact = False\n",
        "                if ignore_case:\n",
        "                    if canary.lower() in generated_text.lower():\n",
        "                        leaked_exact = True\n",
        "                else:\n",
        "                    if canary in generated_text:\n",
        "                        leaked_exact = True\n",
        "\n",
        "                # Check for similarity-based leakage if exact match wasn't found\n",
        "                leaked_similar = False\n",
        "                similarity_score = 0.0\n",
        "\n",
        "                if not leaked_exact and not exact_match:\n",
        "                    # Get embedding for generated text\n",
        "                    gen_embedding = self.embedding_model.encode(generated_text, batch_size=1)\n",
        "\n",
        "                    # Calculate similarity\n",
        "                    if canary_embeddings is not None:\n",
        "                        similarity_score = cosine_similarity(\n",
        "                            [gen_embedding],\n",
        "                            [canary_embeddings[j]]\n",
        "                        )[0][0]\n",
        "\n",
        "                        if similarity_score >= similarity_threshold:\n",
        "                            leaked_similar = True\n",
        "\n",
        "                # Record leakage\n",
        "                if leaked_exact or leaked_similar:\n",
        "                    leaked_canaries.append({\n",
        "                        \"prompt_idx\": i,\n",
        "                        \"canary_idx\": j,\n",
        "                        \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
        "                        \"canary\": canary[:100] + \"...\" if len(canary) > 100 else canary,\n",
        "                        \"match_type\": \"exact\" if leaked_exact else \"similar\",\n",
        "                        \"similarity\": 1.0 if leaked_exact else float(similarity_score)\n",
        "                    })\n",
        "\n",
        "        # Calculate metrics\n",
        "        success_rate = len(leaked_canaries) / (total_prompts * len(canary_phrases))\n",
        "\n",
        "        return {\n",
        "            \"total_tests\": total_prompts * len(canary_phrases),\n",
        "            \"successful_elicitations\": len(leaked_canaries),\n",
        "            \"success_rate\": success_rate,\n",
        "            \"leaked_canaries\": leaked_canaries\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrrS2Qz1y1qV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple, Set, Any, Union, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "    ROUGE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ROUGE_AVAILABLE = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class UtilityEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluates utility metrics for synthetic text generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        use_perplexity: bool = True,\n",
        "        gpt2_model: str = \"gpt2\",\n",
        "        device: str = None,\n",
        "        batch_size: int = 8\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the utility evaluator.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: SentenceTransformer model to use for embeddings.\n",
        "            use_perplexity: Whether to calculate perplexity metrics.\n",
        "            gpt2_model: GPT-2 model to use for perplexity calculation.\n",
        "            device: Device to use for models (cuda, cpu, mps).\n",
        "            batch_size: Batch size for computing embeddings and perplexity.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Determine device\n",
        "        if device is None:\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = \"cuda\"\n",
        "            elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "                self.device = \"mps\"\n",
        "            else:\n",
        "                self.device = \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load embedding model\n",
        "        try:\n",
        "            logger.info(f\"Loading embedding model: {embedding_model}\")\n",
        "            self.embedding_model = SentenceTransformer(embedding_model, device=self.device)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load embedding model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Load GPT-2 model if perplexity is requested\n",
        "        self.use_perplexity = use_perplexity\n",
        "        if use_perplexity:\n",
        "            try:\n",
        "                logger.info(f\"Loading GPT-2 model: {gpt2_model}\")\n",
        "                self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(gpt2_model)\n",
        "                # Set pad token to eos token\n",
        "                self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n",
        "                self.gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model).to(self.device)\n",
        "                self.gpt2_model.eval()\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load GPT-2 model: {str(e)}\")\n",
        "                self.use_perplexity = False\n",
        "                logger.warning(\"Perplexity calculations will be skipped\")\n",
        "\n",
        "    def calculate_perplexity(self, texts: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calculate perplexity for a list of texts using GPT-2.\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to calculate perplexity for.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with perplexity metrics.\n",
        "        \"\"\"\n",
        "        if not self.use_perplexity:\n",
        "            return {\"error\": \"Perplexity calculation is disabled\"}\n",
        "\n",
        "        # Calculate perplexity\n",
        "        perplexities = []\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch_texts = texts[i:i+self.batch_size]\n",
        "\n",
        "            encodings = self.gpt2_tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=1024\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # We need to provide labels for loss calculation\n",
        "                input_ids = encodings.input_ids\n",
        "                outputs = self.gpt2_model(input_ids=input_ids, labels=input_ids)\n",
        "                if outputs.loss is not None:\n",
        "                    log_likelihood = outputs.loss.item() * input_ids.size(1)\n",
        "                else:\n",
        "                    # Skip this batch if loss is None\n",
        "                    continue\n",
        "\n",
        "                # Calculate perplexity for each text in the batch\n",
        "                for j, text in enumerate(batch_texts):\n",
        "                    # Tokenize individual text\n",
        "                    input_ids = self.gpt2_tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "                    # Skip empty texts\n",
        "                    if input_ids.size(1) <= 1:\n",
        "                        continue\n",
        "\n",
        "                    # Calculate perplexity\n",
        "                    with torch.no_grad():\n",
        "                        outputs = self.gpt2_model(input_ids, labels=input_ids)\n",
        "                        neg_log_likelihood = outputs.loss.item()\n",
        "                        ppl = math.exp(neg_log_likelihood)\n",
        "                        perplexities.append(ppl)\n",
        "\n",
        "        # Calculate metrics\n",
        "        if perplexities:\n",
        "            result = {\n",
        "                \"mean_perplexity\": float(np.mean(perplexities)),\n",
        "                \"median_perplexity\": float(np.median(perplexities)),\n",
        "                \"min_perplexity\": float(np.min(perplexities)),\n",
        "                \"max_perplexity\": float(np.max(perplexities)),\n",
        "                \"std_deviation\": float(np.std(perplexities))\n",
        "            }\n",
        "        else:\n",
        "            result = {\"error\": \"Failed to calculate perplexity for any text\"}\n",
        "\n",
        "        return result\n",
        "\n",
        "    def calculate_semantic_similarity(\n",
        "        self,\n",
        "        original_texts: List[str],\n",
        "        generated_texts: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calculate semantic similarity between original and generated texts.\n",
        "\n",
        "        Args:\n",
        "            original_texts: List of original texts.\n",
        "            generated_texts: List of generated texts.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with semantic similarity metrics.\n",
        "        \"\"\"\n",
        "        if len(original_texts) != len(generated_texts):\n",
        "            return {\"error\": \"Number of original and generated texts must match\"}\n",
        "\n",
        "        # Calculate embeddings\n",
        "        logger.info(\"Computing embeddings for original texts...\")\n",
        "        original_embeddings = self.embedding_model.encode(\n",
        "            original_texts,\n",
        "            batch_size=self.batch_size,\n",
        "            show_progress_bar=len(original_texts) > 10\n",
        "        )\n",
        "\n",
        "        logger.info(\"Computing embeddings for generated texts...\")\n",
        "        generated_embeddings = self.embedding_model.encode(\n",
        "            generated_texts,\n",
        "            batch_size=self.batch_size,\n",
        "            show_progress_bar=len(generated_texts) > 10\n",
        "        )\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for i in range(len(original_embeddings)):\n",
        "            similarity = cosine_similarity(\n",
        "                [original_embeddings[i]],\n",
        "                [generated_embeddings[i]]\n",
        "            )[0][0]\n",
        "            similarities.append(float(similarity))\n",
        "\n",
        "        # Calculate metrics\n",
        "        result = {\n",
        "            \"mean_similarity\": float(np.mean(similarities)),\n",
        "            \"median_similarity\": float(np.median(similarities)),\n",
        "            \"min_similarity\": float(np.min(similarities)),\n",
        "            \"max_similarity\": float(np.max(similarities)),\n",
        "            \"std_deviation\": float(np.std(similarities))\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def calculate_rouge_scores(\n",
        "        self,\n",
        "        original_texts: List[str],\n",
        "        generated_texts: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calculate ROUGE scores between original and generated texts.\n",
        "\n",
        "        Args:\n",
        "            original_texts: List of original texts.\n",
        "            generated_texts: List of generated texts.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with ROUGE score metrics.\n",
        "        \"\"\"\n",
        "        if not ROUGE_AVAILABLE:\n",
        "            return {\"error\": \"rouge_score library not available\"}\n",
        "\n",
        "        if len(original_texts) != len(generated_texts):\n",
        "            return {\"error\": \"Number of original and generated texts must match\"}\n",
        "\n",
        "        # Initialize ROUGE scorer\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "        # Calculate scores\n",
        "        scores = []\n",
        "        for original, generated in zip(original_texts, generated_texts):\n",
        "            score = scorer.score(original, generated)\n",
        "            scores.append({\n",
        "                'rouge1_precision': score['rouge1'].precision,\n",
        "                'rouge1_recall': score['rouge1'].recall,\n",
        "                'rouge1_fmeasure': score['rouge1'].fmeasure,\n",
        "                'rouge2_precision': score['rouge2'].precision,\n",
        "                'rouge2_recall': score['rouge2'].recall,\n",
        "                'rouge2_fmeasure': score['rouge2'].fmeasure,\n",
        "                'rougeL_precision': score['rougeL'].precision,\n",
        "                'rougeL_recall': score['rougeL'].recall,\n",
        "                'rougeL_fmeasure': score['rougeL'].fmeasure\n",
        "            })\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_scores = {}\n",
        "        for metric in scores[0].keys():\n",
        "            avg_scores[metric] = float(np.mean([s[metric] for s in scores]))\n",
        "\n",
        "        return {\n",
        "            \"average_scores\": avg_scores,\n",
        "            \"all_scores\": scores[:10]  # Return first 10 individual scores\n",
        "        }\n",
        "\n",
        "    def calculate_n_gram_diversity(\n",
        "        self,\n",
        "        original_texts: List[str],\n",
        "        generated_texts: List[str],\n",
        "        n_values: List[int] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calculate n-gram diversity statistics.\n",
        "\n",
        "        Args:\n",
        "            original_texts: List of original texts.\n",
        "            generated_texts: List of generated texts.\n",
        "            n_values: List of n-gram sizes to calculate.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with n-gram diversity metrics.\n",
        "        \"\"\"\n",
        "        n_values = n_values or [1, 2, 3]\n",
        "\n",
        "        def get_n_grams(texts: List[str], n: int) -> Counter:\n",
        "            \"\"\"Get n-gram counts for a list of texts.\"\"\"\n",
        "            n_grams = Counter()\n",
        "            for text in texts:\n",
        "                tokens = text.lower().split()\n",
        "                if len(tokens) < n:\n",
        "                    continue\n",
        "                text_n_grams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "                n_grams.update(text_n_grams)\n",
        "            return n_grams\n",
        "\n",
        "        # Calculate n-gram distributions\n",
        "        result = {}\n",
        "        for n in n_values:\n",
        "            original_n_grams = get_n_grams(original_texts, n)\n",
        "            generated_n_grams = get_n_grams(generated_texts, n)\n",
        "\n",
        "            # Calculate unique n-grams\n",
        "            unique_original = len(original_n_grams)\n",
        "            unique_generated = len(generated_n_grams)\n",
        "            total_original = sum(original_n_grams.values())\n",
        "            total_generated = sum(generated_n_grams.values())\n",
        "\n",
        "            # Calculate probabilities for Jensen-Shannon divergence\n",
        "            if total_original > 0 and total_generated > 0 and unique_original > 0 and unique_generated > 0:\n",
        "                # Get sets of n-grams\n",
        "                all_n_grams = set(original_n_grams.keys()) | set(generated_n_grams.keys())\n",
        "\n",
        "                # Calculate probability distributions\n",
        "                orig_probs = np.array([original_n_grams.get(n_gram, 0) / total_original for n_gram in all_n_grams])\n",
        "                gen_probs = np.array([generated_n_grams.get(n_gram, 0) / total_generated for n_gram in all_n_grams])\n",
        "\n",
        "                # Add smoothing for zero probabilities\n",
        "                orig_probs = np.maximum(orig_probs, 1e-10)\n",
        "                gen_probs = np.maximum(gen_probs, 1e-10)\n",
        "\n",
        "                # Normalize\n",
        "                orig_probs = orig_probs / np.sum(orig_probs)\n",
        "                gen_probs = gen_probs / np.sum(gen_probs)\n",
        "\n",
        "                # Calculate Jensen-Shannon divergence\n",
        "                jsd = jensenshannon(orig_probs, gen_probs)\n",
        "\n",
        "                # Calculate normalized entropy\n",
        "                generated_entropy = -np.sum(gen_probs * np.log2(gen_probs)) / np.log2(len(gen_probs))\n",
        "                original_entropy = -np.sum(orig_probs * np.log2(orig_probs)) / np.log2(len(orig_probs))\n",
        "\n",
        "                result[f\"{n}-gram\"] = {\n",
        "                    \"unique_original\": unique_original,\n",
        "                    \"unique_generated\": unique_generated,\n",
        "                    \"total_original\": total_original,\n",
        "                    \"total_generated\": total_generated,\n",
        "                    \"unique_ratio_original\": unique_original / total_original if total_original > 0 else 0,\n",
        "                    \"unique_ratio_generated\": unique_generated / total_generated if total_generated > 0 else 0,\n",
        "                    \"jensen_shannon_divergence\": float(jsd),\n",
        "                    \"original_entropy_normalized\": float(original_entropy),\n",
        "                    \"generated_entropy_normalized\": float(generated_entropy)\n",
        "                }\n",
        "            else:\n",
        "                result[f\"{n}-gram\"] = {\n",
        "                    \"error\": \"Insufficient data for n-gram diversity calculation\"\n",
        "                }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def calculate_output_statistics(\n",
        "        self,\n",
        "        original_texts: List[str],\n",
        "        generated_texts: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calculate basic output statistics.\n",
        "\n",
        "        Args:\n",
        "            original_texts: List of original texts.\n",
        "            generated_texts: List of generated texts.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with output statistics.\n",
        "        \"\"\"\n",
        "        # Calculate lengths\n",
        "        original_lengths = [len(text.split()) for text in original_texts]\n",
        "        generated_lengths = [len(text.split()) for text in generated_texts]\n",
        "\n",
        "        # Calculate character lengths\n",
        "        original_char_lengths = [len(text) for text in original_texts]\n",
        "        generated_char_lengths = [len(text) for text in generated_texts]\n",
        "\n",
        "        # Calculate lengths of sentences\n",
        "        sentence_pattern = r'[.!?]+\\s+'\n",
        "        original_sentence_counts = [len(re.split(sentence_pattern, text)) for text in original_texts]\n",
        "        generated_sentence_counts = [len(re.split(sentence_pattern, text)) for text in generated_texts]\n",
        "\n",
        "        # Count placeholder tokens\n",
        "        placeholder_pattern = r'\\[(REDACTED|PII|NAME|EMAIL|PHONE|ADDRESS|SSN|CREDIT_CARD|PERSON|ORG|GPE|LOC|\\w+_\\d+)\\]'\n",
        "        placeholder_counts = [len(re.findall(placeholder_pattern, text)) for text in generated_texts]\n",
        "\n",
        "        result = {\n",
        "            \"word_count\": {\n",
        "                \"original_mean\": float(np.mean(original_lengths)),\n",
        "                \"original_median\": float(np.median(original_lengths)),\n",
        "                \"original_min\": float(np.min(original_lengths)),\n",
        "                \"original_max\": float(np.max(original_lengths)),\n",
        "                \"original_std\": float(np.std(original_lengths)),\n",
        "                \"generated_mean\": float(np.mean(generated_lengths)),\n",
        "                \"generated_median\": float(np.median(generated_lengths)),\n",
        "                \"generated_min\": float(np.min(generated_lengths)),\n",
        "                \"generated_max\": float(np.max(generated_lengths)),\n",
        "                \"generated_std\": float(np.std(generated_lengths)),\n",
        "                \"ratio_mean\": float(np.mean(generated_lengths) / np.mean(original_lengths)) if np.mean(original_lengths) > 0 else 0\n",
        "            },\n",
        "            \"char_count\": {\n",
        "                \"original_mean\": float(np.mean(original_char_lengths)),\n",
        "                \"original_median\": float(np.median(original_char_lengths)),\n",
        "                \"original_min\": float(np.min(original_char_lengths)),\n",
        "                \"original_max\": float(np.max(original_char_lengths)),\n",
        "                \"original_std\": float(np.std(original_char_lengths)),\n",
        "                \"generated_mean\": float(np.mean(generated_char_lengths)),\n",
        "                \"generated_median\": float(np.median(generated_char_lengths)),\n",
        "                \"generated_min\": float(np.min(generated_char_lengths)),\n",
        "                \"generated_max\": float(np.max(generated_char_lengths)),\n",
        "                \"generated_std\": float(np.std(generated_char_lengths)),\n",
        "                \"ratio_mean\": float(np.mean(generated_char_lengths) / np.mean(original_char_lengths)) if np.mean(original_char_lengths) > 0 else 0\n",
        "            },\n",
        "            \"sentence_count\": {\n",
        "                \"original_mean\": float(np.mean(original_sentence_counts)),\n",
        "                \"original_median\": float(np.median(original_sentence_counts)),\n",
        "                \"original_min\": float(np.min(original_sentence_counts)),\n",
        "                \"original_max\": float(np.max(original_sentence_counts)),\n",
        "                \"original_std\": float(np.std(original_sentence_counts)),\n",
        "                \"generated_mean\": float(np.mean(generated_sentence_counts)),\n",
        "                \"generated_median\": float(np.median(generated_sentence_counts)),\n",
        "                \"generated_min\": float(np.min(generated_sentence_counts)),\n",
        "                \"generated_max\": float(np.max(generated_sentence_counts)),\n",
        "                \"generated_std\": float(np.std(generated_sentence_counts)),\n",
        "                \"ratio_mean\": float(np.mean(generated_sentence_counts) / np.mean(original_sentence_counts)) if np.mean(original_sentence_counts) > 0 else 0\n",
        "            },\n",
        "            \"placeholder_count\": {\n",
        "                \"mean\": float(np.mean(placeholder_counts)),\n",
        "                \"median\": float(np.median(placeholder_counts)),\n",
        "                \"min\": float(np.min(placeholder_counts)),\n",
        "                \"max\": float(np.max(placeholder_counts)),\n",
        "                \"std\": float(np.std(placeholder_counts)),\n",
        "                \"texts_with_placeholders\": sum(1 for count in placeholder_counts if count > 0),\n",
        "                \"texts_with_placeholders_rate\": sum(1 for count in placeholder_counts if count > 0) / len(generated_texts)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def calculate_all_metrics(\n",
        "        self,\n",
        "        original_texts: List[str],\n",
        "        generated_texts: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calculate all utility metrics.\n",
        "\n",
        "        Args:\n",
        "            original_texts: List of original texts.\n",
        "            generated_texts: List of generated texts.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all metrics.\n",
        "        \"\"\"\n",
        "        # Ensure the inputs have the same length\n",
        "        if len(original_texts) != len(generated_texts):\n",
        "            return {\"error\": \"Number of original and generated texts must match\"}\n",
        "\n",
        "        # Calculate perplexity\n",
        "        logger.info(\"Calculating perplexity...\")\n",
        "        perplexity_metrics = {}\n",
        "        if self.use_perplexity:\n",
        "            perplexity_metrics = self.calculate_perplexity(generated_texts)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        logger.info(\"Calculating semantic similarity...\")\n",
        "        similarity_metrics = self.calculate_semantic_similarity(original_texts, generated_texts)\n",
        "\n",
        "        # Calculate ROUGE scores\n",
        "        logger.info(\"Calculating ROUGE scores...\")\n",
        "        rouge_metrics = {}\n",
        "        if ROUGE_AVAILABLE:\n",
        "            rouge_metrics = self.calculate_rouge_scores(original_texts, generated_texts)\n",
        "\n",
        "        # Calculate n-gram diversity\n",
        "        logger.info(\"Calculating n-gram diversity...\")\n",
        "        n_gram_metrics = self.calculate_n_gram_diversity(original_texts, generated_texts)\n",
        "\n",
        "        # Calculate output statistics\n",
        "        logger.info(\"Calculating output statistics...\")\n",
        "        output_stats = self.calculate_output_statistics(original_texts, generated_texts)\n",
        "\n",
        "        # Combine all metrics\n",
        "        all_metrics = {\n",
        "            \"perplexity\": perplexity_metrics,\n",
        "            \"semantic_similarity\": similarity_metrics,\n",
        "            \"rouge_scores\": rouge_metrics,\n",
        "            \"n_gram_diversity\": n_gram_metrics,\n",
        "            \"output_statistics\": output_stats\n",
        "        }\n",
        "\n",
        "        return all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zf2j3Oky-bE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkSHgOOQzEj5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PrivacyFrameworkEvaluator:\n",
        "    \"\"\"\n",
        "    Main evaluation pipeline for the synthetic data privacy framework.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        privacy_evaluator: Optional[PrivacyEvaluator] = None,\n",
        "        utility_evaluator: Optional[UtilityEvaluator] = None,\n",
        "        device: str = None,\n",
        "        results_dir: str = \"results\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the privacy framework evaluator.\n",
        "\n",
        "        Args:\n",
        "            privacy_evaluator: Preconfigured PrivacyEvaluator instance.\n",
        "            utility_evaluator: Preconfigured UtilityEvaluator instance.\n",
        "            device: Device to use for evaluations (cuda, cpu, mps).\n",
        "            results_dir: Directory to store evaluation results.\n",
        "        \"\"\"\n",
        "        # Initialize evaluators if not provided\n",
        "        self.privacy_evaluator = privacy_evaluator or PrivacyEvaluator(device=device)\n",
        "        self.utility_evaluator = utility_evaluator or UtilityEvaluator(device=device)\n",
        "\n",
        "        # Setup results directory\n",
        "        self.results_dir = results_dir\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        # Store experimental configurations\n",
        "        self.configurations = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def add_configuration(\n",
        "        self,\n",
        "        config_id: str,\n",
        "        config_name: str,\n",
        "        config_description: str,\n",
        "        configuration: Dict[str, Any]\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Add a configuration to be evaluated.\n",
        "\n",
        "        Args:\n",
        "            config_id: Unique identifier for the configuration.\n",
        "            config_name: Short name for the configuration.\n",
        "            config_description: Description of the configuration.\n",
        "            configuration: Configuration parameters.\n",
        "        \"\"\"\n",
        "        self.configurations[config_id] = {\n",
        "            \"id\": config_id,\n",
        "            \"name\": config_name,\n",
        "            \"description\": config_description,\n",
        "            \"configuration\": configuration\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Added configuration: {config_name} (ID: {config_id})\")\n",
        "\n",
        "    def evaluate_configuration(\n",
        "        self,\n",
        "        config_id: str,\n",
        "        original_data: pd.DataFrame,\n",
        "        generated_data: Union[pd.DataFrame, List[str]],\n",
        "        text_column: str,\n",
        "        pii_metadata_column: str = \"pii_metadata\",\n",
        "        canary_metadata_column: str = \"canary_metadata\",\n",
        "        adversarial_results: Optional[Dict[str, Any]] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate a specific configuration.\n",
        "\n",
        "        Args:\n",
        "            config_id: Identifier of the configuration to evaluate.\n",
        "            original_data: DataFrame with original data and metadata.\n",
        "            generated_data: DataFrame or list with generated texts.\n",
        "            text_column: Column containing the text.\n",
        "            pii_metadata_column: Column containing PII metadata.\n",
        "            canary_metadata_column: Column containing canary metadata.\n",
        "            adversarial_results: Optional results from adversarial testing.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation results.\n",
        "        \"\"\"\n",
        "        if config_id not in self.configurations:\n",
        "            logger.error(f\"Configuration ID {config_id} not found\")\n",
        "            return {\"error\": f\"Configuration ID {config_id} not found\"}\n",
        "\n",
        "        config = self.configurations[config_id]\n",
        "        logger.info(f\"Evaluating configuration: {config['name']} (ID: {config_id})\")\n",
        "\n",
        "        # Extract original texts\n",
        "        original_texts = original_data[text_column].tolist()\n",
        "\n",
        "        # Extract generated texts\n",
        "        if isinstance(generated_data, pd.DataFrame):\n",
        "            if text_column not in generated_data.columns:\n",
        "                logger.error(f\"Text column '{text_column}' not found in generated data\")\n",
        "                return {\"error\": f\"Text column '{text_column}' not found in generated data\"}\n",
        "\n",
        "            generated_texts = generated_data[text_column].tolist()\n",
        "        else:\n",
        "            generated_texts = generated_data\n",
        "\n",
        "        # Ensure we have matching counts\n",
        "        if len(original_texts) != len(generated_texts):\n",
        "            logger.error(f\"Number of original texts ({len(original_texts)}) does not match number of generated texts ({len(generated_texts)})\")\n",
        "            return {\"error\": \"Number of original and generated texts must match\"}\n",
        "\n",
        "        # Evaluate privacy metrics\n",
        "        logger.info(\"Evaluating privacy metrics...\")\n",
        "\n",
        "        pii_leakage = self.privacy_evaluator.evaluate_pii_leakage(\n",
        "            original_data=original_data,\n",
        "            generated_texts=generated_texts,\n",
        "            text_column=text_column,\n",
        "            pii_metadata_column=pii_metadata_column\n",
        "        )\n",
        "\n",
        "        canary_leakage = self.privacy_evaluator.evaluate_canary_leakage(\n",
        "            original_data=original_data,\n",
        "            generated_texts=generated_texts,\n",
        "            text_column=text_column,\n",
        "            canary_metadata_column=canary_metadata_column\n",
        "        )\n",
        "\n",
        "        similarity_distribution = self.privacy_evaluator.evaluate_similarity_distribution(\n",
        "            original_data=original_data,\n",
        "            generated_texts=generated_texts,\n",
        "            text_column=text_column\n",
        "        )\n",
        "\n",
        "        # Evaluate utility metrics\n",
        "        logger.info(\"Evaluating utility metrics...\")\n",
        "\n",
        "        utility_metrics = self.utility_evaluator.calculate_all_metrics(\n",
        "            original_texts=original_texts,\n",
        "            generated_texts=generated_texts\n",
        "        )\n",
        "\n",
        "        # Combine all results\n",
        "        results = {\n",
        "            \"config_id\": config_id,\n",
        "            \"config_name\": config[\"name\"],\n",
        "            \"config_description\": config[\"description\"],\n",
        "            \"configuration\": config[\"configuration\"],\n",
        "            \"privacy_metrics\": {\n",
        "                \"pii_leakage\": pii_leakage,\n",
        "                \"canary_leakage\": canary_leakage,\n",
        "                \"similarity_distribution\": similarity_distribution,\n",
        "                \"adversarial_results\": adversarial_results or {}\n",
        "            },\n",
        "            \"utility_metrics\": utility_metrics\n",
        "        }\n",
        "\n",
        "        # Store results\n",
        "        self.results[config_id] = results\n",
        "\n",
        "        # Save results to file\n",
        "        self._save_results(config_id, results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _save_results(self, config_id: str, results: Dict[str, Any]):\n",
        "        \"\"\"Save evaluation results to file.\"\"\"\n",
        "        results_path = os.path.join(self.results_dir, f\"{config_id}_results.json\")\n",
        "\n",
        "        with open(results_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Saved evaluation results to {results_path}\")\n",
        "\n",
        "    def create_comparison_charts(\n",
        "        self,\n",
        "        config_ids: List[str] = None,\n",
        "        output_dir: Optional[str] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create comparison charts for multiple configurations.\n",
        "\n",
        "        Args:\n",
        "            config_ids: List of configuration IDs to compare. If None, use all.\n",
        "            output_dir: Directory to save charts. If None, use results_dir.\n",
        "        \"\"\"\n",
        "        # Use all configurations if none specified\n",
        "        if config_ids is None:\n",
        "            config_ids = list(self.results.keys())\n",
        "\n",
        "        # Skip if no results\n",
        "        if not config_ids or not self.results:\n",
        "            logger.warning(\"No results available for comparison\")\n",
        "            return\n",
        "\n",
        "        # Get results for selected configurations\n",
        "        configs_to_compare = []\n",
        "        for config_id in config_ids:\n",
        "            if config_id in self.results:\n",
        "                configs_to_compare.append(self.results[config_id])\n",
        "            else:\n",
        "                logger.warning(f\"Configuration {config_id} not found in results\")\n",
        "\n",
        "        if not configs_to_compare:\n",
        "            logger.warning(\"No valid configurations found for comparison\")\n",
        "            return\n",
        "\n",
        "        # Set output directory\n",
        "        output_dir = output_dir or self.results_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Set plot style\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "        # 1. Privacy-Utility Trade-off Chart\n",
        "        self._create_privacy_utility_tradeoff(configs_to_compare, output_dir)\n",
        "\n",
        "        # 2. PII Leakage Rate Comparison\n",
        "        self._create_pii_leakage_comparison(configs_to_compare, output_dir)\n",
        "\n",
        "        # 3. Semantic Similarity Distribution\n",
        "        self._create_similarity_distribution_chart(configs_to_compare, output_dir)\n",
        "\n",
        "        # 4. Perplexity Comparison\n",
        "        self._create_perplexity_comparison(configs_to_compare, output_dir)\n",
        "\n",
        "        # 5. Output Length Comparison\n",
        "        self._create_output_length_comparison(configs_to_compare, output_dir)\n",
        "\n",
        "    def _create_privacy_utility_tradeoff(\n",
        "        self,\n",
        "        configs: List[Dict[str, Any]],\n",
        "        output_dir: str\n",
        "    ):\n",
        "        \"\"\"Create privacy-utility trade-off chart.\"\"\"\n",
        "        # Extract data for the chart\n",
        "        labels = []\n",
        "        pii_leakage_rates = []\n",
        "        semantic_similarities = []\n",
        "        perplexities = []\n",
        "\n",
        "        for config in configs:\n",
        "            config_name = config[\"config_name\"]\n",
        "            labels.append(config_name)\n",
        "\n",
        "            # Get PII leakage rate\n",
        "            pii_leakage = config[\"privacy_metrics\"][\"pii_leakage\"].get(\"pii_leakage_rate\", 0)\n",
        "            pii_leakage_rates.append(pii_leakage)\n",
        "\n",
        "            # Get semantic similarity\n",
        "            semantic_similarity = config[\"utility_metrics\"][\"semantic_similarity\"].get(\"mean_similarity\", 0)\n",
        "            semantic_similarities.append(semantic_similarity)\n",
        "\n",
        "            # Get perplexity (lower is better)\n",
        "            perplexity = config[\"utility_metrics\"][\"perplexity\"].get(\"mean_perplexity\", 0)\n",
        "            if perplexity > 0:\n",
        "                # Normalize perplexity to 0-1 range (inverted)\n",
        "                perplexities.append(1.0 / perplexity)\n",
        "            else:\n",
        "                perplexities.append(0)\n",
        "\n",
        "        # Create the trade-off chart\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        scatter = ax.scatter(\n",
        "            pii_leakage_rates,\n",
        "            semantic_similarities,\n",
        "            s=[p * 200 for p in perplexities],  # Size based on perplexity\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "        # Add labels for each point\n",
        "        for i, label in enumerate(labels):\n",
        "            ax.annotate(\n",
        "                label,\n",
        "                (pii_leakage_rates[i], semantic_similarities[i]),\n",
        "                xytext=(5, 5),\n",
        "                textcoords='offset points'\n",
        "            )\n",
        "\n",
        "        ax.set_xlabel('PII Leakage Rate (lower is better)')\n",
        "        ax.set_ylabel('Semantic Similarity (higher is better)')\n",
        "        ax.set_title('Privacy-Utility Trade-off')\n",
        "\n",
        "        # Set axis ranges\n",
        "        ax.set_xlim(-0.05, max(pii_leakage_rates) + 0.1)\n",
        "        ax.set_ylim(min(semantic_similarities) - 0.1, 1.05)\n",
        "\n",
        "        # Add grid\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Save the chart\n",
        "        plt.tight_layout()\n",
        "        output_path = os.path.join(output_dir, 'privacy_utility_tradeoff.png')\n",
        "        plt.savefig(output_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Created privacy-utility trade-off chart: {output_path}\")\n",
        "\n",
        "    def _create_pii_leakage_comparison(\n",
        "        self,\n",
        "        configs: List[Dict[str, Any]],\n",
        "        output_dir: str\n",
        "    ):\n",
        "        \"\"\"Create PII leakage comparison chart.\"\"\"\n",
        "        # Extract data for the chart\n",
        "        labels = []\n",
        "        pii_leakage_rates = []\n",
        "        canary_leakage_rates = []\n",
        "\n",
        "        for config in configs:\n",
        "            config_name = config[\"config_name\"]\n",
        "            labels.append(config_name)\n",
        "\n",
        "            # Get PII leakage rate\n",
        "            pii_leakage = config[\"privacy_metrics\"][\"pii_leakage\"].get(\"pii_leakage_rate\", 0)\n",
        "            pii_leakage_rates.append(pii_leakage)\n",
        "\n",
        "            # Get canary leakage rate\n",
        "            canary_leakage = config[\"privacy_metrics\"][\"canary_leakage\"].get(\"total_leakage_rate\", 0)\n",
        "            canary_leakage_rates.append(canary_leakage)\n",
        "\n",
        "        # Create the comparison chart\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        x = np.arange(len(labels))\n",
        "        width = 0.35\n",
        "\n",
        "        ax.bar(x - width/2, pii_leakage_rates, width, label='PII Leakage Rate')\n",
        "        ax.bar(x + width/2, canary_leakage_rates, width, label='Canary Leakage Rate')\n",
        "\n",
        "        ax.set_xlabel('Configuration')\n",
        "        ax.set_ylabel('Leakage Rate')\n",
        "        ax.set_title('PII and Canary Leakage Rates by Configuration')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "\n",
        "        # Set y-axis range\n",
        "        ax.set_ylim(0, max(max(pii_leakage_rates), max(canary_leakage_rates)) + 0.1)\n",
        "\n",
        "        # Add grid\n",
        "        ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Save the chart\n",
        "        plt.tight_layout()\n",
        "        output_path = os.path.join(output_dir, 'pii_leakage_comparison.png')\n",
        "        plt.savefig(output_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Created PII leakage comparison chart: {output_path}\")\n",
        "\n",
        "    def _create_similarity_distribution_chart(\n",
        "        self,\n",
        "        configs: List[Dict[str, Any]],\n",
        "        output_dir: str\n",
        "    ):\n",
        "        \"\"\"Create similarity distribution chart.\"\"\"\n",
        "        # Extract data for the chart\n",
        "        all_similarities = {}\n",
        "\n",
        "        for config in configs:\n",
        "            config_name = config[\"config_name\"]\n",
        "\n",
        "            # Get similarity distribution\n",
        "            similarities = config[\"privacy_metrics\"][\"similarity_distribution\"].get(\"all_similarities\", [])\n",
        "            if similarities:\n",
        "                all_similarities[config_name] = similarities\n",
        "\n",
        "        if not all_similarities:\n",
        "            logger.warning(\"No similarity data available for distribution chart\")\n",
        "            return\n",
        "\n",
        "        # Create the distribution chart\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        for config_name, similarities in all_similarities.items():\n",
        "            sns.kdeplot(similarities, label=config_name, ax=ax)\n",
        "\n",
        "        ax.set_xlabel('Cosine Similarity')\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.set_title('Similarity Distribution by Configuration')\n",
        "        ax.legend()\n",
        "\n",
        "        # Set x-axis range\n",
        "        ax.set_xlim(0, 1)\n",
        "\n",
        "        # Add grid\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Save the chart\n",
        "        plt.tight_layout()\n",
        "        output_path = os.path.join(output_dir, 'similarity_distribution.png')\n",
        "        plt.savefig(output_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Created similarity distribution chart: {output_path}\")\n",
        "\n",
        "    def _create_perplexity_comparison(\n",
        "        self,\n",
        "        configs: List[Dict[str, Any]],\n",
        "        output_dir: str\n",
        "    ):\n",
        "        \"\"\"Create perplexity comparison chart.\"\"\"\n",
        "        # Extract data for the chart\n",
        "        labels = []\n",
        "        perplexities = []\n",
        "\n",
        "        for config in configs:\n",
        "            config_name = config[\"config_name\"]\n",
        "\n",
        "            # Get perplexity\n",
        "            perplexity = config[\"utility_metrics\"][\"perplexity\"].get(\"mean_perplexity\")\n",
        "            if perplexity is not None:\n",
        "                labels.append(config_name)\n",
        "                perplexities.append(perplexity)\n",
        "\n",
        "        if not perplexities:\n",
        "            logger.warning(\"No perplexity data available for comparison chart\")\n",
        "            return\n",
        "\n",
        "        # Create the comparison chart\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        ax.bar(labels, perplexities, alpha=0.7)\n",
        "\n",
        "        ax.set_xlabel('Configuration')\n",
        "        ax.set_ylabel('Perplexity (lower is better)')\n",
        "        ax.set_title('Perplexity by Configuration')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Add grid\n",
        "        ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Save the chart\n",
        "        plt.tight_layout()\n",
        "        output_path = os.path.join(output_dir, 'perplexity_comparison.png')\n",
        "        plt.savefig(output_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Created perplexity comparison chart: {output_path}\")\n",
        "\n",
        "    def _create_output_length_comparison(\n",
        "        self,\n",
        "        configs: List[Dict[str, Any]],\n",
        "        output_dir: str\n",
        "    ):\n",
        "        \"\"\"Create output length comparison chart.\"\"\"\n",
        "        # Extract data for the chart\n",
        "        labels = []\n",
        "        orig_lengths = []\n",
        "        gen_lengths = []\n",
        "        placeholder_rates = []\n",
        "\n",
        "        for config in configs:\n",
        "            config_name = config[\"config_name\"]\n",
        "            labels.append(config_name)\n",
        "\n",
        "            # Get output statistics\n",
        "            output_stats = config[\"utility_metrics\"][\"output_statistics\"]\n",
        "\n",
        "            # Get word counts\n",
        "            orig_len = output_stats.get(\"word_count\", {}).get(\"original_mean\", 0)\n",
        "            gen_len = output_stats.get(\"word_count\", {}).get(\"generated_mean\", 0)\n",
        "            orig_lengths.append(orig_len)\n",
        "            gen_lengths.append(gen_len)\n",
        "\n",
        "            # Get placeholder rate\n",
        "            placeholder_rate = output_stats.get(\"placeholder_count\", {}).get(\"texts_with_placeholders_rate\", 0)\n",
        "            placeholder_rates.append(placeholder_rate)\n",
        "\n",
        "        # Create the comparison chart\n",
        "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        x = np.arange(len(labels))\n",
        "        width = 0.35\n",
        "\n",
        "        ax1.bar(x - width/2, orig_lengths, width, label='Original Length', color='tab:blue', alpha=0.7)\n",
        "        ax1.bar(x + width/2, gen_lengths, width, label='Generated Length', color='tab:orange', alpha=0.7)\n",
        "\n",
        "        ax1.set_xlabel('Configuration')\n",
        "        ax1.set_ylabel('Average Word Count')\n",
        "        ax1.set_title('Text Length and Placeholder Rate by Configuration')\n",
        "        ax1.set_xticks(x)\n",
        "        ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
        "        ax1.legend(loc='upper left')\n",
        "\n",
        "        # Create second y-axis for placeholder rate\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.plot(x, placeholder_rates, 'r-o', label='Placeholder Rate')\n",
        "        ax2.set_ylabel('Placeholder Rate')\n",
        "        ax2.set_ylim(0, max(placeholder_rates) + 0.1)\n",
        "        ax2.legend(loc='upper right')\n",
        "\n",
        "        # Add grid\n",
        "        ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Save the chart\n",
        "        plt.tight_layout()\n",
        "        output_path = os.path.join(output_dir, 'output_length_comparison.png')\n",
        "        plt.savefig(output_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Created output length comparison chart: {output_path}\")\n",
        "\n",
        "    def create_summary_report(\n",
        "        self,\n",
        "        output_path: Optional[str] = None,\n",
        "        config_ids: List[str] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a summary report of evaluation results.\n",
        "\n",
        "        Args:\n",
        "            output_path: Path to save the report. If None, use default.\n",
        "            config_ids: List of configuration IDs to include. If None, use all.\n",
        "        \"\"\"\n",
        "        # Use all configurations if none specified\n",
        "        if config_ids is None:\n",
        "            config_ids = list(self.results.keys())\n",
        "\n",
        "        # Skip if no results\n",
        "        if not config_ids or not self.results:\n",
        "            logger.warning(\"No results available for report\")\n",
        "            return\n",
        "\n",
        "        # Get results for selected configurations\n",
        "        configs_to_report = []\n",
        "        for config_id in config_ids:\n",
        "            if config_id in self.results:\n",
        "                configs_to_report.append(self.results[config_id])\n",
        "            else:\n",
        "                logger.warning(f\"Configuration {config_id} not found in results\")\n",
        "\n",
        "        if not configs_to_report:\n",
        "            logger.warning(\"No valid configurations found for report\")\n",
        "            return\n",
        "\n",
        "        # Set output path\n",
        "        if output_path is None:\n",
        "            output_path = os.path.join(self.results_dir, 'evaluation_summary.json')\n",
        "\n",
        "        # Create summary data\n",
        "        summary = {\n",
        "            \"overview\": {\n",
        "                \"num_configurations\": len(configs_to_report),\n",
        "                \"configurations\": [cfg[\"config_name\"] for cfg in configs_to_report]\n",
        "            },\n",
        "            \"privacy_metrics\": {},\n",
        "            \"utility_metrics\": {},\n",
        "            \"configurations\": {}\n",
        "        }\n",
        "\n",
        "        # Add configuration details\n",
        "        for config in configs_to_report:\n",
        "            config_id = config[\"config_id\"]\n",
        "            config_name = config[\"config_name\"]\n",
        "\n",
        "            # Extract key metrics\n",
        "            privacy_metrics = config[\"privacy_metrics\"]\n",
        "            utility_metrics = config[\"utility_metrics\"]\n",
        "\n",
        "            pii_leakage_rate = privacy_metrics[\"pii_leakage\"].get(\"pii_leakage_rate\", 0)\n",
        "            canary_leakage_rate = privacy_metrics[\"canary_leakage\"].get(\"total_leakage_rate\", 0)\n",
        "            similarity_mean = utility_metrics[\"semantic_similarity\"].get(\"mean_similarity\", 0)\n",
        "            perplexity = utility_metrics[\"perplexity\"].get(\"mean_perplexity\", 0)\n",
        "\n",
        "            # Add to summary\n",
        "            summary[\"configurations\"][config_id] = {\n",
        "                \"name\": config_name,\n",
        "                \"description\": config[\"config_description\"],\n",
        "                \"key_metrics\": {\n",
        "                    \"pii_leakage_rate\": pii_leakage_rate,\n",
        "                    \"canary_leakage_rate\": canary_leakage_rate,\n",
        "                    \"semantic_similarity\": similarity_mean,\n",
        "                    \"perplexity\": perplexity\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Add comparative privacy metrics\n",
        "        summary[\"privacy_metrics\"] = {\n",
        "            \"pii_leakage_rates\": {cfg[\"config_name\"]: cfg[\"privacy_metrics\"][\"pii_leakage\"].get(\"pii_leakage_rate\", 0) for cfg in configs_to_report},\n",
        "            \"canary_leakage_rates\": {cfg[\"config_name\"]: cfg[\"privacy_metrics\"][\"canary_leakage\"].get(\"total_leakage_rate\", 0) for cfg in configs_to_report},\n",
        "            \"best_privacy_configuration\": min([(cfg[\"config_name\"], cfg[\"privacy_metrics\"][\"pii_leakage\"].get(\"pii_leakage_rate\", 0)) for cfg in configs_to_report], key=lambda x: x[1])[0]\n",
        "        }\n",
        "\n",
        "        # Add comparative utility metrics\n",
        "        summary[\"utility_metrics\"] = {\n",
        "            \"semantic_similarities\": {cfg[\"config_name\"]: cfg[\"utility_metrics\"][\"semantic_similarity\"].get(\"mean_similarity\", 0) for cfg in configs_to_report},\n",
        "            \"perplexities\": {cfg[\"config_name\"]: cfg[\"utility_metrics\"][\"perplexity\"].get(\"mean_perplexity\", 0) for cfg in configs_to_report if \"mean_perplexity\" in cfg[\"utility_metrics\"][\"perplexity\"]},\n",
        "            \"best_utility_configuration\": max([(cfg[\"config_name\"], cfg[\"utility_metrics\"][\"semantic_similarity\"].get(\"mean_similarity\", 0)) for cfg in configs_to_report], key=lambda x: x[1])[0]\n",
        "        }\n",
        "\n",
        "        # Calculate privacy-utility balance\n",
        "        privacy_utility_scores = []\n",
        "        for cfg in configs_to_report:\n",
        "            # Privacy score (0-1, higher is better)\n",
        "            privacy_score = 1.0 - cfg[\"privacy_metrics\"][\"pii_leakage\"].get(\"pii_leakage_rate\", 0)\n",
        "\n",
        "            # Utility score (0-1, higher is better)\n",
        "            utility_score = cfg[\"utility_metrics\"][\"semantic_similarity\"].get(\"mean_similarity\", 0)\n",
        "\n",
        "            # Combined score (simple average)\n",
        "            combined_score = (privacy_score + utility_score) / 2\n",
        "\n",
        "            privacy_utility_scores.append((cfg[\"config_name\"], combined_score))\n",
        "\n",
        "        # Find best balanced configuration\n",
        "        best_balanced = max(privacy_utility_scores, key=lambda x: x[1])\n",
        "        summary[\"best_balanced_configuration\"] = {\n",
        "            \"name\": best_balanced[0],\n",
        "            \"score\": best_balanced[1]\n",
        "        }\n",
        "\n",
        "        # Save summary report\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Saved evaluation summary report to {output_path}\")\n",
        "\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOxExaynzJck"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igJjDjsv2Fcz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Script to run the evaluation pipeline on different filter configurations.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('evaluation.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def load_dataset(dataset_type, num_samples, seed, dataset_path=None, text_column=\"text\"):\n",
        "    \"\"\"Load the dataset for evaluation.\"\"\"\n",
        "    # Load dataset\n",
        "    if dataset_type == \"enron\":\n",
        "        logger.info(f\"Loading Enron dataset with {num_samples} samples...\")\n",
        "        data = load_enron_sample(num_samples=num_samples, seed=seed)\n",
        "        text_column = \"body\"\n",
        "    elif dataset_type == \"stackoverflow\":\n",
        "        logger.info(f\"Loading Stack Overflow dataset with {num_samples} samples...\")\n",
        "        data = load_stackoverflow_sample(num_samples=num_samples, seed=seed)\n",
        "        text_column = \"body\"\n",
        "    else:  # Custom dataset\n",
        "        if not dataset_path:\n",
        "            raise ValueError(\"dataset_path is required for custom dataset\")\n",
        "\n",
        "        logger.info(f\"Loading custom dataset from {dataset_path}...\")\n",
        "\n",
        "        file_ext = Path(dataset_path).suffix.lower()\n",
        "        if file_ext == \".csv\":\n",
        "            data = pd.read_csv(dataset_path)\n",
        "        elif file_ext == \".json\":\n",
        "            data = pd.read_json(dataset_path)\n",
        "        elif file_ext in [\".pkl\", \".pickle\"]:\n",
        "            data = pd.read_pickle(dataset_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file extension: {file_ext}\")\n",
        "\n",
        "    return data, text_column\n",
        "\n",
        "def inject_sensitive_data(data, text_column, output_dir, seed, names_file=None, pii_rate=0.3, canary_rate=0.1):\n",
        "    \"\"\"Inject sensitive data into the dataset.\"\"\"\n",
        "    # Set up data injector\n",
        "    logger.info(\"Setting up data injector...\")\n",
        "    injector = SensitiveDataInjector(\n",
        "        seed=seed,\n",
        "        name_list_path=names_file,\n",
        "        canary_phrases=None,  # Use default canary phrases\n",
        "        pii_injection_rate=pii_rate,\n",
        "        canary_injection_rate=canary_rate\n",
        "    )\n",
        "\n",
        "    # Inject sensitive data\n",
        "    logger.info(\"Injecting sensitive data...\")\n",
        "    processed_data, stats = injector.process_dataset(\n",
        "        input_data=data,\n",
        "        text_column=text_column,\n",
        "        output_path=os.path.join(output_dir, \"processed_dataset.json\"),\n",
        "        output_format=\"json\"\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Injected PII into {stats['pii_injections']} samples ({stats['pii_injection_rate']:.2%})\")\n",
        "    logger.info(f\"Injected canary phrases into {stats['canary_injections']} samples ({stats['canary_injection_rate']:.2%})\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def create_configurations():\n",
        "    \"\"\"Create filter configurations for evaluation.\"\"\"\n",
        "    configs = {\n",
        "        \"baseline\": {\n",
        "            \"name\": \"Baseline (No Filters)\",\n",
        "            \"description\": \"No privacy filters applied\",\n",
        "            \"pre_filter\": None,\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"repetition_penalty\": 1.0,\n",
        "                \"blocked_words\": []\n",
        "            },\n",
        "            \"post_filter\": None\n",
        "        },\n",
        "        \"pre_only\": {\n",
        "            \"name\": \"Pre-Generation Filter Only\",\n",
        "            \"description\": \"Only pre-generation filtering applied\",\n",
        "            \"pre_filter\": {\n",
        "                \"use_token_level\": True,\n",
        "                \"type_specific_placeholders\": True\n",
        "            },\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"repetition_penalty\": 1.0,\n",
        "                \"blocked_words\": []\n",
        "            },\n",
        "            \"post_filter\": None\n",
        "        },\n",
        "        \"post_only\": {\n",
        "            \"name\": \"Post-Generation Filter Only\",\n",
        "            \"description\": \"Only post-generation filtering applied\",\n",
        "            \"pre_filter\": None,\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"repetition_penalty\": 1.0,\n",
        "                \"blocked_words\": []\n",
        "            },\n",
        "            \"post_filter\": {\n",
        "                \"base_threshold\": 0.92,\n",
        "                \"reapply_pii_detection\": True,\n",
        "                \"use_token_level_redaction\": True\n",
        "            }\n",
        "        },\n",
        "        \"pre_post\": {\n",
        "            \"name\": \"Pre+Post Filters\",\n",
        "            \"description\": \"Pre-generation and post-generation filters applied\",\n",
        "            \"pre_filter\": {\n",
        "                \"use_token_level\": True,\n",
        "                \"type_specific_placeholders\": True\n",
        "            },\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"repetition_penalty\": 1.0,\n",
        "                \"blocked_words\": []\n",
        "            },\n",
        "            \"post_filter\": {\n",
        "                \"base_threshold\": 0.92,\n",
        "                \"reapply_pii_detection\": True,\n",
        "                \"use_token_level_redaction\": True\n",
        "            }\n",
        "        },\n",
        "        \"all_filters\": {\n",
        "            \"name\": \"All Filters\",\n",
        "            \"description\": \"All privacy filters applied (pre, in, post)\",\n",
        "            \"pre_filter\": {\n",
        "                \"use_token_level\": True,\n",
        "                \"type_specific_placeholders\": True\n",
        "            },\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"repetition_penalty\": 1.2,\n",
        "                \"blocked_words\": [\"password\", \"social security\", \"credit card\", \"address\", \"phone number\", \"email\"]\n",
        "            },\n",
        "            \"post_filter\": {\n",
        "                \"base_threshold\": 0.92,\n",
        "                \"reapply_pii_detection\": True,\n",
        "                \"use_token_level_redaction\": True\n",
        "            }\n",
        "        },\n",
        "        \"all_strict\": {\n",
        "            \"name\": \"All Filters (Strict)\",\n",
        "            \"description\": \"All privacy filters with stricter settings\",\n",
        "            \"pre_filter\": {\n",
        "                \"use_token_level\": True,\n",
        "                \"type_specific_placeholders\": True\n",
        "            },\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_k\": 40,\n",
        "                \"top_p\": 0.9,\n",
        "                \"repetition_penalty\": 1.3,\n",
        "                \"blocked_words\": [\"password\", \"social security\", \"credit card\", \"address\", \"phone number\", \"email\"]\n",
        "            },\n",
        "            \"post_filter\": {\n",
        "                \"base_threshold\": 0.85,  # Lower threshold = more filtering\n",
        "                \"reapply_pii_detection\": True,\n",
        "                \"use_token_level_redaction\": True\n",
        "            }\n",
        "        },\n",
        "        \"adaptive\": {\n",
        "            \"name\": \"Adaptive Thresholds\",\n",
        "            \"description\": \"All filters with content-dependent adaptive thresholds\",\n",
        "            \"pre_filter\": {\n",
        "                \"use_token_level\": True,\n",
        "                \"type_specific_placeholders\": True\n",
        "            },\n",
        "            \"in_generation\": {\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "                \"repetition_penalty\": 1.2,\n",
        "                \"blocked_words\": [\"password\", \"social security\", \"credit card\", \"address\", \"phone number\", \"email\"]\n",
        "            },\n",
        "            \"post_filter\": {\n",
        "                \"base_threshold\": 0.92,\n",
        "                \"adaptive_thresholds\": {\n",
        "                    \"technical\": 0.95,\n",
        "                    \"general\": 0.9,\n",
        "                    \"personal\": 0.85\n",
        "                },\n",
        "                \"reapply_pii_detection\": True,\n",
        "                \"use_token_level_redaction\": True\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return configs\n",
        "\n",
        "def setup_filter_pipeline(config, sensitive_items=None):\n",
        "    \"\"\"Set up filter pipeline based on configuration.\"\"\"\n",
        "    # Set up pre-generation filter\n",
        "    pre_filter = None\n",
        "    if config[\"pre_filter\"] is not None:\n",
        "        pre_filter = PreGenerationFilter(**config[\"pre_filter\"])\n",
        "\n",
        "    # Set up post-generation filter\n",
        "    post_filter = None\n",
        "    if config[\"post_filter\"] is not None:\n",
        "        post_config = dict(config[\"post_filter\"])\n",
        "        if sensitive_items:\n",
        "            post_config[\"sensitive_items\"] = sensitive_items\n",
        "        post_filter = PostGenerationFilter(**post_config)\n",
        "\n",
        "    return pre_filter, post_filter\n",
        "\n",
        "def generate_text(\n",
        "    config_id,\n",
        "    config,\n",
        "    data,\n",
        "    text_column,\n",
        "    hf_token='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',\n",
        "    max_tokens=256,\n",
        "    num_samples=None,\n",
        "):\n",
        "    \"\"\"Generate text using the specified configuration.\"\"\"\n",
        "    logger.info(f\"Generating text with configuration: {config['name']} (ID: {config_id})\")\n",
        "\n",
        "    # Set up filters\n",
        "    pre_filter, post_filter = setup_filter_pipeline(\n",
        "        config,\n",
        "        sensitive_items=data.loc[data[\"has_pii\"] == True, text_column].tolist()[:10]\n",
        "    )\n",
        "\n",
        "    # Set up LLM\n",
        "    logger.info(\"Setting up LLM...\")\n",
        "\n",
        "    in_gen_config = dict(config[\"in_generation\"])\n",
        "    generator = GemmaTextGenerator(\n",
        "        model_name=\"google/gemma-2b-it\",\n",
        "        max_new_tokens=max_tokens,\n",
        "        **in_gen_config,\n",
        "        hf_auth_token=hf_token or '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'\n",
        "    )\n",
        "\n",
        "    # Process samples\n",
        "    if num_samples is None:\n",
        "        num_samples = len(data)\n",
        "    else:\n",
        "        num_samples = min(num_samples, len(data))\n",
        "\n",
        "    samples = data.iloc[:num_samples]\n",
        "    generated_texts = []\n",
        "    pre_processed_texts = []\n",
        "\n",
        "    for i, (_, row) in enumerate(samples.iterrows()):\n",
        "        original_text = row[text_column]\n",
        "\n",
        "        # Apply pre-generation filter\n",
        "        if pre_filter:\n",
        "            pre_processed, _ = pre_filter.process(original_text)\n",
        "            prompt = pre_processed\n",
        "        else:\n",
        "            prompt = original_text\n",
        "            pre_processed = original_text\n",
        "\n",
        "        pre_processed_texts.append(pre_processed)\n",
        "\n",
        "        # Generate text\n",
        "        try:\n",
        "            # Only show progress periodically\n",
        "            if i % 5 == 0:\n",
        "                logger.info(f\"Generating text for sample {i+1}/{num_samples}...\")\n",
        "\n",
        "            generated_text, _ = generator.generate(\n",
        "                prompt=f\"Based on the following text, generate a variation that preserves the meaning but uses different wording:\\n\\n{prompt}\",\n",
        "                max_new_tokens=max_tokens\n",
        "            )\n",
        "\n",
        "            # Apply post-generation filter\n",
        "            if post_filter:\n",
        "                filtered_text, _ = post_filter.process(generated_text)\n",
        "                generated_text = filtered_text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating text for sample {i}: {str(e)}\")\n",
        "            generated_text = \"[GENERATION_ERROR]\"\n",
        "\n",
        "        generated_texts.append(generated_text)\n",
        "\n",
        "    # Create DataFrame with results\n",
        "    results_df = samples.copy()\n",
        "    results_df[\"pre_processed_text\"] = pre_processed_texts\n",
        "    results_df[\"generated_text\"] = generated_texts\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt4g5-LU8Z5b"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('full_evaluation.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LqFXWCA6yxH"
      },
      "outputs": [],
      "source": [
        "# Set up output directory\n",
        "os.makedirs('results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_RFBtv763Jf"
      },
      "outputs": [],
      "source": [
        "data, text_column = load_dataset(\n",
        "        dataset_type=\"enron\",\n",
        "        num_samples=100,\n",
        "        seed=42,\n",
        "        dataset_path=None,\n",
        "        text_column='text'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUlksPHE7GIs"
      },
      "outputs": [],
      "source": [
        "processed_data = inject_sensitive_data(\n",
        "        data=data,\n",
        "        text_column=text_column,\n",
        "        output_dir='results',\n",
        "        seed=42,\n",
        "        names_file=None,\n",
        "        pii_rate=0.3,\n",
        "        canary_rate=0.1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGR7NIum7TDq"
      },
      "outputs": [],
      "source": [
        "all_configs = create_configurations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTQoatsb7VI7"
      },
      "outputs": [],
      "source": [
        "configs_to_run = None\n",
        "if configs_to_run:\n",
        "    configs = {k: v for k, v in all_configs.items() if k in configs_to_run}\n",
        "    if not configs:\n",
        "        logger.error(f\"None of the specified configurations found: {configs_to_run}\")\n",
        "        logger.info(f\"Available configurations: {list(all_configs.keys())}\")\n",
        "        print(\"done\")\n",
        "else:\n",
        "    configs = all_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fARZ6Zg47dML"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Setting up evaluators...\")\n",
        "privacy_evaluator = PrivacyEvaluator(device=None)\n",
        "utility_evaluator = UtilityEvaluator(device=None)\n",
        "evaluator = PrivacyFrameworkEvaluator(\n",
        "        privacy_evaluator=privacy_evaluator,\n",
        "        utility_evaluator=utility_evaluator,\n",
        "        results_dir='results'\n",
        "  )\n",
        "for config_id, config in configs.items():\n",
        "        evaluator.add_configuration(\n",
        "            config_id=config_id,\n",
        "            config_name=config[\"name\"],\n",
        "            config_description=config[\"description\"],\n",
        "            configuration=config\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8GNueNX88duE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1686956422df4f5db50902304eba7ac8",
            "ed3d48a9bda64f3b9baac8045ca99e53",
            "dc0cb291268a4a16ba9f851b552646b5",
            "69ffbfdaf96a431aaeffdc277baa58d9",
            "d2113c4bfd4640ae90e35051af4e5cd6",
            "003cbc2eb4554715a5aa2406f0c163b7",
            "daa0afe957b240809a74f76c975af6d6",
            "27da3fbd968746749af6edea1da0d54f",
            "4980501a0c2e46dc8163ce0b0b16e446",
            "9696faf81280404eafa2db16003d1005",
            "663fd8bb2c5f4a4ebb8c76328c507676",
            "a38e717587aa471389bdf377f0541872",
            "c930577f062043e1820baf07966395ba",
            "e7a2923004c545ef88ea09ea59c58b09",
            "d4ac9ca895b049a18c825981c2a7566c",
            "5ffc74d782b241e391a666a1f62af22a",
            "f70c576338ba40a180acd09acd007b4a",
            "222f52ce6f7e4881a4cdd2e07c3c7516",
            "07f396fe034c46a49310bde8ec1bd02a",
            "ce94332d5a3a40a4841bc6232786b680",
            "893f3244ac6e4c6081b5688c030d90f9",
            "663c5c0c81a14b3fa90feadcb6da44bb",
            "2a1aafce16dd4a1c936a6bf56f42af51",
            "36d0e1a8f5724521ab5aa21a8d680adc",
            "b9c932c63f744997b441f244617e7c30",
            "70c6826bf5664771a475c978687508d6",
            "510c2e2ca231492aa1ee6d2b4f5f354a",
            "4b62a020b225487a8530cfc7d31cdcfa",
            "280d6088dab443cc818bfff892c92879",
            "65cc68f572174b56aceb53daab203267",
            "6ef04e8c3a2e43f3a5bf09eaf87a14ab",
            "fd919d4a0ba24cf4b5fa46cc30e0aff2",
            "e4a5f4d1834e40fd87ec2866565378f8"
          ]
        },
        "id": "RrVTHpgw71t5",
        "outputId": "6ef94a70-33bf-4a53-f32b-99817f9aab6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1686956422df4f5db50902304eba7ac8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor'> to see related `.generate()` flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a38e717587aa471389bdf377f0541872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a1aafce16dd4a1c936a6bf56f42af51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "ERROR:__main__:Generation error: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
          ]
        }
      ],
      "source": [
        "for config_id, config in configs.items():\n",
        "        # Generate text with this configuration\n",
        "        logger.info(f\"Processing configuration: {config['name']} (ID: {config_id})\")\n",
        "\n",
        "        generation_results = generate_text(\n",
        "            config_id=config_id,\n",
        "            config=config,\n",
        "            data=processed_data,\n",
        "            text_column=text_column,\n",
        "            hf_token='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',\n",
        "            max_tokens=256,\n",
        "            num_samples=100,\n",
        "        )\n",
        "\n",
        "        # Save generation results\n",
        "        output_path = os.path.join('results', f\"{config_id}_generated.json\")\n",
        "        generation_results.to_json(output_path, orient=\"records\", indent=2)\n",
        "        logger.info(f\"Saved generation results to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09YNkbwAeEr4"
      },
      "outputs": [],
      "source": [
        "for config_id, config in configs.items():\n",
        "        # Evaluate configuration\n",
        "        logger.info(f\"Evaluating configuration: {config['name']} (ID: {config_id})\")\n",
        "        evaluation_results = evaluator.evaluate_configuration(\n",
        "            config_id=config_id,\n",
        "            original_data=processed_data,\n",
        "            generated_data=generation_results,\n",
        "            text_column=\"generated_text\",\n",
        "            pii_metadata_column=\"pii_metadata\",\n",
        "            canary_metadata_column=\"canary_metadata\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYbwa-9Y8EfX"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Creating comparison charts...\")\n",
        "evaluator.create_comparison_charts()\n",
        "\n",
        "    # Create summary report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wF3Ciq4p8GWH"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Creating summary report...\")\n",
        "summary = evaluator.create_summary_report()\n",
        "\n",
        "logger.info(f\"Evaluation complete. Results saved to {'results'}\")\n",
        "\n",
        "    # Print summary of best configurations\n",
        "print(\"\\n----- Evaluation Summary -----\")\n",
        "print(f\"Best privacy configuration: {summary['privacy_metrics']['best_privacy_configuration']}\")\n",
        "print(f\"Best utility configuration: {summary['utility_metrics']['best_utility_configuration']}\")\n",
        "print(f\"Best balanced configuration: {summary['best_balanced_configuration']['name']} (Score: {summary['best_balanced_configuration']['score']:.4f})\")\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-tanKNf6lb8"
      },
      "outputs": [],
      "source": [
        "def run_evaluation(\n",
        "    dataset_type=\"enron\",\n",
        "    dataset_path=None,\n",
        "    text_column=\"text\",\n",
        "    num_samples=100,\n",
        "    pii_rate=0.3,\n",
        "    canary_rate=0.1,\n",
        "    names_file=None,\n",
        "    hf_token='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',\n",
        "    max_tokens=256,\n",
        "    device=None,\n",
        "    configs_to_run=None,\n",
        "    output_dir=\"results\",\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"Run the evaluation pipeline with the given parameters.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Load dataset\n",
        "\n",
        "\n",
        "    # Inject sensitive data\n",
        "\n",
        "\n",
        "    # Get configurations\n",
        "\n",
        "\n",
        "    # Filter to selected configurations if specified\n",
        "\n",
        "\n",
        "    # Set up evaluator\n",
        "\n",
        "\n",
        "    # Add configurations to evaluator\n",
        "\n",
        "\n",
        "    # Get HuggingFace token from environment if not provided\n",
        "    if not hf_token:\n",
        "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "    # Generate and evaluate each configuration\n",
        "    for config_id, config in configs.items():\n",
        "        # Generate text with this configuration\n",
        "        logger.info(f\"Processing configuration: {config['name']} (ID: {config_id})\")\n",
        "\n",
        "        generation_results = generate_text(\n",
        "            config_id=config_id,\n",
        "            config=config,\n",
        "            data=processed_data,\n",
        "            text_column=text_column,\n",
        "            hf_token=hf_token,\n",
        "            max_tokens=max_tokens,\n",
        "            num_samples=num_samples,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Save generation results\n",
        "        output_path = os.path.join(output_dir, f\"{config_id}_generated.json\")\n",
        "        generation_results.to_json(output_path, orient=\"records\", indent=2)\n",
        "        logger.info(f\"Saved generation results to {output_path}\")\n",
        "\n",
        "        # Evaluate configuration\n",
        "        logger.info(f\"Evaluating configuration: {config['name']} (ID: {config_id})\")\n",
        "        evaluation_results = evaluator.evaluate_configuration(\n",
        "            config_id=config_id,\n",
        "            original_data=processed_data,\n",
        "            generated_data=generation_results,\n",
        "            text_column=\"generated\",\n",
        "            pii_metadata_column=\"pii_metadata\",\n",
        "            canary_metadata_column=\"canary_metadata\"\n",
        "        )\n",
        "\n",
        "    # Create comparison charts\n",
        "    logger.info(\"Creating comparison charts...\")\n",
        "    evaluator.create_comparison_charts()\n",
        "\n",
        "    # Create summary report\n",
        "    logger.info(\"Creating summary report...\")\n",
        "    summary = evaluator.create_summary_report()\n",
        "\n",
        "    logger.info(f\"Evaluation complete. Results saved to {output_dir}\")\n",
        "\n",
        "    # Print summary of best configurations\n",
        "    print(\"\\n----- Evaluation Summary -----\")\n",
        "    print(f\"Best privacy configuration: {summary['privacy_metrics']['best_privacy_configuration']}\")\n",
        "    print(f\"Best utility configuration: {summary['utility_metrics']['best_utility_configuration']}\")\n",
        "    print(f\"Best balanced configuration: {summary['best_balanced_configuration']['name']} (Score: {summary['best_balanced_configuration']['score']:.4f})\")\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980,
          "referenced_widgets": [
            "fd7df56bdd234795a48afc908363a673",
            "0a49e213dee74557b9760dbc9468858f",
            "6cdaeb6fc4774566927e6e2a91fb9803",
            "88bde829528d42c1a46e348aa1767caa",
            "58bd02c99bf54807b5e2ce6b3a3862c3",
            "2e7634afed2f4ec284d9c96e0790988b",
            "d7011cfd63d94da6887ab0207a9259c5",
            "aa3fb96e00cc493f990ae483c6c7d226",
            "941b9ffbd767472699308f7da50109e9",
            "1e3506dee6fa4d498607cafc41cd452c",
            "b3032884412046bea0152c498a543feb",
            "d08bdf4fe1b1437eaf6f66cd19e34013",
            "b91aa7f7e8d64b158633faedf0c3ae3e",
            "b03bb99fab944f69b57f2d06cecb61bd",
            "bb0ef7fc24b64014b71aa9daa1f4a19c",
            "dcaae39989f94cca969e193650f194a6",
            "7f7ae66881d349a69c6b96efbd8ab353",
            "5b30f78e1491422eb0e134ec9f0ea444",
            "f612b533e1e542e997961e87611c1ccc",
            "7bcca16465274233aca5b76a8fb4f8c7",
            "1ea32e1f9ecd4db6913ee4422a115628",
            "e10e095d211045e59c07d0f3c7327595",
            "ff410ad5e1db4173b981b23a31a1f289",
            "a64189033432400a8828ffe4ca7e8a5d",
            "1ab3362c3aa041a1a4eaaa5f8fc4c522",
            "d8c365c2504e42478a31e75d8229e586",
            "07a97ea5ea2f4d619445b3d3998cb385",
            "087acdcc8c474355b5f83bc8ad68845a",
            "ac844ec318374958aa225a5b696e6773",
            "ff550bbf63534c228f381531370a4b94",
            "5ed2cdc72a8646c0844b963cf28aec54",
            "5a304e2a50444ae2881d61f25441836b",
            "e5a89ffaff894b16923ec65f0f2f79b3",
            "c71c70e3ced2416b85682b5dcbe09741",
            "f248ef5afb7b41ee8fb4bd1a14e66a1a",
            "1090886723ec4c4aa5ac4719bc7fdbc6",
            "d13f43435bba443db5bcacc000baea47",
            "946c68a6c2cd44cc887e8dc0a6c1e3c2",
            "14552df3f0734fdaae4d25a484f753f7",
            "fc757a3a80e34550823807faea3bfd8f",
            "ab26ecddc4974dec82ea01ab6e0e8c25",
            "6a02c7c68fd24e5cafb1d364f7b8b823",
            "74103ed24fb44ab08c6f2775c21b2bf6",
            "3fd3306a8a364b81b5b09a5037822244",
            "6cce4850f67a4c53a683dd0ad2301bd0",
            "a5f6a76601874c94bc7e59887ce0fae7",
            "c178c46dfe8a465a86483869c1f0258e",
            "ae6aff2eeede48d99f8e8dc15c2ca755",
            "a57c09b3d79c4edfb0e7ed78224b930f",
            "c9f7c4c8ba684e32a16469d7c670f4e7",
            "a82eb92cfa034f6c93f82aa87bce1721",
            "f8207693d83b400fac385dd6a8337013",
            "6b85cfcc0635410ab1fcf9113e351382",
            "dfe2b1bb7d99498d992c097d95f35968",
            "adf5d6c4d5494ba4a1b33edf2dcc54fe",
            "c71b822a0bed48e797d5db02c7efa67c",
            "2925d6752ec8424e8d88672e7002133b",
            "c002fed93e19457f9c285c573868f258",
            "12b1f6e9254e4908bc6117a2f24fe205",
            "d616ddfcdf854b86a51a5ba450573d43",
            "d4435fe59a9345c89572c104e6584fec",
            "88459cb20e4e4bb1ab670209a72813af",
            "59e57bc6f9314dccba5043a5632ad963",
            "65a3ea9bde1f4488879241547a9b568f",
            "ffe70228d49e4cb5b4533c33d57f6de5",
            "4bff9bb13df54448adaf27b856995d4e",
            "4151690cf1024261bda7e829cb602335",
            "ad388374faae490d8d4eb42883a87fbf",
            "8399da3d4720436f95399bf307478c54",
            "d1b8ca4a12c246169784f3d9d8fe1e17",
            "6c9a3e4055684bcca4886ad2a833a380",
            "728c80f51a1b46919f42f6240bda8a5b",
            "c84e74a8f68449ffbefe184d750edac2",
            "540ed8a1e13440ffb2d8b18cc37f96d3",
            "3ad3f03bc06c48b08a1458be3a67bc52",
            "50a8b6f9b558458491ee63f370d7abb1",
            "ab59340944634038ac23389d80192a4d",
            "6acd8bf3e37d4883987bd7ebf59fe290",
            "36aeab7d08574733a963bb8d3d3a81db",
            "dff8cc9466674aeaa9d2a5465b52890c",
            "12531fc8b70640e3b33887f8464d2154",
            "f811900057054203bed58a75a524540c",
            "bba64c723e2b43a892b45c1bde34b045",
            "22e00a19b7f54c3b890fbcc9587e1e86",
            "fcc90c39920c4f69b81eea3d63bb2e1a",
            "18a4ee63690c40e1a96f5ba66f900714",
            "239dc411a1b44398ae78c614b6313757",
            "6e542f42661a4e728b4d1ac3b0a1fa05"
          ]
        },
        "id": "M4tvNwX7zbu1",
        "outputId": "2bd189ed-277c-47ac-d236-80831c75003c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7df56bdd234795a48afc908363a673",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d08bdf4fe1b1437eaf6f66cd19e34013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff410ad5e1db4173b981b23a31a1f289",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c71c70e3ced2416b85682b5dcbe09741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cce4850f67a4c53a683dd0ad2301bd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c71b822a0bed48e797d5db02c7efa67c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4151690cf1024261bda7e829cb602335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6acd8bf3e37d4883987bd7ebf59fe290",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'generated_text'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'generated_text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-338a5a073d1b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-338a5a073d1b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Run full evaluation with default parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mrun_full_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-338a5a073d1b>\u001b[0m in \u001b[0;36mrun_full_evaluation\u001b[0;34m(dataset_type, dataset_path, text_column, num_samples, pii_rate, canary_rate, hf_token, max_tokens, device, configs_to_run, output_dir, seed, prepare_only)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Full evaluation completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ee53c415e9da>\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(dataset_type, dataset_path, text_column, num_samples, pii_rate, canary_rate, names_file, hf_token, max_tokens, device, configs_to_run, output_dir, seed)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Evaluate configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating configuration: {config['name']} (ID: {config_id})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         evaluation_results = evaluator.evaluate_configuration(\n\u001b[0m\u001b[1;32m    415\u001b[0m             \u001b[0mconfig_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0moriginal_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1c1a8e7b4aaf>\u001b[0m in \u001b[0;36mevaluate_configuration\u001b[0;34m(self, config_id, original_data, generated_data, text_column, pii_metadata_column, canary_metadata_column, adversarial_results)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Extract original texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0moriginal_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Extract generated texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'generated_text'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Script to run a full evaluation of the synthetic data privacy framework.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('full_evaluation.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def run_full_evaluation(\n",
        "    dataset_type=\"enron\",\n",
        "    dataset_path=None,\n",
        "    text_column=\"text\",\n",
        "    num_samples=100,\n",
        "    pii_rate=0.3,\n",
        "    canary_rate=0.1,\n",
        "    hf_token='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',\n",
        "    max_tokens=256,\n",
        "    device=None,\n",
        "    configs_to_run=None,\n",
        "    output_dir=\"results\",\n",
        "    seed=42,\n",
        "    prepare_only=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Run a full evaluation of the synthetic data privacy framework.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataset_type : str\n",
        "        Type of dataset to use. Options: \"enron\", \"stackoverflow\", \"custom\".\n",
        "    dataset_path : str, optional\n",
        "        Path to custom dataset file. Required if dataset_type is \"custom\".\n",
        "    text_column : str, optional\n",
        "        Column containing text in custom dataset.\n",
        "    num_samples : int, optional\n",
        "        Number of samples to use for evaluation.\n",
        "    pii_rate : float, optional\n",
        "        PII injection rate (0-1).\n",
        "    canary_rate : float, optional\n",
        "        Canary injection rate (0-1).\n",
        "    hf_token : str, optional\n",
        "        HuggingFace API token. Will use environment variable if not provided.\n",
        "    max_tokens : int, optional\n",
        "        Maximum tokens to generate per sample.\n",
        "    device : str, optional\n",
        "        Device to use for models. Options: \"cuda\", \"cpu\", \"mps\".\n",
        "    configs_to_run : list, optional\n",
        "        Specific configurations to evaluate. If None, all configurations will be evaluated.\n",
        "        Options: \"baseline\", \"pre_only\", \"post_only\", \"pre_post\", \"all_filters\", \"all_strict\", \"adaptive\".\n",
        "    output_dir : str, optional\n",
        "        Directory to store results.\n",
        "    seed : int, optional\n",
        "        Random seed.\n",
        "    prepare_only : bool, optional\n",
        "        Only prepare the dataset, don't run evaluation.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict or None\n",
        "        Evaluation summary if prepare_only is False, None otherwise.\n",
        "    \"\"\"\n",
        "    # Set up output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare dataset\n",
        "    logger.info(f\"Preparing dataset: {dataset_type}\")\n",
        "\n",
        "    # If we're just preparing the dataset, don't continue\n",
        "    if prepare_only:\n",
        "        logger.info(\"Dataset preparation completed. Skipping evaluation as requested.\")\n",
        "        return None\n",
        "\n",
        "    # Run evaluation\n",
        "    logger.info(\"Running evaluation with the following parameters:\")\n",
        "    params = {\n",
        "        \"dataset_type\": dataset_type,\n",
        "        \"dataset_path\": dataset_path,\n",
        "        \"text_column\": text_column,\n",
        "        \"num_samples\": num_samples,\n",
        "        \"pii_rate\": pii_rate,\n",
        "        \"canary_rate\": canary_rate,\n",
        "        \"hf_token\": hf_token,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"device\": device,\n",
        "        \"configs_to_run\": configs_to_run,\n",
        "        \"output_dir\": output_dir,\n",
        "        \"seed\": seed\n",
        "    }\n",
        "\n",
        "    logger.info(str(params))\n",
        "\n",
        "    evaluation_results = run_evaluation(**params)\n",
        "\n",
        "    logger.info(\"Full evaluation completed!\")\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run a full evaluation with default parameters.\n",
        "\n",
        "    To customize parameters, modify the parameters in this function\n",
        "    or call run_full_evaluation directly with your desired parameters.\n",
        "    \"\"\"\n",
        "    # Default parameters\n",
        "    params = {\n",
        "        \"dataset_type\": \"enron\",\n",
        "        \"dataset_path\": None,\n",
        "        \"text_column\": \"text\",\n",
        "        \"num_samples\": 100,\n",
        "        \"pii_rate\": 0.3,\n",
        "        \"canary_rate\": 0.1,\n",
        "        \"hf_token\": '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',\n",
        "        \"max_tokens\": 256,\n",
        "        \"device\": None,\n",
        "        \"configs_to_run\": None,  # Run all configs\n",
        "        \"output_dir\": \"results\",\n",
        "        \"seed\": 42,\n",
        "        \"prepare_only\": False\n",
        "    }\n",
        "\n",
        "    # Run full evaluation with default parameters\n",
        "    run_full_evaluation(**params)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AICBBaar9Qzw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07a97ea5ea2f4d619445b3d3998cb385": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "087acdcc8c474355b5f83bc8ad68845a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a49e213dee74557b9760dbc9468858f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e7634afed2f4ec284d9c96e0790988b",
            "placeholder": "​",
            "style": "IPY_MODEL_d7011cfd63d94da6887ab0207a9259c5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1090886723ec4c4aa5ac4719bc7fdbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab26ecddc4974dec82ea01ab6e0e8c25",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a02c7c68fd24e5cafb1d364f7b8b823",
            "value": 1355256
          }
        },
        "12531fc8b70640e3b33887f8464d2154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239dc411a1b44398ae78c614b6313757",
            "placeholder": "​",
            "style": "IPY_MODEL_6e542f42661a4e728b4d1ac3b0a1fa05",
            "value": " 2/2 [00:21&lt;00:00,  9.00s/it]"
          }
        },
        "12b1f6e9254e4908bc6117a2f24fe205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe70228d49e4cb5b4533c33d57f6de5",
            "placeholder": "​",
            "style": "IPY_MODEL_4bff9bb13df54448adaf27b856995d4e",
            "value": " 548M/548M [00:03&lt;00:00, 195MB/s]"
          }
        },
        "14552df3f0734fdaae4d25a484f753f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a4ee63690c40e1a96f5ba66f900714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ab3362c3aa041a1a4eaaa5f8fc4c522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff550bbf63534c228f381531370a4b94",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ed2cdc72a8646c0844b963cf28aec54",
            "value": 456318
          }
        },
        "1e3506dee6fa4d498607cafc41cd452c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ea32e1f9ecd4db6913ee4422a115628": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e00a19b7f54c3b890fbcc9587e1e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "239dc411a1b44398ae78c614b6313757": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2925d6752ec8424e8d88672e7002133b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4435fe59a9345c89572c104e6584fec",
            "placeholder": "​",
            "style": "IPY_MODEL_88459cb20e4e4bb1ab670209a72813af",
            "value": "model.safetensors: 100%"
          }
        },
        "2e7634afed2f4ec284d9c96e0790988b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36aeab7d08574733a963bb8d3d3a81db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bba64c723e2b43a892b45c1bde34b045",
            "placeholder": "​",
            "style": "IPY_MODEL_22e00a19b7f54c3b890fbcc9587e1e86",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3ad3f03bc06c48b08a1458be3a67bc52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fd3306a8a364b81b5b09a5037822244": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4151690cf1024261bda7e829cb602335": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad388374faae490d8d4eb42883a87fbf",
              "IPY_MODEL_8399da3d4720436f95399bf307478c54",
              "IPY_MODEL_d1b8ca4a12c246169784f3d9d8fe1e17"
            ],
            "layout": "IPY_MODEL_6c9a3e4055684bcca4886ad2a833a380"
          }
        },
        "4bff9bb13df54448adaf27b856995d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50a8b6f9b558458491ee63f370d7abb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "540ed8a1e13440ffb2d8b18cc37f96d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58bd02c99bf54807b5e2ce6b3a3862c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59e57bc6f9314dccba5043a5632ad963": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a304e2a50444ae2881d61f25441836b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b30f78e1491422eb0e134ec9f0ea444": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed2cdc72a8646c0844b963cf28aec54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65a3ea9bde1f4488879241547a9b568f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a02c7c68fd24e5cafb1d364f7b8b823": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6acd8bf3e37d4883987bd7ebf59fe290": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36aeab7d08574733a963bb8d3d3a81db",
              "IPY_MODEL_dff8cc9466674aeaa9d2a5465b52890c",
              "IPY_MODEL_12531fc8b70640e3b33887f8464d2154"
            ],
            "layout": "IPY_MODEL_f811900057054203bed58a75a524540c"
          }
        },
        "6b85cfcc0635410ab1fcf9113e351382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c9a3e4055684bcca4886ad2a833a380": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cce4850f67a4c53a683dd0ad2301bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f6a76601874c94bc7e59887ce0fae7",
              "IPY_MODEL_c178c46dfe8a465a86483869c1f0258e",
              "IPY_MODEL_ae6aff2eeede48d99f8e8dc15c2ca755"
            ],
            "layout": "IPY_MODEL_a57c09b3d79c4edfb0e7ed78224b930f"
          }
        },
        "6cdaeb6fc4774566927e6e2a91fb9803": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa3fb96e00cc493f990ae483c6c7d226",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_941b9ffbd767472699308f7da50109e9",
            "value": 26
          }
        },
        "6e542f42661a4e728b4d1ac3b0a1fa05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "728c80f51a1b46919f42f6240bda8a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74103ed24fb44ab08c6f2775c21b2bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bcca16465274233aca5b76a8fb4f8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f7ae66881d349a69c6b96efbd8ab353": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8399da3d4720436f95399bf307478c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_540ed8a1e13440ffb2d8b18cc37f96d3",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ad3f03bc06c48b08a1458be3a67bc52",
            "value": 124
          }
        },
        "88459cb20e4e4bb1ab670209a72813af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88bde829528d42c1a46e348aa1767caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e3506dee6fa4d498607cafc41cd452c",
            "placeholder": "​",
            "style": "IPY_MODEL_b3032884412046bea0152c498a543feb",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.41kB/s]"
          }
        },
        "941b9ffbd767472699308f7da50109e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "946c68a6c2cd44cc887e8dc0a6c1e3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a57c09b3d79c4edfb0e7ed78224b930f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f6a76601874c94bc7e59887ce0fae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9f7c4c8ba684e32a16469d7c670f4e7",
            "placeholder": "​",
            "style": "IPY_MODEL_a82eb92cfa034f6c93f82aa87bce1721",
            "value": "config.json: 100%"
          }
        },
        "a64189033432400a8828ffe4ca7e8a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_087acdcc8c474355b5f83bc8ad68845a",
            "placeholder": "​",
            "style": "IPY_MODEL_ac844ec318374958aa225a5b696e6773",
            "value": "merges.txt: 100%"
          }
        },
        "a82eb92cfa034f6c93f82aa87bce1721": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa3fb96e00cc493f990ae483c6c7d226": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab26ecddc4974dec82ea01ab6e0e8c25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab59340944634038ac23389d80192a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac844ec318374958aa225a5b696e6773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad388374faae490d8d4eb42883a87fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_728c80f51a1b46919f42f6240bda8a5b",
            "placeholder": "​",
            "style": "IPY_MODEL_c84e74a8f68449ffbefe184d750edac2",
            "value": "generation_config.json: 100%"
          }
        },
        "adf5d6c4d5494ba4a1b33edf2dcc54fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae6aff2eeede48d99f8e8dc15c2ca755": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfe2b1bb7d99498d992c097d95f35968",
            "placeholder": "​",
            "style": "IPY_MODEL_adf5d6c4d5494ba4a1b33edf2dcc54fe",
            "value": " 665/665 [00:00&lt;00:00, 53.1kB/s]"
          }
        },
        "b03bb99fab944f69b57f2d06cecb61bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f612b533e1e542e997961e87611c1ccc",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bcca16465274233aca5b76a8fb4f8c7",
            "value": 1042301
          }
        },
        "b3032884412046bea0152c498a543feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b91aa7f7e8d64b158633faedf0c3ae3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7ae66881d349a69c6b96efbd8ab353",
            "placeholder": "​",
            "style": "IPY_MODEL_5b30f78e1491422eb0e134ec9f0ea444",
            "value": "vocab.json: 100%"
          }
        },
        "bb0ef7fc24b64014b71aa9daa1f4a19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ea32e1f9ecd4db6913ee4422a115628",
            "placeholder": "​",
            "style": "IPY_MODEL_e10e095d211045e59c07d0f3c7327595",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 6.99MB/s]"
          }
        },
        "bba64c723e2b43a892b45c1bde34b045": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c002fed93e19457f9c285c573868f258": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59e57bc6f9314dccba5043a5632ad963",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65a3ea9bde1f4488879241547a9b568f",
            "value": 548105171
          }
        },
        "c178c46dfe8a465a86483869c1f0258e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8207693d83b400fac385dd6a8337013",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b85cfcc0635410ab1fcf9113e351382",
            "value": 665
          }
        },
        "c71b822a0bed48e797d5db02c7efa67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2925d6752ec8424e8d88672e7002133b",
              "IPY_MODEL_c002fed93e19457f9c285c573868f258",
              "IPY_MODEL_12b1f6e9254e4908bc6117a2f24fe205"
            ],
            "layout": "IPY_MODEL_d616ddfcdf854b86a51a5ba450573d43"
          }
        },
        "c71c70e3ced2416b85682b5dcbe09741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f248ef5afb7b41ee8fb4bd1a14e66a1a",
              "IPY_MODEL_1090886723ec4c4aa5ac4719bc7fdbc6",
              "IPY_MODEL_d13f43435bba443db5bcacc000baea47"
            ],
            "layout": "IPY_MODEL_946c68a6c2cd44cc887e8dc0a6c1e3c2"
          }
        },
        "c84e74a8f68449ffbefe184d750edac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9f7c4c8ba684e32a16469d7c670f4e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d08bdf4fe1b1437eaf6f66cd19e34013": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b91aa7f7e8d64b158633faedf0c3ae3e",
              "IPY_MODEL_b03bb99fab944f69b57f2d06cecb61bd",
              "IPY_MODEL_bb0ef7fc24b64014b71aa9daa1f4a19c"
            ],
            "layout": "IPY_MODEL_dcaae39989f94cca969e193650f194a6"
          }
        },
        "d13f43435bba443db5bcacc000baea47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74103ed24fb44ab08c6f2775c21b2bf6",
            "placeholder": "​",
            "style": "IPY_MODEL_3fd3306a8a364b81b5b09a5037822244",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 19.7MB/s]"
          }
        },
        "d1b8ca4a12c246169784f3d9d8fe1e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50a8b6f9b558458491ee63f370d7abb1",
            "placeholder": "​",
            "style": "IPY_MODEL_ab59340944634038ac23389d80192a4d",
            "value": " 124/124 [00:00&lt;00:00, 4.32kB/s]"
          }
        },
        "d4435fe59a9345c89572c104e6584fec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d616ddfcdf854b86a51a5ba450573d43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7011cfd63d94da6887ab0207a9259c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c365c2504e42478a31e75d8229e586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a304e2a50444ae2881d61f25441836b",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a89ffaff894b16923ec65f0f2f79b3",
            "value": " 456k/456k [00:00&lt;00:00, 12.2MB/s]"
          }
        },
        "dcaae39989f94cca969e193650f194a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfe2b1bb7d99498d992c097d95f35968": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff8cc9466674aeaa9d2a5465b52890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc90c39920c4f69b81eea3d63bb2e1a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18a4ee63690c40e1a96f5ba66f900714",
            "value": 2
          }
        },
        "e10e095d211045e59c07d0f3c7327595": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5a89ffaff894b16923ec65f0f2f79b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f248ef5afb7b41ee8fb4bd1a14e66a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14552df3f0734fdaae4d25a484f753f7",
            "placeholder": "​",
            "style": "IPY_MODEL_fc757a3a80e34550823807faea3bfd8f",
            "value": "tokenizer.json: 100%"
          }
        },
        "f612b533e1e542e997961e87611c1ccc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f811900057054203bed58a75a524540c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8207693d83b400fac385dd6a8337013": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc757a3a80e34550823807faea3bfd8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcc90c39920c4f69b81eea3d63bb2e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7df56bdd234795a48afc908363a673": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a49e213dee74557b9760dbc9468858f",
              "IPY_MODEL_6cdaeb6fc4774566927e6e2a91fb9803",
              "IPY_MODEL_88bde829528d42c1a46e348aa1767caa"
            ],
            "layout": "IPY_MODEL_58bd02c99bf54807b5e2ce6b3a3862c3"
          }
        },
        "ff410ad5e1db4173b981b23a31a1f289": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a64189033432400a8828ffe4ca7e8a5d",
              "IPY_MODEL_1ab3362c3aa041a1a4eaaa5f8fc4c522",
              "IPY_MODEL_d8c365c2504e42478a31e75d8229e586"
            ],
            "layout": "IPY_MODEL_07a97ea5ea2f4d619445b3d3998cb385"
          }
        },
        "ff550bbf63534c228f381531370a4b94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe70228d49e4cb5b4533c33d57f6de5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fc1522525649a38f1ae96410b4170b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90503c2141844d1a943870f42d0d1564",
              "IPY_MODEL_79ef355c55ff4279b0b6a56e27eac41e",
              "IPY_MODEL_03356978a19d48958a6b987823f8a8a1"
            ],
            "layout": "IPY_MODEL_607747b751f94cb197571e3a9d269dc6"
          }
        },
        "90503c2141844d1a943870f42d0d1564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb2823eeee14209adefb4a381a5b5b1",
            "placeholder": "​",
            "style": "IPY_MODEL_9873724a000a458a8f8c05390a3d9fab",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "79ef355c55ff4279b0b6a56e27eac41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_388a6135d5cd47b7b3dee1352b55860e",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f00447dcb5914a3d9dec1f4ab620d847",
            "value": 34173
          }
        },
        "03356978a19d48958a6b987823f8a8a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_440c652c57984ef59ff51d89b60d43ad",
            "placeholder": "​",
            "style": "IPY_MODEL_c970aef5ce254d32b112046b381ce2fe",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 3.76MB/s]"
          }
        },
        "607747b751f94cb197571e3a9d269dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb2823eeee14209adefb4a381a5b5b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9873724a000a458a8f8c05390a3d9fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "388a6135d5cd47b7b3dee1352b55860e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00447dcb5914a3d9dec1f4ab620d847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "440c652c57984ef59ff51d89b60d43ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c970aef5ce254d32b112046b381ce2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a92d259f60d84704961a728d75fc1801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfba185289cd4bfebc5541469a845148",
              "IPY_MODEL_ff58d8fe78724421a55e43c4d0dc64b7",
              "IPY_MODEL_7414cc2c1fa548a78736924f25703c29"
            ],
            "layout": "IPY_MODEL_da941b949bd641f2b26b0111a710443a"
          }
        },
        "dfba185289cd4bfebc5541469a845148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e9c10e3e0f7410c8da75e4c500f930e",
            "placeholder": "​",
            "style": "IPY_MODEL_83156b4fa27e475f90bf6f788c95d708",
            "value": "tokenizer.model: 100%"
          }
        },
        "ff58d8fe78724421a55e43c4d0dc64b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c4465bcdba94819823207975bfc5f57",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d50b49387e414898839b2e383a1f6e4a",
            "value": 4241003
          }
        },
        "7414cc2c1fa548a78736924f25703c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6d7074c27a4437b63daf603341e141",
            "placeholder": "​",
            "style": "IPY_MODEL_96f1c80da31046b5ada7569afff99a24",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 34.2MB/s]"
          }
        },
        "da941b949bd641f2b26b0111a710443a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e9c10e3e0f7410c8da75e4c500f930e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83156b4fa27e475f90bf6f788c95d708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4465bcdba94819823207975bfc5f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d50b49387e414898839b2e383a1f6e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f6d7074c27a4437b63daf603341e141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f1c80da31046b5ada7569afff99a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9af107f45eb44ff09cd2e9b8cad22c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e03d96c6bd6401b808894669f791f7e",
              "IPY_MODEL_099d969d3d2b48cd8f9758206c1352e0",
              "IPY_MODEL_ca3d62f9c04b405384d32806e05407f5"
            ],
            "layout": "IPY_MODEL_57f6295cd9af49a9b80bae8c6223f09d"
          }
        },
        "5e03d96c6bd6401b808894669f791f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_485c429733014b31ae427cf99bdc8f03",
            "placeholder": "​",
            "style": "IPY_MODEL_5d45b220b9154e2f86393729bec59909",
            "value": "tokenizer.json: 100%"
          }
        },
        "099d969d3d2b48cd8f9758206c1352e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d668eef4ecf46b681c4cb6ca11591c3",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cd9d3d4740346b0a005e023aaf68c82",
            "value": 17518497
          }
        },
        "ca3d62f9c04b405384d32806e05407f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f65199e23b9b4429935e6eceffc77679",
            "placeholder": "​",
            "style": "IPY_MODEL_d7958c18d9cd456bb3dae566452fd820",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 159MB/s]"
          }
        },
        "57f6295cd9af49a9b80bae8c6223f09d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485c429733014b31ae427cf99bdc8f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d45b220b9154e2f86393729bec59909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d668eef4ecf46b681c4cb6ca11591c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd9d3d4740346b0a005e023aaf68c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f65199e23b9b4429935e6eceffc77679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7958c18d9cd456bb3dae566452fd820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d7d0fc7c16d4ab88e99aab2f1f10186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a417bd961f844badb4506af79a7f6c35",
              "IPY_MODEL_6cc01642394c462699ff75c0f39a1ada",
              "IPY_MODEL_e1f3c3f6f4604c78b83dad6df261d7c7"
            ],
            "layout": "IPY_MODEL_97d184f36e294400b3edb5e928d6d2cd"
          }
        },
        "a417bd961f844badb4506af79a7f6c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e2984f5d3974e159d00c5a89d4e9091",
            "placeholder": "​",
            "style": "IPY_MODEL_c1df11c8a17d4c849256023faa6cc2c9",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6cc01642394c462699ff75c0f39a1ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b2feed82a54e15b4347bf86f06f4b9",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be4397ec9ae94d66aae46b0c6c97dc6a",
            "value": 636
          }
        },
        "e1f3c3f6f4604c78b83dad6df261d7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da5bb40ef4af480fb34764153e388759",
            "placeholder": "​",
            "style": "IPY_MODEL_24012eae2f6f4b67b3db4434efc97a35",
            "value": " 636/636 [00:00&lt;00:00, 75.1kB/s]"
          }
        },
        "97d184f36e294400b3edb5e928d6d2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e2984f5d3974e159d00c5a89d4e9091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1df11c8a17d4c849256023faa6cc2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6b2feed82a54e15b4347bf86f06f4b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4397ec9ae94d66aae46b0c6c97dc6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da5bb40ef4af480fb34764153e388759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24012eae2f6f4b67b3db4434efc97a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abdc9915a4c74a4186eea2d420ca22e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbb102d20a79497d9ee300a8ca95419b",
              "IPY_MODEL_b88ed5b93d284a938bdb052ac41ea64b",
              "IPY_MODEL_f19975c82c1540e99befd3ed16dc06b4"
            ],
            "layout": "IPY_MODEL_57920ad91570446e84ae32fdab7624e3"
          }
        },
        "bbb102d20a79497d9ee300a8ca95419b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3f3e3406984854bb9e9f14aee024da",
            "placeholder": "​",
            "style": "IPY_MODEL_5e940f0f819141899d75dabd2d5cb523",
            "value": "config.json: 100%"
          }
        },
        "b88ed5b93d284a938bdb052ac41ea64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_625ce3cd8b0d45f2826bad30876a1e04",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_348671d759f640869d23ba6efe1c1557",
            "value": 627
          }
        },
        "f19975c82c1540e99befd3ed16dc06b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_facabc990703422d8900a4d61644b524",
            "placeholder": "​",
            "style": "IPY_MODEL_84a4093337314610818c42a6da6174f3",
            "value": " 627/627 [00:00&lt;00:00, 75.9kB/s]"
          }
        },
        "57920ad91570446e84ae32fdab7624e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a3f3e3406984854bb9e9f14aee024da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e940f0f819141899d75dabd2d5cb523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "625ce3cd8b0d45f2826bad30876a1e04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348671d759f640869d23ba6efe1c1557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "facabc990703422d8900a4d61644b524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84a4093337314610818c42a6da6174f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bae13fe72c5e439485597e8e80e8465c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b808af808de249bbb222ce87f70f9920",
              "IPY_MODEL_ee4dd76746544ba6bcd43c34532fd788",
              "IPY_MODEL_c5cf90fceed547c09a2c54f87a04c70e"
            ],
            "layout": "IPY_MODEL_2babb7d63f734e0bbe2ee2e96fcfe46f"
          }
        },
        "b808af808de249bbb222ce87f70f9920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0040399ab6641eca288c2b458e820ab",
            "placeholder": "​",
            "style": "IPY_MODEL_77860de1af984dd5b44aadf8eb3dfb9a",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "ee4dd76746544ba6bcd43c34532fd788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02b5544f4ec6476d823518b605260c4e",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e6bcfd849fd41b98b0e605ff47d759e",
            "value": 13489
          }
        },
        "c5cf90fceed547c09a2c54f87a04c70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f113c193b654ac6b7589f8eb1ecf1ea",
            "placeholder": "​",
            "style": "IPY_MODEL_5e38841727814fa8bf8823a2b294a9c3",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 1.46MB/s]"
          }
        },
        "2babb7d63f734e0bbe2ee2e96fcfe46f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0040399ab6641eca288c2b458e820ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77860de1af984dd5b44aadf8eb3dfb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02b5544f4ec6476d823518b605260c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6bcfd849fd41b98b0e605ff47d759e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f113c193b654ac6b7589f8eb1ecf1ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e38841727814fa8bf8823a2b294a9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0048fccd8af0429f94c3a505cfdd4d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc47a786e6cc406e99d7549771ebbccc",
              "IPY_MODEL_785818fd7d014b51bd532fa58f5c713f",
              "IPY_MODEL_4751b20c139649a991a2df43be4a0066"
            ],
            "layout": "IPY_MODEL_91e920c0015a4704a3b0b9fcd4b88c64"
          }
        },
        "cc47a786e6cc406e99d7549771ebbccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56e976bcc8446069657752224ec20fc",
            "placeholder": "​",
            "style": "IPY_MODEL_409fa822e6a0439f8c20bd57241fa963",
            "value": "Fetching 2 files: 100%"
          }
        },
        "785818fd7d014b51bd532fa58f5c713f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb60ca90db75486cab237033d5e6d059",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfdfdd112038482192716dfada8ec586",
            "value": 2
          }
        },
        "4751b20c139649a991a2df43be4a0066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cadf2520b29451bb7780d9984b4334c",
            "placeholder": "​",
            "style": "IPY_MODEL_aa0d33f9b15e44ffa4398c23db91f0c1",
            "value": " 2/2 [00:13&lt;00:00, 13.11s/it]"
          }
        },
        "91e920c0015a4704a3b0b9fcd4b88c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56e976bcc8446069657752224ec20fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "409fa822e6a0439f8c20bd57241fa963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb60ca90db75486cab237033d5e6d059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfdfdd112038482192716dfada8ec586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cadf2520b29451bb7780d9984b4334c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0d33f9b15e44ffa4398c23db91f0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93f46fbca6b54f408dbccbf7ded2a21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cda5ac508e5d48c1a8463debcce03786",
              "IPY_MODEL_4b71f36bf67d4cf2a94dcba10ad21833",
              "IPY_MODEL_1229e2b8a1a649ebb79ac66430042aab"
            ],
            "layout": "IPY_MODEL_4ba750060b0d4cd8a13368998e8aa220"
          }
        },
        "cda5ac508e5d48c1a8463debcce03786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faa666d2337e412a9ca1ce3dddd0634f",
            "placeholder": "​",
            "style": "IPY_MODEL_776f554abdf647e7b62bd4d1ca7c73e7",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "4b71f36bf67d4cf2a94dcba10ad21833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa9fe8c0380e45219e237cd2668a1a2b",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0502159dc034a6695bfc3ce456d9726",
            "value": 4945242264
          }
        },
        "1229e2b8a1a649ebb79ac66430042aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd27b4d597044f39bda39e582c6844d1",
            "placeholder": "​",
            "style": "IPY_MODEL_8f0a78c968ca441b9bbcc7dfff7591f1",
            "value": " 4.95G/4.95G [00:12&lt;00:00, 415MB/s]"
          }
        },
        "4ba750060b0d4cd8a13368998e8aa220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faa666d2337e412a9ca1ce3dddd0634f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776f554abdf647e7b62bd4d1ca7c73e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa9fe8c0380e45219e237cd2668a1a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0502159dc034a6695bfc3ce456d9726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd27b4d597044f39bda39e582c6844d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f0a78c968ca441b9bbcc7dfff7591f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44a677416e3b4a8593de047e9bc968b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6fcf854e7314f9f9d7ad4e2ef327ae2",
              "IPY_MODEL_9313390c7a694df893ed46e13efcc87e",
              "IPY_MODEL_e3c48cc8caf941b3babf0a5816681d28"
            ],
            "layout": "IPY_MODEL_18029dc47f1543918662b76d778dbfd4"
          }
        },
        "d6fcf854e7314f9f9d7ad4e2ef327ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcfcd2b735884616aafba383d3cfb98c",
            "placeholder": "​",
            "style": "IPY_MODEL_d9b24d3eb10448c597f98b81faeb2dbe",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "9313390c7a694df893ed46e13efcc87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e503f73bd6f840ccb645bf9994ad13f7",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2016665e4b574a868d348e070d91e22f",
            "value": 67121608
          }
        },
        "e3c48cc8caf941b3babf0a5816681d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64d52abdb6c1426c8a2c8d371068685f",
            "placeholder": "​",
            "style": "IPY_MODEL_ee773f37046d4844abdc79288b91c836",
            "value": " 67.1M/67.1M [00:00&lt;00:00, 159MB/s]"
          }
        },
        "18029dc47f1543918662b76d778dbfd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfcd2b735884616aafba383d3cfb98c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b24d3eb10448c597f98b81faeb2dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e503f73bd6f840ccb645bf9994ad13f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2016665e4b574a868d348e070d91e22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64d52abdb6c1426c8a2c8d371068685f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee773f37046d4844abdc79288b91c836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ff03200d5ee49d0a485437d584218b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67a3f934d1d843a7984398a505e3a0d9",
              "IPY_MODEL_3d5b199d914d42b183bf704c892f13da",
              "IPY_MODEL_d9fba5e602cb414fa5ce5a6f5f10852f"
            ],
            "layout": "IPY_MODEL_a29120c8a07b41429f8b2a3d7e1fb4ed"
          }
        },
        "67a3f934d1d843a7984398a505e3a0d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1adcd96c77a241e58ccc2c044e5dd01a",
            "placeholder": "​",
            "style": "IPY_MODEL_2f9864116bd44f9c87d624800ccf0a38",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3d5b199d914d42b183bf704c892f13da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75193da6a3574a7baafb0be5a19e54f2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ebadc395d6547f994fb0afdc1d97aff",
            "value": 2
          }
        },
        "d9fba5e602cb414fa5ce5a6f5f10852f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bdaf75525dc41fda8f7687143519bd5",
            "placeholder": "​",
            "style": "IPY_MODEL_8d1ea9ef26f14430b899c516cf6a51b9",
            "value": " 2/2 [00:01&lt;00:00,  1.55s/it]"
          }
        },
        "a29120c8a07b41429f8b2a3d7e1fb4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1adcd96c77a241e58ccc2c044e5dd01a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9864116bd44f9c87d624800ccf0a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75193da6a3574a7baafb0be5a19e54f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ebadc395d6547f994fb0afdc1d97aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bdaf75525dc41fda8f7687143519bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d1ea9ef26f14430b899c516cf6a51b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9d41a01f0a44fd6a32428af44b4f6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfa5807dda1b4c49932879beb41aac22",
              "IPY_MODEL_f6f22d8615714892977d42fde0f0fc85",
              "IPY_MODEL_c61d361bf2fd41a98ae8b8f57e68dd88"
            ],
            "layout": "IPY_MODEL_82e63d7f21d7490989fe46b789ff3c7b"
          }
        },
        "dfa5807dda1b4c49932879beb41aac22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb59ae445274d06a066103f04cbaaf3",
            "placeholder": "​",
            "style": "IPY_MODEL_a06c6055296c4e4eb1efb5d1434df129",
            "value": "generation_config.json: 100%"
          }
        },
        "f6f22d8615714892977d42fde0f0fc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da37f1b672d4d1d8fd033dff1ad7ef0",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03210a6ad76b40a8bbbbf3325d3ba733",
            "value": 137
          }
        },
        "c61d361bf2fd41a98ae8b8f57e68dd88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe09c6c497a74e85a52de5cfc1e2840e",
            "placeholder": "​",
            "style": "IPY_MODEL_c95d93f3c0b64bca9bc1c3fede0dca42",
            "value": " 137/137 [00:00&lt;00:00, 17.0kB/s]"
          }
        },
        "82e63d7f21d7490989fe46b789ff3c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fb59ae445274d06a066103f04cbaaf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06c6055296c4e4eb1efb5d1434df129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9da37f1b672d4d1d8fd033dff1ad7ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03210a6ad76b40a8bbbbf3325d3ba733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe09c6c497a74e85a52de5cfc1e2840e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c95d93f3c0b64bca9bc1c3fede0dca42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1686956422df4f5db50902304eba7ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed3d48a9bda64f3b9baac8045ca99e53",
              "IPY_MODEL_dc0cb291268a4a16ba9f851b552646b5",
              "IPY_MODEL_69ffbfdaf96a431aaeffdc277baa58d9"
            ],
            "layout": "IPY_MODEL_d2113c4bfd4640ae90e35051af4e5cd6"
          }
        },
        "ed3d48a9bda64f3b9baac8045ca99e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_003cbc2eb4554715a5aa2406f0c163b7",
            "placeholder": "​",
            "style": "IPY_MODEL_daa0afe957b240809a74f76c975af6d6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dc0cb291268a4a16ba9f851b552646b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27da3fbd968746749af6edea1da0d54f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4980501a0c2e46dc8163ce0b0b16e446",
            "value": 2
          }
        },
        "69ffbfdaf96a431aaeffdc277baa58d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9696faf81280404eafa2db16003d1005",
            "placeholder": "​",
            "style": "IPY_MODEL_663fd8bb2c5f4a4ebb8c76328c507676",
            "value": " 2/2 [00:02&lt;00:00,  2.03s/it]"
          }
        },
        "d2113c4bfd4640ae90e35051af4e5cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "003cbc2eb4554715a5aa2406f0c163b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daa0afe957b240809a74f76c975af6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27da3fbd968746749af6edea1da0d54f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4980501a0c2e46dc8163ce0b0b16e446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9696faf81280404eafa2db16003d1005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663fd8bb2c5f4a4ebb8c76328c507676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a38e717587aa471389bdf377f0541872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c930577f062043e1820baf07966395ba",
              "IPY_MODEL_e7a2923004c545ef88ea09ea59c58b09",
              "IPY_MODEL_d4ac9ca895b049a18c825981c2a7566c"
            ],
            "layout": "IPY_MODEL_5ffc74d782b241e391a666a1f62af22a"
          }
        },
        "c930577f062043e1820baf07966395ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f70c576338ba40a180acd09acd007b4a",
            "placeholder": "​",
            "style": "IPY_MODEL_222f52ce6f7e4881a4cdd2e07c3c7516",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e7a2923004c545ef88ea09ea59c58b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f396fe034c46a49310bde8ec1bd02a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce94332d5a3a40a4841bc6232786b680",
            "value": 2
          }
        },
        "d4ac9ca895b049a18c825981c2a7566c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893f3244ac6e4c6081b5688c030d90f9",
            "placeholder": "​",
            "style": "IPY_MODEL_663c5c0c81a14b3fa90feadcb6da44bb",
            "value": " 2/2 [00:02&lt;00:00,  2.04s/it]"
          }
        },
        "5ffc74d782b241e391a666a1f62af22a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f70c576338ba40a180acd09acd007b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "222f52ce6f7e4881a4cdd2e07c3c7516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07f396fe034c46a49310bde8ec1bd02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce94332d5a3a40a4841bc6232786b680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "893f3244ac6e4c6081b5688c030d90f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663c5c0c81a14b3fa90feadcb6da44bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1aafce16dd4a1c936a6bf56f42af51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d0e1a8f5724521ab5aa21a8d680adc",
              "IPY_MODEL_b9c932c63f744997b441f244617e7c30",
              "IPY_MODEL_70c6826bf5664771a475c978687508d6"
            ],
            "layout": "IPY_MODEL_510c2e2ca231492aa1ee6d2b4f5f354a"
          }
        },
        "36d0e1a8f5724521ab5aa21a8d680adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b62a020b225487a8530cfc7d31cdcfa",
            "placeholder": "​",
            "style": "IPY_MODEL_280d6088dab443cc818bfff892c92879",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b9c932c63f744997b441f244617e7c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65cc68f572174b56aceb53daab203267",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ef04e8c3a2e43f3a5bf09eaf87a14ab",
            "value": 2
          }
        },
        "70c6826bf5664771a475c978687508d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd919d4a0ba24cf4b5fa46cc30e0aff2",
            "placeholder": "​",
            "style": "IPY_MODEL_e4a5f4d1834e40fd87ec2866565378f8",
            "value": " 2/2 [00:01&lt;00:00,  1.38s/it]"
          }
        },
        "510c2e2ca231492aa1ee6d2b4f5f354a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b62a020b225487a8530cfc7d31cdcfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280d6088dab443cc818bfff892c92879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65cc68f572174b56aceb53daab203267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef04e8c3a2e43f3a5bf09eaf87a14ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd919d4a0ba24cf4b5fa46cc30e0aff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4a5f4d1834e40fd87ec2866565378f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}