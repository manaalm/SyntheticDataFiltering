# -*- coding: utf-8 -*-
"""privacy data project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sC5E3V8wglenRSbmzommddy-1FffC1dR
"""

!pip install sentence_transformers

import sys
import os
from typing import List, Dict, Tuple, Any, Optional, Union, Callable
import json

import logging
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
from transformers import LogitsProcessorList, StoppingCriteriaList, StoppingCriteria
from transformers.generation.logits_process import (
    LogitsProcessor,
    RepetitionPenaltyLogitsProcessor,
    TopKLogitsWarper,
    TopPLogitsWarper,
)

import pandas as pd
import re
import logging
from typing import Dict, List, Tuple, Set, Optional, Union

import spacy
from spacy.tokens import Doc, Span, Token

PII_PATTERNS = {
    'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'phone_us': r'\b(\+\d{1,2}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b',
    'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
    'credit_card': r'\b(?:\d{4}[-\s]?){3}\d{4}\b',
    'ip_address': r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
    'customer_id': r'\bCUST-\d{4}-\d{4}\b',
    'url': r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w\.-]*\??[-\w&=%]*',
    'date_of_birth': r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b',
    'zip_code': r'\b\d{5}(?:-\d{4})?\b',
}

# Compiled regex patterns for faster matching
COMPILED_PATTERNS = {name: re.compile(pattern) for name, pattern in PII_PATTERNS.items()}

# Entity types to detect with NER
NER_ENTITY_TYPES = [
    'PERSON',
    'ORG',
    'GPE',  # Geo-political entities (countries, cities)
    'LOC',  # Non-GPE locations
    'FACILITY',
    'PRODUCT',
    'EVENT',
    'WORK_OF_ART',
    'LAW',
    'LANGUAGE',
    'FAC',  # Buildings, airports
    'NORP'  # Nationalities, religious groups
]

# Placeholder formats
def get_placeholder(entity_type, index=None):
    """Generate a placeholder for the detected entity type."""
    if index is not None:
        return f"[{entity_type}_{index}]"
    return f"[{entity_type}]"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PreGenerationFilter:
    """
    Pre-Generation Filter that combines regex-based and NER-based PII detection
    with token-level redaction capabilities.
    """

    def __init__(
        self,
        spacy_model: str = "en_core_web_sm",
        use_token_level: bool = True,
        preserve_context: bool = True,
        redaction_char: str = "â–ˆ",
        default_placeholder: str = "[REDACTED]",
        type_specific_placeholders: bool = True,
        min_entity_length: int = 2,
        custom_patterns: Dict[str, str] = None,
        excluded_entity_types: Set[str] = None
    ):
        """
        Initialize the Pre-Generation Filter.

        Args:
            spacy_model: The spaCy model to use for NER.
            use_token_level: Whether to use token-level redaction (vs. whole entity).
            preserve_context: Whether to preserve context around redacted entities.
            redaction_char: Character used for inline redaction.
            default_placeholder: Default placeholder for redacted entities.
            type_specific_placeholders: Whether to use type-specific placeholders.
            min_entity_length: Minimum length for entities to be redacted.
            custom_patterns: Additional regex patterns to use for detection.
            excluded_entity_types: Entity types to exclude from redaction.
        """
        try:
            self.nlp = spacy.load(spacy_model)
            logger.info(f"Loaded spaCy model: {spacy_model}")
        except Exception as e:
            logger.error(f"Failed to load spaCy model: {str(e)}")
            logger.warning("Downloading spaCy model...")
            spacy.cli.download(spacy_model)
            self.nlp = spacy.load(spacy_model)

        self.use_token_level = use_token_level
        self.preserve_context = preserve_context
        self.redaction_char = redaction_char
        self.default_placeholder = default_placeholder
        self.type_specific_placeholders = type_specific_placeholders
        self.min_entity_length = min_entity_length
        self.excluded_entity_types = excluded_entity_types or set()

        # Use default patterns and add any custom ones
        self.patterns = dict(COMPILED_PATTERNS)
        if custom_patterns:
            for name, pattern in custom_patterns.items():
                self.patterns[name] = re.compile(pattern)

        # Track replacement counts for reporting
        self.replacements = {
            "regex": 0,
            "ner": 0,
            "total": 0
        }

    def reset_counts(self):
        """Reset the replacement counters."""
        self.replacements = {key: 0 for key in self.replacements}

    def process(self, text: str) -> Tuple[str, Dict]:
        """
        Process the input text to detect and redact PII.

        Args:
            text: The input text to process.

        Returns:
            Tuple containing:
            - The processed text with redacted PII
            - Statistics about the redaction process
        """
        if not text:
            return "", {"error": "Empty text provided"}

        self.reset_counts()

        # Process text with spaCy for NER
        doc = self.nlp(text)

        # Token-level tracking for entities to redact
        tokens_to_redact = []
        entity_tracking = {}  # Track entity replacements

        # First pass: NER-based detection
        for ent in doc.ents:
            if (ent.label_ in NER_ENTITY_TYPES and
                ent.label_ not in self.excluded_entity_types and
                len(ent.text) >= self.min_entity_length):

                # Generate placeholder based on entity type
                entity_type = ent.label_
                count = entity_tracking.get(entity_type, 0) + 1
                entity_tracking[entity_type] = count

                if self.type_specific_placeholders:
                    placeholder = get_placeholder(entity_type, count)
                else:
                    placeholder = self.default_placeholder

                if self.use_token_level:
                    # Mark all tokens in this entity for redaction
                    for token in ent:
                        tokens_to_redact.append((token.i, token.text, placeholder))
                else:
                    # Replace the whole entity span
                    tokens_to_redact.extend([(token.i, token.text, placeholder) for token in ent])

                self.replacements["ner"] += 1

        # Second pass: regex-based detection for structured PII
        for token in doc:
            # Skip tokens already marked for redaction
            if any(token.i == t[0] for t in tokens_to_redact):
                continue

            for pattern_name, pattern in self.patterns.items():
                if pattern.search(token.text):
                    placeholder = get_placeholder(pattern_name.upper()) if self.type_specific_placeholders else self.default_placeholder
                    tokens_to_redact.append((token.i, token.text, placeholder))
                    self.replacements["regex"] += 1
                    break  # Once a token is matched by one pattern, no need to check others

        # Apply redactions
        result = self._apply_token_redactions(doc, tokens_to_redact)

        # Update total replacements count
        self.replacements["total"] = self.replacements["regex"] + self.replacements["ner"]

        stats = {
            "input_length": len(text),
            "output_length": len(result),
            "replacements": dict(self.replacements),
            "entity_types": list(entity_tracking.keys())
        }

        return result, stats

    def _apply_token_redactions(self, doc: Doc, tokens_to_redact: List[Tuple[int, str, str]]) -> str:
        """
        Apply token-level redactions to the document.

        Args:
            doc: The spaCy Doc object.
            tokens_to_redact: List of (token_idx, original_text, placeholder) tuples.

        Returns:
            The redacted text.
        """
        if not tokens_to_redact:
            return doc.text

        # Sort by token index to process in order
        tokens_to_redact.sort(key=lambda x: x[0])

        # Group consecutive tokens with the same placeholder
        grouped_tokens = []
        current_group = [tokens_to_redact[0]]

        for i in range(1, len(tokens_to_redact)):
            prev_token = tokens_to_redact[i-1]
            curr_token = tokens_to_redact[i]

            # If consecutive and same placeholder, add to group
            if curr_token[0] == prev_token[0] + 1 and curr_token[2] == prev_token[2]:
                current_group.append(curr_token)
            else:
                grouped_tokens.append(current_group)
                current_group = [curr_token]

        # Add the last group
        if current_group:
            grouped_tokens.append(current_group)

        # Create the redacted text
        result = []
        last_idx = 0

        for group in grouped_tokens:
            start_token_idx = group[0][0]
            end_token_idx = group[-1][0]
            placeholder = group[0][2]

            # Add text before this redaction
            if start_token_idx > 0 and last_idx < start_token_idx:
                result.append(doc[last_idx:start_token_idx].text_with_ws.rstrip())

            # Add the placeholder
            result.append(placeholder)

            # Update last index
            last_idx = end_token_idx + 1

        # Add any remaining text
        if last_idx < len(doc):
            result.append(doc[last_idx:].text)

        return " ".join(result)

    def redact_entities(self, text: str) -> str:
        """
        Simple interface to redact PII from text.

        Args:
            text: The input text to redact.

        Returns:
            The redacted text.
        """
        redacted_text, _ = self.process(text)
        return redacted_text

!pip install --upgrade transformers accelerate

# Configure logging
import accelerate
from accelerate import Accelerator
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BlockedWordsLogitsProcessor(LogitsProcessor):
    """
    LogitsProcessor that blocks specific tokens or sequences from being generated.
    """

    def __init__(self, tokenizer, blocked_words: List[str], min_probability: float = -100.0):
        """
        Initialize the processor.

        Args:
            tokenizer: The tokenizer used by the model.
            blocked_words: List of words or phrases to block.
            min_probability: Log probability assigned to blocked tokens (default: -100.0).
        """
        self.tokenizer = tokenizer
        self.min_probability = min_probability
        self.blocked_token_ids = []

        # Convert words to token ids
        for word in blocked_words:
            token_ids = tokenizer.encode(word, add_special_tokens=False)
            if token_ids:  # Ensure we have valid token IDs
                self.blocked_token_ids.append(token_ids)

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        """
        Process logits to block specific tokens.

        Args:
            input_ids: Token IDs already generated.
            scores: Current token scores/logits.

        Returns:
            Modified scores with blocked tokens penalized.
        """
        # For single token blocks, directly modify the scores
        for token_ids in self.blocked_token_ids:
            if len(token_ids) == 1:
                scores[:, token_ids[0]] = self.min_probability

        # For multi-token sequences, check if we're at the start of a blocked sequence
        batch_size = input_ids.shape[0]
        for b in range(batch_size):
            for token_seq in self.blocked_token_ids:
                if len(token_seq) > 1:
                    # Get the current sequence length to compare
                    seq_len = min(len(token_seq) - 1, input_ids.shape[1])

                    # Check if the last tokens match the beginning of the blocked sequence
                    if seq_len > 0 and input_ids[b, -seq_len:].tolist() == token_seq[:seq_len]:
                        # If it matches, block the next token in the sequence
                        next_token = token_seq[seq_len]
                        scores[b, next_token] = self.min_probability

        return scores


class GemmaTextGenerator:
    """
    Text generation with Gemma 2B model with privacy-focused generation controls.
    """

    def __init__(
        self,
        model_name: str = "google/gemma-2b-it",
        max_new_tokens: int = 512,
        temperature: float = 0.8,
        top_k: int = 50,
        top_p: float = 0.95,
        repetition_penalty: float = 1.2,
        blocked_words: List[str] = None,
        cache_dir: str = None,
        load_in_8bit: bool = False,
        hf_auth_token: str = "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",
    ):
        """
        Initialize the Gemma text generator with privacy controls.

        Args:
            model_name: HuggingFace model identifier.
            device: Device to use (cuda, cpu, mps). If None, will auto-detect.
            max_new_tokens: Maximum number of new tokens to generate.
            temperature: Sampling temperature. Higher = more random.
            top_k: Keep only top k tokens with highest probability.
            top_p: Keep tokens with cumulative probability >= top_p.
            repetition_penalty: Penalty for repeating tokens. Higher = less repetition.
            blocked_words: List of words or phrases to block from generation.
            cache_dir: Directory for caching models.
            load_in_8bit: Whether to load the model in 8-bit precision.
            hf_auth_token: HuggingFace API token for accessing gated models.
        """
        self.model_name = model_name
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p
        self.repetition_penalty = repetition_penalty
        self.blocked_words = blocked_words or []

        # Set HF_AUTH_TOKEN if provided
        if hf_auth_token:
            os.environ["HF_TOKEN"] = hf_auth_token
            logger.info("Using provided HuggingFace auth token")

        # Determine device1
        if torch.cuda.is_available():
            self.device = "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        logger.info(f"Using device: {self.device}")


        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            use_auth_token=os.environ.get("HF_TOKEN")
        )

        # Set padding side to left for better handling of context sizes
        self.tokenizer.padding_side = "left"

        # Load model
        model_kwargs = {
            "cache_dir": cache_dir,
            "use_auth_token": os.environ.get("HF_TOKEN")
        }

        if load_in_8bit:
            logger.info("Loading model in 8-bit precision")
            model_kwargs["load_in_8bit"] = True

        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else None,
            device_map="auto" if self.device == "cuda" else None,
            **model_kwargs
        )

        if self.device != "cuda":
            self.model.to(self.device)

        logger.info(f"Loaded model: {model_name}")
        accelerator = Accelerator()

        # Create pipeline for easy use
        self.pipeline = TextGenerationPipeline(model=self.model, tokenizer=self.tokenizer)

        # Initialize logits processors list
        self.custom_logits_processors = []

        if self.blocked_words:
            self.custom_logits_processors.append(
                BlockedWordsLogitsProcessor(self.tokenizer, self.blocked_words)
            )

    def _get_generation_config(self) -> Dict[str, Any]:
        """
        Get the generation configuration based on current settings.

        Returns:
            Dict containing generation parameters.
        """
        config = {
            "max_new_tokens": self.max_new_tokens,
            "do_sample": True,
            "temperature": self.temperature,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "repetition_penalty": self.repetition_penalty,
            "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
        }

        return config

    def generate(self, prompt: str, **kwargs) -> Tuple[str, Dict[str, Any]]:
        """
        Generate text based on the prompt with privacy controls applied.

        Args:
            prompt: The input prompt.
            **kwargs: Additional generation parameters to override defaults.

        Returns:
            Tuple containing:
            - The generated text
            - Dict with metadata about the generation process
        """
        # Get base generation config and update with any overrides
        gen_config = self._get_generation_config()
        gen_config.update(kwargs)

        # Build logits processors list
        logits_processors = LogitsProcessorList(self.custom_logits_processors)

        if gen_config.get("repetition_penalty", 1.0) > 1.0:
            logits_processors.append(
                RepetitionPenaltyLogitsProcessor(gen_config["repetition_penalty"])
            )

        if gen_config.get("top_k", 0) > 0:
            logits_processors.append(
                TopKLogitsWarper(gen_config["top_k"])
            )

        if gen_config.get("top_p", 1.0) < 1.0:
            logits_processors.append(
                TopPLogitsWarper(gen_config["top_p"])
            )

        # Start generation timer
        start_time = torch.cuda.Event(enable_timing=True) if self.device == "cuda" else None
        end_time = torch.cuda.Event(enable_timing=True) if self.device == "cuda" else None

        if start_time:
            start_time.record()

        # Encode the prompt
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_length = inputs["input_ids"].shape[1]

        # Generate text with custom processors
        try:
            with torch.no_grad():
                gen_config["logits_processor"] = logits_processors

                outputs = self.model.generate(
                    **inputs,
                    **gen_config
                )

            # Extract only the newly generated tokens
            generated_tokens = outputs[0, input_length:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

            if end_time:
                end_time.record()
                torch.cuda.synchronize()
                gen_time_ms = start_time.elapsed_time(end_time)
            else:
                gen_time_ms = None

            # Prepare metadata
            metadata = {
                "model": self.model_name,
                "input_tokens": input_length,
                "generated_tokens": len(generated_tokens),
                "generation_time_ms": gen_time_ms,
                "generation_config": {k: v for k, v in gen_config.items() if k != "logits_processor"}
            }

            return generated_text, metadata

        except Exception as e:
            logger.error(f"Generation error: {str(e)}")
            return f"Error generating text: {str(e)}", {"error": str(e)}

    def update_blocked_words(self, blocked_words: List[str]):
        """
        Update the list of blocked words.

        Args:
            blocked_words: New list of words to block.
        """
        self.blocked_words = blocked_words

        # Rebuild logits processors
        self.custom_logits_processors = []

        if self.blocked_words:
            self.custom_logits_processors.append(
                BlockedWordsLogitsProcessor(self.tokenizer, self.blocked_words)
            )

    def update_generation_params(self, **kwargs):
        """
        Update generation parameters.

        Args:
            **kwargs: Parameters to update (temperature, top_k, top_p, etc).
        """
        valid_params = [
            "max_new_tokens", "temperature", "top_k", "top_p", "repetition_penalty"
        ]

        for param, value in kwargs.items():
            if param in valid_params:
                setattr(self, param, value)
                logger.info(f"Updated {param} to {value}")
            else:
                logger.warning(f"Ignored invalid parameter: {param}")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PostGenerationFilter:
    """
    Post-Generation Filter that uses similarity-based detection, with adaptive thresholds
    and optional re-application of PII detection from the Pre-Generation filter.
    """

    def __init__(
        self,
        sensitive_items: List[str] = None,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        base_threshold: float = 0.92,
        adaptive_thresholds: Dict[str, float] = None,
        content_type_classifier: Optional[Callable[[str], str]] = None,
        reapply_pii_detection: bool = True,
        pii_filter: Optional[PreGenerationFilter] = None,
        replacement_text: str = "[REDACTED_CONTENT_HIGH_SIMILARITY]",
        use_token_level_redaction: bool = True,
        batch_size: int = 32,
        device: str = None
    ):
        """
        Initialize the Post-Generation Filter.

        Args:
            sensitive_items: List of sensitive text items to check against.
            embedding_model: SentenceTransformer model to use for embeddings.
            base_threshold: Base similarity threshold (0.0-1.0).
            adaptive_thresholds: Dict mapping content types to thresholds.
            content_type_classifier: Function that classifies text into content types.
            reapply_pii_detection: Whether to reapply PII detection on outputs.
            pii_filter: Pre-configured PreGenerationFilter instance to use.
            replacement_text: Text to use for replacing highly similar content.
            use_token_level_redaction: Use token-level redaction for PII.
            batch_size: Batch size for computing embeddings.
            device: Device to use for embeddings (cuda, cpu, mps).
        """
        # Initialize sensitive items list
        self.sensitive_items = sensitive_items or []
        self.sensitive_embeddings = None

        # Similarity settings
        self.base_threshold = base_threshold
        self.adaptive_thresholds = adaptive_thresholds or {
            "default": base_threshold,
            "technical": 0.95,  # Higher threshold for technical content
            "general": 0.9,     # Lower threshold for general content
            "personal": 0.85    # Even lower threshold for personal info
        }

        # Content classification function
        self.content_type_classifier = content_type_classifier or self._simple_classifier

        # Text replacement settings
        self.replacement_text = replacement_text
        self.use_token_level_redaction = use_token_level_redaction

        # PII detection
        self.reapply_pii_detection = reapply_pii_detection
        self.pii_filter = pii_filter or PreGenerationFilter(
            use_token_level=use_token_level_redaction,
            type_specific_placeholders=True
        )

        # Load embedding model
        try:
            logger.info(f"Loading embedding model: {embedding_model}")
            self.embedding_model = SentenceTransformer(embedding_model, device=device)
            self.batch_size = batch_size
            self._precompute_embeddings()
        except Exception as e:
            logger.error(f"Failed to load embedding model: {str(e)}")
            raise

        # Tracking stats
        self.stats = {
            "total_processed": 0,
            "similarity_redactions": 0,
            "pii_redactions": 0,
            "content_types": {}
        }

    def reset_stats(self):
        """Reset the filtering statistics."""
        self.stats = {
            "total_processed": 0,
            "similarity_redactions": 0,
            "pii_redactions": 0,
            "content_types": {}
        }

    def _precompute_embeddings(self):
        """Precompute embeddings for sensitive items."""
        if not self.sensitive_items:
            self.sensitive_embeddings = None
            return

        logger.info(f"Precomputing embeddings for {len(self.sensitive_items)} sensitive items")
        self.sensitive_embeddings = self.embedding_model.encode(
            self.sensitive_items,
            batch_size=self.batch_size,
            show_progress_bar=len(self.sensitive_items) > 10
        )

    def _simple_classifier(self, text: str) -> str:
        """
        Simple rule-based content type classifier.

        Args:
            text: Text to classify.

        Returns:
            Content type classification (technical, personal, general).
        """
        # Check for patterns indicating personal information
        personal_indicators = [
            r'\b(?:I|my|me|mine|myself)\b',
            r'\b(?:you|your|yours|yourself)\b',
            r'\b(?:he|him|his|himself|she|her|hers|herself)\b',
            r'\b(?:we|us|our|ours|ourselves)\b',
            r'\b(?:they|them|their|theirs|themselves)\b',
            r'\b(?:address|phone|email|contact|birth|birthday|age|gender|identity|family)\b'
        ]

        personal_count = sum(1 for pattern in personal_indicators if re.search(pattern, text, re.IGNORECASE))

        # Check for patterns indicating technical content
        technical_indicators = [
            r'\b(?:code|function|algorithm|data|technical|system|software|hardware|server|database|API|protocol)\b',
            r'\b(?:network|security|cybersecurity|encryption|programming|developer|architecture|framework)\b',
            r'\b(?:implementation|design|pattern|interface|class|object|procedure|method|structure)\b'
        ]

        technical_count = sum(1 for pattern in technical_indicators if re.search(pattern, text, re.IGNORECASE))

        # Determine content type based on matches
        if personal_count > technical_count and personal_count > 2:
            return "personal"
        elif technical_count > personal_count and technical_count > 2:
            return "technical"
        else:
            return "general"

    def get_threshold_for_content(self, text: str) -> float:
        """
        Get the appropriate similarity threshold for the given content.

        Args:
            text: Text to get threshold for.

        Returns:
            Similarity threshold value.
        """
        content_type = self.content_type_classifier(text)

        # Update stats
        self.stats["content_types"][content_type] = self.stats["content_types"].get(content_type, 0) + 1

        # Get threshold for this content type
        threshold = self.adaptive_thresholds.get(content_type, self.base_threshold)

        return threshold

    def add_sensitive_items(self, items: List[str]):
        """
        Add new items to the sensitive items list.

        Args:
            items: List of sensitive text items to add.
        """
        if not items:
            return

        # Add items
        self.sensitive_items.extend(items)

        # Recompute embeddings
        self._precompute_embeddings()

    def check_similarity(self, text: str, threshold: Optional[float] = None) -> Tuple[float, int]:
        """
        Check the similarity of the text against sensitive items.

        Args:
            text: Text to check.
            threshold: Optional override threshold. If None, uses content-dependent threshold.

        Returns:
            Tuple containing:
            - Maximum similarity score (float)
            - Index of the most similar item (-1 if no items)
        """
        if not self.sensitive_items or not self.sensitive_embeddings is not None:
            return 0.0, -1

        # Get text embedding
        text_embedding = self.embedding_model.encode(text, batch_size=1)

        # Reshape for similarity calculation
        text_embedding = text_embedding.reshape(1, -1)

        # Calculate similarity to all sensitive items
        similarities = cosine_similarity(text_embedding, self.sensitive_embeddings)[0]

        # Get max similarity and index
        max_similarity = similarities.max()
        max_index = similarities.argmax()

        return float(max_similarity), int(max_index)

    def process(self, text: str) -> Tuple[str, Dict]:
        """
        Process the generated text to detect and redact sensitive content.

        Args:
            text: The text to process.

        Returns:
            Tuple containing:
            - The processed text
            - Statistics about the processing
        """
        if not text:
            return "", {"error": "Empty text provided"}

        self.stats["total_processed"] += 1

        # Get content-dependent threshold
        threshold = self.get_threshold_for_content(text)

        # Check similarity against sensitive items
        max_similarity, max_index = self.check_similarity(text, threshold)

        result_stats = {
            "content_type": self.content_type_classifier(text),
            "threshold": threshold,
            "max_similarity": max_similarity,
            "similarity_redacted": False,
            "pii_redacted": False
        }

        # If similarity exceeds threshold, redact the entire text
        if max_similarity >= threshold and max_index >= 0:
            result = self.replacement_text
            result_stats["similarity_redacted"] = True
            self.stats["similarity_redactions"] += 1
            return result, result_stats

        # If similarity check passes, optionally apply PII detection
        if self.reapply_pii_detection:
            redacted_text, pii_stats = self.pii_filter.process(text)

            # If PII was found, update result and stats
            if pii_stats["replacements"]["total"] > 0:
                result = redacted_text
                result_stats["pii_redacted"] = True
                result_stats["pii_stats"] = pii_stats
                self.stats["pii_redactions"] += 1
                return result, result_stats

        # If all checks pass, return the original text
        return text, result_stats

    def get_stats(self) -> Dict:
        """
        Get current filtering statistics.

        Returns:
            Dictionary with filtering statistics.
        """
        if self.stats["total_processed"] > 0:
            similarity_rate = self.stats["similarity_redactions"] / self.stats["total_processed"]
            pii_rate = self.stats["pii_redactions"] / self.stats["total_processed"]
            safe_rate = 1.0 - (similarity_rate + pii_rate)
        else:
            similarity_rate = 0.0
            pii_rate = 0.0
            safe_rate = 0.0

        return {
            **self.stats,
            "similarity_redaction_rate": similarity_rate,
            "pii_redaction_rate": pii_rate,
            "safe_rate": safe_rate
        }

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PrivacyFilterPipeline:
    """
    End-to-end pipeline for synthetic data generation with privacy filtering.
    """

    def __init__(
        self,
        pre_filter_config: Optional[Dict[str, Any]] = None,
        generator_config: Optional[Dict[str, Any]] = None,
        post_filter_config: Optional[Dict[str, Any]] = None,
        model_name: str = "google/gemma-2b-it",
        sensitive_items: List[str] = None,
        device: str = None,
        hf_auth_token: str = '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'
    ):
        """
        Initialize the privacy filter pipeline.

        Args:
            pre_filter_config: Configuration for the pre-generation filter.
            generator_config: Configuration for the text generator.
            post_filter_config: Configuration for the post-generation filter.
            model_name: Name of the model to use for text generation.
            sensitive_items: List of sensitive items to check against.
            device: Device to use for models (cuda, cpu, mps).
            hf_auth_token: HuggingFace API token.
        """
        # Set default configurations
        pre_filter_config = pre_filter_config or {
            "use_token_level": True,
            "type_specific_placeholders": True
        }

        generator_config = generator_config or {
            "temperature": 0.8,
            "top_k": 50,
            "top_p": 0.95,
            "repetition_penalty": 1.2,
            "max_new_tokens": 256,
            "blocked_words": ["password", "social security", "credit card", "address", "phone number", "email"]
        }

        post_filter_config = post_filter_config or {
            "base_threshold": 0.92,
            "adaptive_thresholds": {
                "technical": 0.95,
                "general": 0.9,
                "personal": 0.85
            },
            "reapply_pii_detection": True,
            "use_token_level_redaction": True
        }

        # Initialize components
        logger.info("Initializing Pre-Generation Filter...")
        self.pre_filter = PreGenerationFilter(**pre_filter_config)

        logger.info(f"Initializing Text Generator with model: {model_name}...")
        # Set HuggingFace token if provided
        if hf_auth_token:
            os.environ["HF_TOKEN"] = hf_auth_token

        # Extract generator-specific configs
        generator_kwargs = dict(generator_config)
        self.generator = GemmaTextGenerator(
            model_name=model_name,
            device=device,
            hf_auth_token=hf_auth_token,
            **generator_kwargs
        )

        logger.info("Initializing Post-Generation Filter...")
        # Add sensitive items to post filter config
        if sensitive_items:
            post_filter_config["sensitive_items"] = sensitive_items
        self.post_filter = PostGenerationFilter(**post_filter_config)

        # Pipeline configuration
        self.config = {
            "pre_filter": pre_filter_config,
            "generator": generator_config,
            "post_filter": post_filter_config,
            "model_name": model_name
        }

        # Pipeline stats
        self.stats = {
            "total_processed": 0,
            "pre_filter_applied": 0,
            "post_filter_applied": 0,
            "total_replacements": 0
        }

        logger.info("Pipeline initialized successfully")

    def reset_stats(self):
        """Reset the pipeline statistics."""
        self.stats = {
            "total_processed": 0,
            "pre_filter_applied": 0,
            "post_filter_applied": 0,
            "total_replacements": 0
        }

    def process(
        self,
        text: str,
        prompt_template: str = None,
        max_new_tokens: int = None,
        return_all_stages: bool = False
    ) -> Union[str, Dict[str, Any]]:
        """
        Process a text through the entire pipeline.

        Args:
            text: The input text to process.
            prompt_template: Template for the generation prompt. Use {text} as placeholder.
            max_new_tokens: Override for max tokens to generate.
            return_all_stages: Whether to return outputs from all pipeline stages.

        Returns:
            If return_all_stages is False: The final processed text.
            If return_all_stages is True: Dict with outputs from all stages.
        """
        self.stats["total_processed"] += 1

        # 1. Apply pre-generation filter
        logger.info("Applying pre-generation filter...")
        pre_filtered_text, pre_stats = self.pre_filter.process(text)

        if pre_stats["replacements"]["total"] > 0:
            self.stats["pre_filter_applied"] += 1
            self.stats["total_replacements"] += pre_stats["replacements"]["total"]

        # 2. Prepare prompt for generation
        if prompt_template is None:
            prompt_template = "Based on the following text, generate a variation that preserves the meaning but uses different wording:\n\n{text}"

        prompt = prompt_template.format(text=pre_filtered_text)

        # 3. Generate text
        logger.info("Generating text...")
        gen_kwargs = {}
        if max_new_tokens is not None:
            gen_kwargs["max_new_tokens"] = max_new_tokens

        generated_text, gen_stats = self.generator.generate(prompt, **gen_kwargs)

        # 4. Apply post-generation filter
        logger.info("Applying post-generation filter...")
        post_filtered_text, post_stats = self.post_filter.process(generated_text)

        if post_stats["similarity_redacted"] or post_stats["pii_redacted"]:
            self.stats["post_filter_applied"] += 1

        # 5. Return result
        if return_all_stages:
            return {
                "original_text": text,
                "pre_filtered_text": pre_filtered_text,
                "prompt": prompt,
                "generated_text": generated_text,
                "final_text": post_filtered_text,
                "stats": {
                    "pre_filter": pre_stats,
                    "generation": gen_stats,
                    "post_filter": post_stats,
                    "pipeline": dict(self.stats)
                }
            }
        else:
            return post_filtered_text

    def process_batch(
        self,
        texts: List[str],
        prompt_template: str = None,
        max_new_tokens: int = None,
        return_all_stages: bool = False,
        progress_callback: callable = None
    ) -> Union[List[str], List[Dict[str, Any]]]:
        """
        Process a batch of texts through the pipeline.

        Args:
            texts: List of input texts to process.
            prompt_template: Template for the generation prompt. Use {text} as placeholder.
            max_new_tokens: Override for max tokens to generate.
            return_all_stages: Whether to return outputs from all pipeline stages.
            progress_callback: Optional callback function to report progress.

        Returns:
            List of processed outputs (strings or dicts depending on return_all_stages).
        """
        results = []
        total = len(texts)

        for i, text in enumerate(texts):
            # Report progress if callback provided
            if progress_callback and callable(progress_callback):
                progress_callback(i, total)

            # Process text
            result = self.process(
                text=text,
                prompt_template=prompt_template,
                max_new_tokens=max_new_tokens,
                return_all_stages=return_all_stages
            )

            results.append(result)

            # Log progress periodically
            if (i + 1) % 10 == 0 or (i + 1) == total:
                logger.info(f"Processed {i + 1}/{total} texts")

        return results

    def process_dataframe(
        self,
        df: pd.DataFrame,
        text_column: str,
        output_column: str = "processed_text",
        all_stages_column: str = None,
        prompt_template: str = None,
        max_new_tokens: int = None,
        inplace: bool = False,
        progress_callback: callable = None
    ) -> pd.DataFrame:
        """
        Process texts in a DataFrame through the pipeline.

        Args:
            df: Input DataFrame.
            text_column: Column containing input texts.
            output_column: Column to store final outputs.
            all_stages_column: Column to store all stage outputs (if desired).
            prompt_template: Template for the generation prompt. Use {text} as placeholder.
            max_new_tokens: Override for max tokens to generate.
            inplace: Whether to modify the input DataFrame in-place.
            progress_callback: Optional callback function to report progress.

        Returns:
            DataFrame with processed results.
        """
        if text_column not in df.columns:
            raise ValueError(f"Text column '{text_column}' not found in DataFrame")

        # Get texts to process
        texts = df[text_column].tolist()

        # Determine if we need to return all stages
        return_all_stages = all_stages_column is not None

        # Process texts
        results = self.process_batch(
            texts=texts,
            prompt_template=prompt_template,
            max_new_tokens=max_new_tokens,
            return_all_stages=return_all_stages,
            progress_callback=progress_callback
        )

        # Create output DataFrame
        if not inplace:
            output_df = df.copy()
        else:
            output_df = df

        # Add results to DataFrame
        if return_all_stages:
            # Extract final texts for the output column
            output_df[output_column] = [r["final_text"] for r in results]
            # Store full results in the all stages column
            output_df[all_stages_column] = results
        else:
            output_df[output_column] = results

        return output_df

    def get_config(self) -> Dict[str, Any]:
        """Get the pipeline configuration."""
        return self.config

    def get_stats(self) -> Dict[str, Any]:
        """Get the pipeline statistics."""
        return self.stats

    def update_pre_filter_config(self, **kwargs):
        """Update the pre-generation filter configuration."""
        for key, value in kwargs.items():
            if hasattr(self.pre_filter, key):
                setattr(self.pre_filter, key, value)
                self.config["pre_filter"][key] = value
                logger.info(f"Updated pre-filter parameter: {key} = {value}")

    def update_generator_config(self, **kwargs):
        """Update the text generator configuration."""
        for key, value in kwargs.items():
            if key in ["temperature", "top_k", "top_p", "repetition_penalty", "max_new_tokens"]:
                # These are parameters that can be updated directly
                self.generator.update_generation_params(**{key: value})
                self.config["generator"][key] = value
                logger.info(f"Updated generator parameter: {key} = {value}")
            elif key == "blocked_words":
                # Update blocked words list
                self.generator.update_blocked_words(value)
                self.config["generator"][key] = value
                logger.info(f"Updated blocked words list with {len(value)} words")

    def update_post_filter_config(self, **kwargs):
        """Update the post-generation filter configuration."""
        for key, value in kwargs.items():
            if hasattr(self.post_filter, key):
                setattr(self.post_filter, key, value)
                self.config["post_filter"][key] = value
                logger.info(f"Updated post-filter parameter: {key} = {value}")

        # Special handling for sensitive items
        if "sensitive_items" in kwargs:
            self.post_filter.add_sensitive_items(kwargs["sensitive_items"])
            logger.info(f"Added {len(kwargs['sensitive_items'])} sensitive items to post-filter")

# Load configuration
with open('simple_config.json', 'r') as f:
    config = json.load(f)

# Initialize the pipeline with configuration
try:
    # First test only the pre-filter to ensure it works
    print("Testing pre-filter only...")
    pre_filter = PreGenerationFilter(
        use_token_level=True,
        type_specific_placeholders=True
    )

    text = "My name is Jane Doe and my email is jane.doe@example.com"
    redacted_text, stats = pre_filter.process(text)
    print("\nOriginal text:")
    print(text)
    print("\nRedacted text:")
    print(redacted_text)
    print("\nStats:")
    print(stats)

    # Set up a simplified pipeline with just preprocessing (no LLM generation)
    print("\n\nTesting simplified pipeline (preprocessing only)...")
    pipeline_args = {}

    if "pre_filter_config" in config:
        pipeline_args["pre_filter_config"] = config["pre_filter_config"]
    if "post_filter_config" in config:
        pipeline_args["post_filter_config"] = config["post_filter_config"]

    # Add sensitive items for post-filter
    sensitive_items = [
        "My SSN is 123-45-6789",
        "My credit card number is 4111-1111-1111-1111"
    ]
    pipeline_args["sensitive_items"] = sensitive_items


    # Create the pipeline
    pipeline = PrivacyFilterPipeline(**pipeline_args)

    # Process a text
    test_text = """
    Hello, my name is John Smith and I work at Acme Corporation.
    You can reach me at john.smith@example.com or call me at (555) 123-4567.
    My address is 123 Main St, New York, NY 10001.
    """

    result = pipeline.process(
        text=test_text,
        return_all_stages=True
    )

    # Print results
    print("\nOriginal Text:")
    print(result["original_text"])
    print("\nPre-Filtered Text:")
    print(result["pre_filtered_text"])
    print("\nFinal Text:")
    print(result["final_text"])
    print("\nPipeline Stats:")
    print(json.dumps(result["stats"], indent=2))
except:
    print("not working")

"""
Dataset preparation utilities for creating datasets with injected sensitive information.
"""

import csv
import json
import logging
import os
import random
import re
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional, Union, Any, Callable

import pandas as pd
import numpy as np



# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SensitiveDataInjector:
    """
    Utility for injecting synthetic sensitive data into datasets for testing privacy filters.
    """

    def __init__(
        self,
        seed: int = 42,
        name_list_path: Optional[str] = None,
        canary_phrases: List[str] = None,
        pii_injection_rate: float = 0.3,
        canary_injection_rate: float = 0.1,
        max_injections_per_sample: int = 2
    ):
        """
        Initialize the sensitive data injector.

        Args:
            seed: Random seed for reproducibility.
            name_list_path: Path to a file containing names for injection (one per line).
            canary_phrases: List of custom "secret" canary phrases to inject.
            pii_injection_rate: Rate at which to inject PII (0.0-1.0).
            canary_injection_rate: Rate at which to inject canary phrases (0.0-1.0).
            max_injections_per_sample: Maximum number of injections per text sample.
        """
        self.rng = random.Random(seed)
        np.random.seed(seed)

        self.pii_injection_rate = pii_injection_rate
        self.canary_injection_rate = canary_injection_rate
        self.max_injections_per_sample = max_injections_per_sample

        # Load names if path provided
        self.names = []
        if name_list_path:
            try:
                with open(name_list_path, 'r', encoding='utf-8') as f:
                    self.names = [line.strip() for line in f if line.strip()]
                logger.info(f"Loaded {len(self.names)} names from {name_list_path}")
            except Exception as e:
                logger.error(f"Failed to load names from {name_list_path}: {str(e)}")
                self._generate_fallback_names()
        else:
            self._generate_fallback_names()

        # Setup canary phrases
        self.canary_phrases = canary_phrases or [
            "The purple elephant dances at midnight under the silver moon",
            "Golden keys unlock the ancient wooden chest of forgotten memories",
            "Whispers of crystal snowflakes tell secrets to winter shadows",
            "Emerald dragons soar through clouds of rainbow silk",
            "The captain's compass always points to dreams of lost islands"
        ]

        # Setup PII generators
        self.pii_generators = {
            'name': self._generate_name,
            'email': self._generate_email,
            'phone': self._generate_phone,
            'ssn': self._generate_ssn,
            'credit_card': self._generate_credit_card,
            'address': self._generate_address,
        }

    def _generate_fallback_names(self):
        """Generate a list of fake names when no name file is provided."""
        first_names = ["James", "Mary", "John", "Patricia", "Robert", "Jennifer", "Michael",
                     "Linda", "William", "Elizabeth", "David", "Susan", "Joseph", "Jessica",
                     "Charles", "Sarah", "Thomas", "Karen", "Daniel", "Lisa"]

        last_names = ["Smith", "Johnson", "Williams", "Brown", "Jones", "Miller", "Davis",
                     "Garcia", "Rodriguez", "Wilson", "Martinez", "Anderson", "Taylor",
                     "Thomas", "Hernandez", "Moore", "Martin", "Jackson", "Thompson", "White"]

        self.names = [f"{first} {last}" for first in first_names for last in last_names]
        self.rng.shuffle(self.names)
        logger.info(f"Generated {len(self.names)} fallback names")

    def _generate_name(self) -> str:
        """Generate a random name."""
        return self.rng.choice(self.names)

    def _generate_email(self) -> str:
        """Generate a random email address."""
        name = self.rng.choice(self.names).lower().replace(" ", ".")
        domains = ["gmail.com", "yahoo.com", "outlook.com", "example.com", "company.org"]
        domain = self.rng.choice(domains)
        return f"{name}@{domain}"

    def _generate_phone(self) -> str:
        """Generate a random US phone number."""
        area_code = self.rng.randint(200, 999)
        prefix = self.rng.randint(200, 999)
        line = self.rng.randint(1000, 9999)
        formats = [
            f"({area_code}) {prefix}-{line}",
            f"{area_code}-{prefix}-{line}",
            f"+1 {area_code} {prefix} {line}"
        ]
        return self.rng.choice(formats)

    def _generate_ssn(self) -> str:
        """Generate a random Social Security Number."""
        area = self.rng.randint(100, 999)
        group = self.rng.randint(10, 99)
        serial = self.rng.randint(1000, 9999)
        return f"{area}-{group}-{serial}"

    def _generate_credit_card(self) -> str:
        """Generate a random credit card number."""
        prefixes = ["4", "51", "36", "34", "6011"]
        prefix = self.rng.choice(prefixes)

        # Generate the rest of the digits
        remaining_length = 16 - len(prefix)
        digits = [str(self.rng.randint(0, 9)) for _ in range(remaining_length)]
        number = prefix + "".join(digits)

        # Format with dashes
        formatted = "-".join([number[i:i+4] for i in range(0, len(number), 4)])
        return formatted

    def _generate_address(self) -> str:
        """Generate a random US address."""
        number = self.rng.randint(1, 9999)
        street_types = ["St", "Ave", "Blvd", "Dr", "Ln", "Rd", "Way", "Place", "Court"]
        street_names = ["Main", "Oak", "Maple", "Washington", "Park", "Elm", "Lake", "Hill",
                      "Pine", "Cedar", "Sunset", "River", "Ocean", "Meadow", "Forest"]

        street = f"{number} {self.rng.choice(street_names)} {self.rng.choice(street_types)}"

        cities = ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia",
                "San Antonio", "San Diego", "Dallas", "San Jose", "Austin", "Jacksonville",
                "Fort Worth", "Columbus", "Charlotte", "Seattle", "Denver", "Boston"]

        states = {
            "AL": "Alabama", "AK": "Alaska", "AZ": "Arizona", "AR": "Arkansas",
            "CA": "California", "CO": "Colorado", "CT": "Connecticut", "DE": "Delaware",
            "FL": "Florida", "GA": "Georgia", "HI": "Hawaii", "ID": "Idaho",
            "IL": "Illinois", "IN": "Indiana", "IA": "Iowa", "KS": "Kansas",
            "KY": "Kentucky", "LA": "Louisiana", "ME": "Maine", "MD": "Maryland",
            "MA": "Massachusetts", "MI": "Michigan", "MN": "Minnesota", "MS": "Mississippi",
            "MO": "Missouri", "MT": "Montana", "NE": "Nebraska", "NV": "Nevada",
            "NH": "New Hampshire", "NJ": "New Jersey", "NM": "New Mexico", "NY": "New York",
            "NC": "North Carolina", "ND": "North Dakota", "OH": "Ohio", "OK": "Oklahoma",
            "OR": "Oregon", "PA": "Pennsylvania", "RI": "Rhode Island", "SC": "South Carolina",
            "SD": "South Dakota", "TN": "Tennessee", "TX": "Texas", "UT": "Utah",
            "VT": "Vermont", "VA": "Virginia", "WA": "Washington", "WV": "West Virginia",
            "WI": "Wisconsin", "WY": "Wyoming"
        }

        state_abbr = self.rng.choice(list(states.keys()))

        city = self.rng.choice(cities)
        zip_code = self.rng.randint(10000, 99999)

        # Decide on format (full state name or abbreviation)
        if self.rng.random() > 0.5:
            return f"{street}, {city}, {state_abbr} {zip_code}"
        else:
            return f"{street}, {city}, {states[state_abbr]} {zip_code}"

    def _select_injection_points(self, text: str, num_injections: int) -> List[int]:
        """
        Select positions in the text to inject sensitive data.

        Args:
            text: The text to inject into.
            num_injections: Number of injection points to select.

        Returns:
            List of character positions for injections.
        """
        # Split text into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)
        if not sentences:
            return []

        # Remove empty sentences
        sentences = [s for s in sentences if s.strip()]
        if not sentences:
            return []

        # Select random sentences for injection
        selected_indices = self.rng.sample(range(len(sentences)), min(num_injections, len(sentences)))

        # Find the character positions of the selected sentences
        positions = []
        current_pos = 0
        for i, sentence in enumerate(sentences):
            if i in selected_indices:
                # Insert at the end of the sentence
                end_pos = current_pos + len(sentence)
                positions.append(end_pos)
            current_pos += len(sentence) + 1  # +1 for the space after the sentence

        return positions

    def inject_pii(self, text: str) -> Tuple[str, Dict]:
        """
        Inject PII into the given text.

        Args:
            text: Text to inject PII into.

        Returns:
            Tuple containing:
            - Text with injected PII
            - Dictionary with metadata about injections
        """
        if not text or len(text.strip()) < 10:
            return text, {"error": "Text too short for injection"}

        # Determine if we should inject PII in this sample
        if self.rng.random() > self.pii_injection_rate:
            return text, {"injected": False}

        # Determine how many PIIs to inject
        num_injections = self.rng.randint(1, self.max_injections_per_sample)

        # Select injection points
        injection_points = self._select_injection_points(text, num_injections)
        if not injection_points:
            return text, {"injected": False, "reason": "No suitable injection points"}

        # Select PII types to inject
        pii_types = self.rng.choices(list(self.pii_generators.keys()), k=len(injection_points))

        # Inject PII
        injected_text = text
        offset = 0
        injections = []

        for point, pii_type in zip(injection_points, pii_types):
            # Generate PII
            pii_value = self.pii_generators[pii_type]()

            # Create injection string
            injection_templates = [
                f" My {pii_type} is {pii_value}.",
                f" By the way, my {pii_type} is {pii_value}.",
                f" For reference, my {pii_type} is {pii_value}.",
                f" Just so you know, my {pii_type} is {pii_value}."
            ]
            injection_str = self.rng.choice(injection_templates)

            # Insert at the adjusted point
            adjusted_point = point + offset
            injected_text = injected_text[:adjusted_point] + injection_str + injected_text[adjusted_point:]

            # Update offset
            offset += len(injection_str)

            # Record injection metadata
            injections.append({
                "type": pii_type,
                "value": pii_value,
                "position": adjusted_point,
                "injection_text": injection_str
            })

        metadata = {
            "injected": True,
            "num_injections": len(injections),
            "injections": injections
        }

        return injected_text, metadata

    def inject_canary(self, text: str) -> Tuple[str, Dict]:
        """
        Inject a canary phrase into the given text.

        Args:
            text: Text to inject canary into.

        Returns:
            Tuple containing:
            - Text with injected canary
            - Dictionary with metadata about the injection
        """
        if not text or len(text.strip()) < 10:
            return text, {"error": "Text too short for injection"}

        # Determine if we should inject a canary in this sample
        if self.rng.random() > self.canary_injection_rate:
            return text, {"injected": False}

        # Select a canary phrase
        canary = self.rng.choice(self.canary_phrases)

        # Select an injection point
        injection_points = self._select_injection_points(text, 1)
        if not injection_points:
            return text, {"injected": False, "reason": "No suitable injection points"}

        # Create injection string
        injection_templates = [
            f" {canary}.",
            f" Remember that \"{canary}\".",
            f" As they say, \"{canary}\".",
            f" I once heard that \"{canary}\"."
        ]
        injection_str = self.rng.choice(injection_templates)

        # Insert at the point
        point = injection_points[0]
        injected_text = text[:point] + injection_str + text[point:]

        metadata = {
            "injected": True,
            "canary": canary,
            "position": point,
            "injection_text": injection_str
        }

        return injected_text, metadata

    def process_dataset(
        self,
        input_data: Union[str, pd.DataFrame, List[str]],
        text_column: str = None,
        output_path: Optional[str] = None,
        output_format: str = "json",
        id_column: str = None
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        Process a dataset by injecting PII and canary phrases.

        Args:
            input_data: File path, DataFrame, or list of strings.
            text_column: Column name containing text (if DataFrame).
            output_path: Path to save the processed dataset.
            output_format: Format to save the output (json, csv, pickle).
            id_column: Column to use as ID (if DataFrame).

        Returns:
            Tuple containing:
            - Processed DataFrame
            - Dictionary with processing statistics
        """
        # Load the dataset
        df = None
        if isinstance(input_data, str):
            # Input is a file path
            file_path = input_data
            extension = Path(file_path).suffix.lower()

            try:
                if extension == ".csv":
                    df = pd.read_csv(file_path)
                elif extension == ".json":
                    df = pd.read_json(file_path)
                elif extension in [".pkl", ".pickle"]:
                    df = pd.read_pickle(file_path)
                elif extension in [".txt", ".text"]:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = [line.strip() for line in f if line.strip()]
                    df = pd.DataFrame({"text": lines})
                    text_column = "text"
                else:
                    raise ValueError(f"Unsupported file extension: {extension}")
            except Exception as e:
                logger.error(f"Failed to load dataset from {file_path}: {str(e)}")
                raise
        elif isinstance(input_data, pd.DataFrame):
            # Input is already a DataFrame
            df = input_data.copy()
        elif isinstance(input_data, list):
            # Input is a list of strings
            df = pd.DataFrame({"text": input_data})
            text_column = "text"
        else:
            raise ValueError("input_data must be a file path, DataFrame, or list of strings")

        # Validate text column
        if text_column is None:
            # Try to guess the text column
            potential_text_cols = ["text", "content", "message", "body", "description"]
            for col in potential_text_cols:
                if col in df.columns:
                    text_column = col
                    logger.info(f"Using guessed text column: {text_column}")
                    break

            if text_column is None:
                raise ValueError("text_column must be specified if it cannot be guessed")

        if text_column not in df.columns:
            raise ValueError(f"Text column '{text_column}' not found in the dataset")

        # Add ID column if needed
        if id_column is None or id_column not in df.columns:
            df["id"] = [f"sample_{i}" for i in range(len(df))]
            id_column = "id"

        # Create clean copies of the dataset
        df_clean = df.copy()

        # Initialize tracking columns
        df["has_pii"] = False
        df["has_canary"] = False
        df["pii_metadata"] = [{}] * len(df)
        df["canary_metadata"] = [{}] * len(df)

        # Process each row
        stats = {
            "total_samples": len(df),
            "pii_injections": 0,
            "canary_injections": 0,
            "pii_types": {},
            "canaries_used": {}
        }

        for idx, row in df.iterrows():
            text = row[text_column]

            # Skip empty or very short texts
            if not isinstance(text, str) or len(text.strip()) < 10:
                continue

            # Inject PII
            text_with_pii, pii_metadata = self.inject_pii(text)
            if pii_metadata.get("injected", False):
                df.at[idx, text_column] = text_with_pii
                df.at[idx, "has_pii"] = True
                df.at[idx, "pii_metadata"] = pii_metadata
                stats["pii_injections"] += 1

                # Track PII types
                for injection in pii_metadata.get("injections", []):
                    pii_type = injection["type"]
                    stats["pii_types"][pii_type] = stats["pii_types"].get(pii_type, 0) + 1

            # Inject canary phrase (possibly on top of the PII)
            current_text = df.at[idx, text_column]
            text_with_canary, canary_metadata = self.inject_canary(current_text)
            if canary_metadata.get("injected", False):
                df.at[idx, text_column] = text_with_canary
                df.at[idx, "has_canary"] = True
                df.at[idx, "canary_metadata"] = canary_metadata
                stats["canary_injections"] += 1

                # Track canaries used
                canary = canary_metadata.get("canary", "")
                stats["canaries_used"][canary] = stats["canaries_used"].get(canary, 0) + 1

        # Calculate percentages
        stats["pii_injection_rate"] = stats["pii_injections"] / stats["total_samples"]
        stats["canary_injection_rate"] = stats["canary_injections"] / stats["total_samples"]

        # Save the processed dataset if requested
        if output_path:
            output_dir = os.path.dirname(output_path)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)

            if output_format == "json":
                df.to_json(output_path, orient="records", indent=2)
            elif output_format == "csv":
                df.to_csv(output_path, index=False)
            elif output_format == "pickle":
                df.to_pickle(output_path)
            else:
                raise ValueError(f"Unsupported output format: {output_format}")

            # Save clean dataset if applicable
            clean_path = Path(output_path)
            clean_stem = clean_path.stem + "_clean"
            clean_path = clean_path.with_stem(clean_stem)

            if output_format == "json":
                df_clean.to_json(clean_path, orient="records", indent=2)
            elif output_format == "csv":
                df_clean.to_csv(clean_path, index=False)
            elif output_format == "pickle":
                df_clean.to_pickle(clean_path)

            logger.info(f"Saved processed dataset to {output_path}")
            logger.info(f"Saved clean dataset to {clean_path}")

        return df, stats


def load_enron_sample(num_samples: int = 1000, seed: int = 42) -> pd.DataFrame:
    """
    Load a sample of the Enron email dataset.

    Args:
        num_samples: Number of samples to load.
        seed: Random seed for reproducibility.

    Returns:
        DataFrame with the loaded samples.
    """
    # This is a placeholder. In a real implementation, you would download or load the actual dataset.
    # For demo purposes, we'll generate some fake emails that resemble the Enron format.

    rng = random.Random(seed)

    subjects = [
        "Meeting Agenda", "Project Update", "Quarterly Financials", "Team Lunch",
        "Conference Call", "Budget Review", "New Policy", "HR Announcement",
        "System Maintenance", "Weekly Report", "Training Session", "Client Meeting",
        "Contract Negotiation", "Marketing Plan", "Sales Targets", "Product Launch"
    ]

    senders = [
        "john.doe@enron.com", "jane.smith@enron.com", "robert.johnson@enron.com",
        "emily.williams@enron.com", "michael.brown@enron.com", "sarah.miller@enron.com",
        "david.wilson@enron.com", "jennifer.moore@enron.com", "richard.taylor@enron.com",
        "jessica.anderson@enron.com", "william.jackson@enron.com", "lisa.white@enron.com"
    ]

    recipients = [
        "sales-team@enron.com", "executive-committee@enron.com", "it-support@enron.com",
        "finance-department@enron.com", "hr-staff@enron.com", "marketing-team@enron.com",
        "legal-department@enron.com", "customer-service@enron.com", "research-team@enron.com"
    ]

    body_templates = [
        "Hi team,\n\n{content}\n\nRegards,\n{sender_name}",
        "Dear colleagues,\n\n{content}\n\nThank you,\n{sender_name}",
        "Hello,\n\n{content}\n\nBest,\n{sender_name}",
        "Team,\n\n{content}\n\nCheers,\n{sender_name}",
        "Greetings,\n\n{content}\n\nBest wishes,\n{sender_name}"
    ]

    content_snippets = [
        "Please find attached the latest financial reports for Q3. We need to review these before our meeting next week.",
        "I wanted to update you on the progress of Project Alpha. We're currently on track to meet our deadlines, but there are some resource constraints we need to address.",
        "The client meeting scheduled for Thursday has been moved to Friday at 2pm. Please adjust your calendars accordingly.",
        "We need to finalize the budget for the next fiscal year. Please send me your department's projections by end of day tomorrow.",
        "The new HR policy regarding remote work will take effect starting next month. More details will be provided in the upcoming town hall.",
        "Our team has exceeded sales targets for this quarter. I'd like to thank everyone for their hard work and dedication.",
        "There will be system maintenance this weekend. Please save your work and log out before leaving on Friday.",
        "The marketing campaign for Product X has been very successful. We've seen a 30% increase in leads since launch.",
        "We're planning a team building event for next month. Please fill out the survey to indicate your preferences.",
        "The legal team has reviewed the contract and has some concerns about clauses 3.2 and 5.7. We need to discuss this further.",
        "I'll be out of office next week attending the industry conference. Please direct urgent matters to my assistant.",
        "We need volunteers for the company charity event. If you're interested, please let me know by Wednesday.",
        "The quarterly performance reviews are coming up. Managers should schedule meetings with their team members soon.",
        "We're considering new vendors for our IT services. If you have any recommendations, please share them with the procurement team.",
        "The customer satisfaction survey results are in, and I'm pleased to say we've improved by 15% compared to last year."
    ]

    # Generate random emails
    emails = []
    for i in range(num_samples):
        sender = rng.choice(senders)
        sender_name = sender.split('@')[0].replace('.', ' ').title()

        recipient = rng.choice(recipients)
        subject = rng.choice(subjects)

        # Create email body
        content = rng.choice(content_snippets)
        template = rng.choice(body_templates)
        body = template.format(content=content, sender_name=sender_name)

        # Add random additional content
        if rng.random() < 0.7:
            additional_content = rng.choice(content_snippets)
            body += f"\n\nP.S. {additional_content}"

        # Create email entry
        email = {
            "id": f"email_{i}",
            "sender": sender,
            "recipient": recipient,
            "subject": subject,
            "body": body,
            "date": f"2023-{rng.randint(1, 12):02d}-{rng.randint(1, 28):02d}"
        }

        emails.append(email)

    return pd.DataFrame(emails)


def load_stackoverflow_sample(num_samples: int = 1000, seed: int = 42) -> pd.DataFrame:
    """
    Load a sample of the Stack Overflow questions dataset.

    Args:
        num_samples: Number of samples to load.
        seed: Random seed for reproducibility.

    Returns:
        DataFrame with the loaded samples.
    """
    # This is a placeholder. In a real implementation, you would download or load the actual dataset.
    # For demo purposes, we'll generate some fake Stack Overflow questions.

    rng = random.Random(seed)

    # Sample question titles
    titles = [
        "How to implement authentication using JWT in React",
        "Python pandas groupby with multiple columns",
        "Best practices for error handling in asynchronous JavaScript",
        "How to optimize SQL queries for large databases",
        "Understanding Docker container networking",
        "Converting DataFrame to dictionary in pandas",
        "Efficient ways to handle state in React components",
        "Using LSTM networks for time series prediction",
        "How to implement pagination in a REST API",
        "Setting up CI/CD pipeline with GitHub Actions",
        "Optimizing performance in Django applications",
        "Best way to handle form validation in Angular",
        "Understanding memory management in Node.js",
        "How to implement websockets in a Flask application",
        "Strategies for database indexing in PostgreSQL",
        "Implementing role-based access control in Express.js",
        "Understanding async/await in JavaScript",
        "Best practices for microservice architecture",
        "How to use Redux with React hooks",
        "Implementing full-text search in MongoDB"
    ]

    # Sample tags
    all_tags = [
        "python", "javascript", "java", "c#", "php", "android", "html", "jquery",
        "css", "ios", "sql", "mysql", "r", "node.js", "arrays", "c++", "json",
        "ruby-on-rails", "c", "sql-server", "swift", "objective-c", "django",
        "angular", "excel", "regex", "ruby", "iphone", "ajax", "xml", "asp.net",
        "linux", "react.js", "database", "wpf", "spring", "algorithm", "python-3.x",
        "vba", "dataframe", "windows", "pandas", "mongodb", "xcode", "multithreading",
        "string", "typescript", "laravel", "oracle", "bash", "git", "firebase",
        "tensorflow", "api", "docker", "rest", "kubernetes", "web-development"
    ]

    # Sample question templates
    question_templates = [
        "I'm trying to {action} but I'm running into an issue where {problem}. Here's my code:\n\n```{language}\n{code_snippet}\n```\n\nWhat am I doing wrong?",
        "What's the best way to {action} in {technology}? I've tried {approach} but it {problem}.",
        "I need to {action} and I'm not sure how to approach it. My current solution is:\n\n```{language}\n{code_snippet}\n```\n\nIs there a more efficient way?",
        "Can someone explain how to {action} in {technology}? The documentation is unclear on this point.",
        "I'm having trouble understanding how {concept} works in {technology}. Specifically, I don't understand {problem}."
    ]

    # Sample actions
    actions = [
        "implement authentication",
        "optimize database queries",
        "handle form validation",
        "set up a CI/CD pipeline",
        "debug memory leaks",
        "implement pagination",
        "handle state management",
        "optimize performance",
        "implement websockets",
        "create a RESTful API",
        "deploy to production",
        "implement caching",
        "handle error logging",
        "set up unit tests",
        "implement file uploads",
        "create a responsive design",
        "handle concurrent requests",
        "implement search functionality",
        "secure API endpoints",
        "migrate to a new framework"
    ]

    # Sample problems
    problems = [
        "I get an error message",
        "it's not working as expected",
        "performance is very slow",
        "the application crashes",
        "the data isn't being saved correctly",
        "it works in development but not in production",
        "it's not compatible with older browsers",
        "it works inconsistently",
        "I'm getting undefined behavior",
        "it's causing memory leaks",
        "I'm getting strange side effects",
        "it conflicts with other parts of my code",
        "it's not secure",
        "I'm getting timeout errors",
        "it's not scalable"
    ]

    # Sample technologies
    technologies = [
        "React", "Angular", "Vue.js", "Node.js", "Django", "Flask", "Rails",
        "Spring Boot", "Express.js", "Laravel", "ASP.NET Core", "Symfony",
        "jQuery", "Bootstrap", "Tailwind CSS", "PostgreSQL", "MongoDB", "MySQL",
        "Redis", "ElasticSearch", "Docker", "Kubernetes", "AWS", "Azure",
        "Google Cloud", "Firebase", "TensorFlow", "PyTorch", "Pandas", "NumPy"
    ]

    # Sample code snippets for different languages
    code_snippets = {
        "python": [
            "def process_data(data):\n    result = []\n    for item in data:\n        if item['valid']:\n            result.append(item['value'] * 2)\n    return result",
            "class User:\n    def __init__(self, username, email):\n        self.username = username\n        self.email = email\n        \n    def validate(self):\n        if '@' not in self.email:\n            raise ValueError('Invalid email')",
            "import pandas as pd\n\ndf = pd.read_csv('data.csv')\ngrouped = df.groupby(['category', 'region']).agg({'sales': 'sum'})",
            "from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/api/users', methods=['GET'])\ndef get_users():\n    page = request.args.get('page', 1, type=int)\n    limit = request.args.get('limit', 10, type=int)\n    # pagination logic here"
        ],
        "javascript": [
            "function fetchData() {\n  return fetch('https://api.example.com/data')\n    .then(response => response.json())\n    .then(data => {\n      return data.items.map(item => {\n        return {\n          id: item.id,\n          name: item.name,\n          value: item.value * 2\n        };\n      });\n    })\n    .catch(error => console.error('Error:', error));\n}",
            "class AuthService {\n  constructor(apiClient) {\n    this.apiClient = apiClient;\n    this.token = localStorage.getItem('token');\n  }\n\n  isAuthenticated() {\n    return !!this.token;\n  }\n\n  login(credentials) {\n    return this.apiClient.post('/login', credentials)\n      .then(response => {\n        this.token = response.data.token;\n        localStorage.setItem('token', this.token);\n        return response;\n      });\n  }\n}",
            "function debounce(func, wait) {\n  let timeout;\n  return function(...args) {\n    clearTimeout(timeout);\n    timeout = setTimeout(() => func.apply(this, args), wait);\n  };\n}"
        ],
        "java": [
            "public class UserService {\n    private final UserRepository userRepository;\n    \n    public UserService(UserRepository userRepository) {\n        this.userRepository = userRepository;\n    }\n    \n    public User createUser(String username, String email, String password) {\n        if (userRepository.findByEmail(email).isPresent()) {\n            throw new UserAlreadyExistsException(\"Email already in use\");\n        }\n        \n        User user = new User(username, email, password);\n        return userRepository.save(user);\n    }\n}",
            "import java.util.List;\nimport java.util.stream.Collectors;\n\npublic List<String> processItems(List<Item> items) {\n    return items.stream()\n            .filter(item -> item.isActive())\n            .map(Item::getName)\n            .collect(Collectors.toList());\n}"
        ],
        "sql": [
            "SELECT\n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as total_orders,\n    SUM(o.total_amount) as total_spent\nFROM\n    customers c\nLEFT JOIN\n    orders o ON c.customer_id = o.customer_id\nWHERE\n    o.order_date BETWEEN '2022-01-01' AND '2022-12-31'\nGROUP BY\n    c.customer_id, c.name\nHAVING\n    COUNT(o.order_id) > 5\nORDER BY\n    total_spent DESC;"
        ]
    }

    # Generate random questions
    questions = []
    for i in range(num_samples):
        title = rng.choice(titles)

        # Generate 2-5 random tags
        num_tags = rng.randint(2, 5)
        tags = rng.sample(all_tags, num_tags)

        # Generate question body
        template = rng.choice(question_templates)
        action = rng.choice(actions)
        problem = rng.choice(problems)
        technology = rng.choice(technologies)

        # Choose a language and code snippet if needed
        language = rng.choice(list(code_snippets.keys()))
        code_snippet = rng.choice(code_snippets[language])

        # Format the question
        body = template.format(
            action=action,
            problem=problem,
            technology=technology,
            language=language,
            code_snippet=code_snippet,
            approach=f"using {rng.choice(technologies)}",
            concept=rng.choice([
                "asynchronous programming", "dependency injection", "state management",
                "object-relational mapping", "middleware", "event propagation"
            ])
        )

        # Create question entry
        question = {
            "id": f"question_{i}",
            "title": title,
            "body": body,
            "tags": tags,
            "score": rng.randint(-2, 50),
            "view_count": rng.randint(10, 5000),
            "answer_count": rng.randint(0, 10),
            "creation_date": f"2023-{rng.randint(1, 12):02d}-{rng.randint(1, 28):02d}"
        }

        questions.append(question)

    return pd.DataFrame(questions)

def load_configuration(config_path: str) -> Dict[str, Any]:
    """Load pipeline configuration from a JSON file."""
    with open(config_path, 'r') as f:
        config = json.load(f)
    return config

def save_results(results, output_path: str):
    """Save results to a file."""
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    with open(output_path, 'w') as f:
        if isinstance(results, dict):
            json.dump(results, f, indent=2)
        else:
            f.write(results)

    logger.info(f"Results saved to {output_path}")


def run_pipeline(
    *,
    text: Optional[str] = None,
    file_path: Optional[str] = None,
    dataset: Optional[str] = None,
    num_samples: int = 5,
    config_path: Optional[str] = None,
    hf_token: str = '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',
    max_tokens: int = 256,
    prompt_template: Optional[str] = None,
    output_path: str = "pipeline_output.json",
    all_stages: bool = False,
):
    """Run the Privacy Filter Pipeline without relying on argparse.

    Parameters
    ----------
    text : str, optional
        Direct text to process through the pipeline.
    file_path : str, optional
        Path to a file whose contents will be processed.
    dataset : {"enron", "stackoverflow"}, optional
        Name of a sample dataset to load. Exactly one of ``text``, ``file_path``, or
        ``dataset`` should be provided.
    num_samples : int, default 5
        Number of samples to draw from the dataset if ``dataset`` is chosen.
    config_path : str, optional
        Path to a JSON configuration file that contains pipeline settings.
    hf_token : str, optional
        Hugging Face token. If ``None``, the code will fall back to the
        ``HF_TOKEN`` environment variable.
    max_tokens : int, default 256
        Maximum length for generated text.
    prompt_template : str, optional
        Custom generation prompt; must contain the ``{text}`` placeholder.
    output_path : str, default "pipeline_output.json"
        Where to write the results.
    all_stages : bool, default False
        If ``True``, return intermediate representations (preâ€‘filter, generated,
        etc.) rather than just the final text.
    """

    # --- Sanity checks -----------------------------------------------------
    supplied = [arg is not None for arg in (text, file_path, dataset)]
    if sum(supplied) != 1:
        raise ValueError("Provide exactly one of `text`, `file_path`, or `dataset`.")

    # --- Token & configuration -------------------------------------------
    if not hf_token:
        hf_token = os.environ.get("HF_TOKEN")
    if not hf_token:
        raise ValueError("A HuggingFace token must be supplied via `hf_token` or the HF_TOKEN environment variable.")

    config = {}
    if config_path:
        config = load_configuration(config_path)

    # Build the pipelineâ€‘specific kwargs
    pipeline_args = {
        "hf_auth_token": hf_token,
        **{k: config[k] for k in ("pre_filter_config", "generator_config", "post_filter_config", "model_name", "sensitive_items") if k in config}
    }

    logger.info("Initializing Privacy Filter Pipeline â€¦")
    pipeline = PrivacyFilterPipeline(**pipeline_args)

    # --- Input acquisition -------------------------------------------------
    if text is not None:
        input_text = text
        process_single = True
    elif file_path is not None:
        with open(file_path, "r", encoding="utfâ€‘8") as f:
            input_text = f.read()
        process_single = True
    else:  # dataset branch
        process_single = False
        if dataset == "enron":
            logger.info(f"Loading {num_samples} samples from Enron dataset â€¦")
            data = load_enron_sample(num_samples=num_samples)
            text_column = "body"
        elif dataset == "stackoverflow":
            logger.info(f"Loading {num_samples} samples from Stack Overflow dataset â€¦")
            data = load_stackoverflow_sample(num_samples=num_samples)
            text_column = "body"
        else:
            raise ValueError("Dataset must be either 'enron' or 'stackoverflow'.")

    # --- Processing --------------------------------------------------------
    if process_single:
        logger.info("Processing single text through the pipeline â€¦")
        result = pipeline.process(
            text=input_text,
            prompt_template=prompt_template,
            max_new_tokens=max_tokens,
            return_all_stages=all_stages,
        )

        _pretty_print_single_result(input_text, result, all_stages)
        save_results(result, output_path)
    else:
        logger.info("Processing dataset through the pipeline â€¦")

        injector = SensitiveDataInjector(
            pii_injection_rate=0.8,  # High rate for demonstration
            canary_injection_rate=0.3,
        )
        processed_data, stats = injector.process_dataset(
            input_data=data,
            text_column=text_column,
        )

        logger.info("Injected PII into %d samples, canaries into %d samples", stats["pii_injections"], stats["canary_injections"])

        result_df = pipeline.process_dataframe(
            df=processed_data,
            text_column=text_column,
            output_column="processed_text",
            all_stages_column="pipeline_stages" if all_stages else None,
            prompt_template=prompt_template,
            max_new_tokens=max_tokens,
        )

        _pretty_print_dataframe_samples(result_df, text_column)
        result_df.to_json(output_path, orient="records", indent=2)
        logger.info("Results saved to %s", output_path)


# ---------------------------------------------------------------------------
# Helper utilities
# ---------------------------------------------------------------------------

def _pretty_print_single_result(original: str, result, show_all: bool):
    if show_all:
        print("\n--- Original Text ---\n", original)
        print("\n--- Preâ€‘Filtered Text ---\n", result["pre_filtered_text"])
        print("\n--- Generated Text ---\n", result["generated_text"])
        print("\n--- Final Text ---\n", result["final_text"])
        print("\n--- Pipeline Stats ---\n", json.dumps(result["stats"]["pipeline"], indent=2))
    else:
        print("\n--- Original Text ---\n", original)
        print("\n--- Processed Text ---\n", result)
        print("\n--- Pipeline Stats ---\n", json.dumps(result.get("pipeline_stats", {}), indent=2))


def _pretty_print_dataframe_samples(df, text_column: str, n: int = 3):
    print("\n--- Sample Results ---")
    for i in range(min(n, len(df))):
        row = df.iloc[i]
        orig = row[text_column]
        proc = row["processed_text"]
        print(f"\nSample {i+1}:")
        print("Original:")
        print((orig[:500] + "â€¦") if len(orig) > 500 else orig)
        print("\nProcessed:")
        print((proc[:500] + "â€¦") if len(proc) > 500 else proc)
        print("-" * 80)

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import spacy

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PrivacyEvaluator:
    """
    Evaluates privacy metrics for synthetic text generation.
    """

    def __init__(
        self,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        similarity_thresholds: List[float] = None,
        device: str = None,
        batch_size: int = 32,
        use_spacy: bool = True,
        spacy_model: str = "en_core_web_sm"
    ):
        """
        Initialize the privacy evaluator.

        Args:
            embedding_model: SentenceTransformer model to use for embeddings.
            similarity_thresholds: List of thresholds for similarity analysis.
            device: Device to use for embeddings (cuda, cpu, mps).
            batch_size: Batch size for computing embeddings.
            use_spacy: Whether to use spaCy for entity extraction.
            spacy_model: spaCy model to use for entity extraction.
        """
        self.similarity_thresholds = similarity_thresholds or [0.8, 0.85, 0.9, 0.95, 0.98]
        self.batch_size = batch_size

        # Load embedding model
        try:
            logger.info(f"Loading embedding model: {embedding_model}")
            self.embedding_model = SentenceTransformer(embedding_model, device=device)
        except Exception as e:
            logger.error(f"Failed to load embedding model: {str(e)}")
            raise

        # Load spaCy model
        self.use_spacy = use_spacy
        if use_spacy:
            try:
                self.nlp = spacy.load(spacy_model)
                logger.info(f"Loaded spaCy model: {spacy_model}")
            except Exception as e:
                logger.error(f"Failed to load spaCy model: {str(e)}")
                logger.warning("Downloading spaCy model...")
                spacy.cli.download(spacy_model)
                self.nlp = spacy.load(spacy_model)

    def evaluate_pii_leakage(
        self,
        original_data: pd.DataFrame,
        generated_texts: List[str],
        text_column: str,
        pii_metadata_column: str = "pii_metadata",
        ignore_case: bool = True
    ) -> Dict[str, Any]:
        """
        Evaluate PII leakage rates in generated texts.

        Args:
            original_data: DataFrame with original data and PII metadata.
            generated_texts: List of generated texts to evaluate.
            text_column: Column containing the text in original_data.
            pii_metadata_column: Column containing PII metadata in original_data.
            ignore_case: Whether to ignore case when matching PII.

        Returns:
            Dictionary with PII leakage metrics.
        """
        if len(original_data) != len(generated_texts):
            raise ValueError("Number of original samples and generated texts must match")

        # Track PII leakage
        total_samples = len(original_data)
        total_with_pii = 0
        total_leaked = 0
        pii_types_leaked = {}
        leaked_samples = []

        # Evaluate each sample
        for i, (_, row) in enumerate(original_data.iterrows()):
            # Skip samples without PII
            pii_metadata = row.get(pii_metadata_column, {})
            if not pii_metadata or not isinstance(pii_metadata, dict) or not pii_metadata.get("injected", False):
                continue

            total_with_pii += 1
            generated_text = generated_texts[i]

            # Check for PII leakage
            sample_leaked = False
            leaked_pii_types = []
            leaked_values = []

            for injection in pii_metadata.get("injections", []):
                pii_type = injection.get("type", "unknown")
                pii_value = injection.get("value", "")

                # Skip empty values
                if not pii_value:
                    continue

                # Check if the PII value is in the generated text
                if ignore_case:
                    found = pii_value.lower() in generated_text.lower()
                else:
                    found = pii_value in generated_text

                if found:
                    sample_leaked = True
                    leaked_pii_types.append(pii_type)
                    leaked_values.append(pii_value)

                    # Update type-specific counts
                    pii_types_leaked[pii_type] = pii_types_leaked.get(pii_type, 0) + 1

            # Record if this sample leaked PII
            if sample_leaked:
                total_leaked += 1
                leaked_samples.append({
                    "sample_idx": i,
                    "pii_types": leaked_pii_types,
                    "leaked_values": leaked_values
                })

        # Calculate metrics
        if total_with_pii > 0:
            leakage_rate = total_leaked / total_with_pii
            per_type_rate = {pii_type: count / total_with_pii for pii_type, count in pii_types_leaked.items()}
        else:
            leakage_rate = 0.0
            per_type_rate = {}

        return {
            "total_samples": total_samples,
            "samples_with_pii": total_with_pii,
            "samples_leaked_pii": total_leaked,
            "pii_leakage_rate": leakage_rate,
            "per_type_leakage_rate": per_type_rate,
            "leaked_samples": leaked_samples
        }

    def evaluate_canary_leakage(
        self,
        original_data: pd.DataFrame,
        generated_texts: List[str],
        text_column: str,
        canary_metadata_column: str = "canary_metadata",
        similarity_threshold: float = 0.95,
        exact_match: bool = True,
        ignore_case: bool = True
    ) -> Dict[str, Any]:
        """
        Evaluate canary phrase leakage rates in generated texts.

        Args:
            original_data: DataFrame with original data and canary metadata.
            generated_texts: List of generated texts to evaluate.
            text_column: Column containing the text in original_data.
            canary_metadata_column: Column containing canary metadata in original_data.
            similarity_threshold: Threshold for similarity-based leakage detection.
            exact_match: Whether to require exact matches (vs. similarity).
            ignore_case: Whether to ignore case when matching canaries.

        Returns:
            Dictionary with canary leakage metrics.
        """
        if len(original_data) != len(generated_texts):
            raise ValueError("Number of original samples and generated texts must match")

        # Track canary leakage
        total_samples = len(original_data)
        total_with_canary = 0
        total_leaked_exact = 0
        total_leaked_similar = 0
        canary_phrase_counts = {}
        leaked_samples = []

        # Get all unique canary phrases for batch embedding
        all_canaries = set()
        for _, row in original_data.iterrows():
            canary_metadata = row.get(canary_metadata_column, {})
            if canary_metadata and isinstance(canary_metadata, dict) and canary_metadata.get("injected", False):
                canary = canary_metadata.get("canary", "")
                if canary:
                    all_canaries.add(canary)

        # Compute embeddings for all canary phrases
        if all_canaries and not exact_match:
            canary_phrases = list(all_canaries)
            canary_embeddings = self.embedding_model.encode(
                canary_phrases,
                batch_size=self.batch_size,
                show_progress_bar=len(canary_phrases) > 10
            )
            canary_embed_map = {phrase: embedding for phrase, embedding in zip(canary_phrases, canary_embeddings)}
        else:
            canary_embed_map = {}

        # Evaluate each sample
        for i, (_, row) in enumerate(original_data.iterrows()):
            # Skip samples without canary phrases
            canary_metadata = row.get(canary_metadata_column, {})
            if not canary_metadata or not isinstance(canary_metadata, dict) or not canary_metadata.get("injected", False):
                continue

            total_with_canary += 1
            generated_text = generated_texts[i]
            canary_phrase = canary_metadata.get("canary", "")

            # Skip empty canary phrases
            if not canary_phrase:
                continue

            # Update canary phrase counts
            canary_phrase_counts[canary_phrase] = canary_phrase_counts.get(canary_phrase, 0) + 1

            # Check for exact match
            leaked_exact = False
            if ignore_case:
                if canary_phrase.lower() in generated_text.lower():
                    leaked_exact = True
            else:
                if canary_phrase in generated_text:
                    leaked_exact = True

            # Check for similarity-based leakage if exact match wasn't found
            leaked_similar = False
            similarity_score = 0.0

            if not leaked_exact and not exact_match:
                # Get embedding for generated text
                gen_embedding = self.embedding_model.encode(generated_text, batch_size=1)

                # Get embedding for this canary phrase
                canary_embedding = canary_embed_map.get(canary_phrase)

                if canary_embedding is not None:
                    # Calculate similarity
                    similarity_score = cosine_similarity(
                        [gen_embedding],
                        [canary_embedding]
                    )[0][0]

                    if similarity_score >= similarity_threshold:
                        leaked_similar = True

            # Record leakage
            if leaked_exact:
                total_leaked_exact += 1
                leaked_samples.append({
                    "sample_idx": i,
                    "canary_phrase": canary_phrase,
                    "match_type": "exact",
                    "similarity": 1.0
                })
            elif leaked_similar:
                total_leaked_similar += 1
                leaked_samples.append({
                    "sample_idx": i,
                    "canary_phrase": canary_phrase,
                    "match_type": "similar",
                    "similarity": float(similarity_score)
                })

        # Calculate metrics
        if total_with_canary > 0:
            exact_leakage_rate = total_leaked_exact / total_with_canary
            similar_leakage_rate = total_leaked_similar / total_with_canary
            total_leakage_rate = (total_leaked_exact + total_leaked_similar) / total_with_canary
        else:
            exact_leakage_rate = 0.0
            similar_leakage_rate = 0.0
            total_leakage_rate = 0.0

        return {
            "total_samples": total_samples,
            "samples_with_canary": total_with_canary,
            "samples_leaked_exact": total_leaked_exact,
            "samples_leaked_similar": total_leaked_similar,
            "exact_leakage_rate": exact_leakage_rate,
            "similar_leakage_rate": similar_leakage_rate,
            "total_leakage_rate": total_leakage_rate,
            "canary_phrase_counts": canary_phrase_counts,
            "leaked_samples": leaked_samples
        }

    def evaluate_similarity_distribution(
        self,
        original_data: pd.DataFrame,
        generated_texts: List[str],
        text_column: str,
        analyze_chunks: bool = True,
        chunk_size: int = 100
    ) -> Dict[str, Any]:
        """
        Evaluate similarity between original and generated texts.

        Args:
            original_data: DataFrame with original data.
            generated_texts: List of generated texts to evaluate.
            text_column: Column containing the text in original_data.
            analyze_chunks: Whether to analyze text chunks for more granular similarity.
            chunk_size: Size of text chunks for chunk-based analysis.

        Returns:
            Dictionary with similarity metrics.
        """
        if len(original_data) != len(generated_texts):
            raise ValueError("Number of original samples and generated texts must match")

        # Extract original texts
        original_texts = original_data[text_column].tolist()

        # Compute embeddings for whole texts
        logger.info("Computing embeddings for original texts...")
        original_embeddings = self.embedding_model.encode(
            original_texts,
            batch_size=self.batch_size,
            show_progress_bar=len(original_texts) > 10
        )

        logger.info("Computing embeddings for generated texts...")
        generated_embeddings = self.embedding_model.encode(
            generated_texts,
            batch_size=self.batch_size,
            show_progress_bar=len(generated_texts) > 10
        )

        # Calculate similarities
        similarities = []
        for i in range(len(original_embeddings)):
            similarity = cosine_similarity(
                [original_embeddings[i]],
                [generated_embeddings[i]]
            )[0][0]
            similarities.append(float(similarity))

        # Calculate metrics for whole texts
        whole_text_metrics = {
            "mean_similarity": float(np.mean(similarities)),
            "median_similarity": float(np.median(similarities)),
            "min_similarity": float(np.min(similarities)),
            "max_similarity": float(np.max(similarities)),
            "std_deviation": float(np.std(similarities))
        }

        # Calculate thresholded counts
        threshold_counts = {}
        for threshold in self.similarity_thresholds:
            count = sum(1 for sim in similarities if sim >= threshold)
            rate = count / len(similarities) if similarities else 0
            threshold_counts[f"above_{threshold:.2f}"] = {
                "count": count,
                "rate": rate
            }

        # Analyze chunks if requested
        chunk_metrics = {}
        if analyze_chunks:
            chunk_similarities = []
            high_similarity_chunks = []

            for i, (orig_text, gen_text) in enumerate(zip(original_texts, generated_texts)):
                # Skip if either text is too short
                if len(orig_text) < chunk_size or len(gen_text) < chunk_size:
                    continue

                # Create chunks
                orig_chunks = [orig_text[j:j+chunk_size] for j in range(0, len(orig_text), chunk_size)]
                gen_chunks = [gen_text[j:j+chunk_size] for j in range(0, len(gen_text), chunk_size)]

                # Calculate similarities between chunks
                for orig_idx, orig_chunk in enumerate(orig_chunks):
                    orig_embed = self.embedding_model.encode(orig_chunk, batch_size=1)

                    for gen_idx, gen_chunk in enumerate(gen_chunks):
                        gen_embed = self.embedding_model.encode(gen_chunk, batch_size=1)

                        chunk_sim = cosine_similarity([orig_embed], [gen_embed])[0][0]
                        chunk_similarities.append(float(chunk_sim))

                        # Record high similarity chunks
                        if chunk_sim >= 0.95:
                            high_similarity_chunks.append({
                                "sample_idx": i,
                                "orig_chunk_idx": orig_idx,
                                "gen_chunk_idx": gen_idx,
                                "orig_chunk": orig_chunk[:100] + "..." if len(orig_chunk) > 100 else orig_chunk,
                                "gen_chunk": gen_chunk[:100] + "..." if len(gen_chunk) > 100 else gen_chunk,
                                "similarity": float(chunk_sim)
                            })

            # Calculate chunk-level metrics
            if chunk_similarities:
                chunk_metrics = {
                    "mean_chunk_similarity": float(np.mean(chunk_similarities)),
                    "median_chunk_similarity": float(np.median(chunk_similarities)),
                    "min_chunk_similarity": float(np.min(chunk_similarities)),
                    "max_chunk_similarity": float(np.max(chunk_similarities)),
                    "std_deviation": float(np.std(chunk_similarities)),
                    "high_similarity_chunks": high_similarity_chunks[:20]  # Limit to first 20
                }

                # Calculate thresholded counts for chunks
                chunk_threshold_counts = {}
                for threshold in self.similarity_thresholds:
                    count = sum(1 for sim in chunk_similarities if sim >= threshold)
                    rate = count / len(chunk_similarities)
                    chunk_threshold_counts[f"above_{threshold:.2f}"] = {
                        "count": count,
                        "rate": rate
                    }

                chunk_metrics["threshold_counts"] = chunk_threshold_counts

        return {
            "whole_text_metrics": whole_text_metrics,
            "threshold_counts": threshold_counts,
            "chunk_metrics": chunk_metrics,
            "all_similarities": similarities
        }

    def run_adversarial_memorization_test(
        self,
        canary_phrases: List[str],
        adversarial_prompts: List[str],
        generated_texts: List[str],
        exact_match: bool = True,
        ignore_case: bool = True,
        similarity_threshold: float = 0.98
    ) -> Dict[str, Any]:
        """
        Evaluate resistance to adversarial prompts designed to elicit memorized content.

        Args:
            canary_phrases: List of canary phrases to test for.
            adversarial_prompts: List of prompts designed to elicit the canaries.
            generated_texts: List of texts generated from the adversarial prompts.
            exact_match: Whether to require exact matches (vs. similarity).
            ignore_case: Whether to ignore case when matching canaries.
            similarity_threshold: Threshold for similarity-based leakage detection.

        Returns:
            Dictionary with adversarial test metrics.
        """
        if len(adversarial_prompts) != len(generated_texts):
            raise ValueError("Number of adversarial prompts and generated texts must match")

        # Compute embeddings for canary phrases
        canary_embeddings = None
        if not exact_match:
            canary_embeddings = self.embedding_model.encode(
                canary_phrases,
                batch_size=self.batch_size,
                show_progress_bar=len(canary_phrases) > 10
            )

        # Track leakage
        total_prompts = len(adversarial_prompts)
        leaked_canaries = []

        for i, (prompt, generated_text) in enumerate(zip(adversarial_prompts, generated_texts)):
            # Check each canary phrase
            for j, canary in enumerate(canary_phrases):
                # Check for exact match
                leaked_exact = False
                if ignore_case:
                    if canary.lower() in generated_text.lower():
                        leaked_exact = True
                else:
                    if canary in generated_text:
                        leaked_exact = True

                # Check for similarity-based leakage if exact match wasn't found
                leaked_similar = False
                similarity_score = 0.0

                if not leaked_exact and not exact_match:
                    # Get embedding for generated text
                    gen_embedding = self.embedding_model.encode(generated_text, batch_size=1)

                    # Calculate similarity
                    if canary_embeddings is not None:
                        similarity_score = cosine_similarity(
                            [gen_embedding],
                            [canary_embeddings[j]]
                        )[0][0]

                        if similarity_score >= similarity_threshold:
                            leaked_similar = True

                # Record leakage
                if leaked_exact or leaked_similar:
                    leaked_canaries.append({
                        "prompt_idx": i,
                        "canary_idx": j,
                        "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
                        "canary": canary[:100] + "..." if len(canary) > 100 else canary,
                        "match_type": "exact" if leaked_exact else "similar",
                        "similarity": 1.0 if leaked_exact else float(similarity_score)
                    })

        # Calculate metrics
        success_rate = len(leaked_canaries) / (total_prompts * len(canary_phrases))

        return {
            "total_tests": total_prompts * len(canary_phrases),
            "successful_elicitations": len(leaked_canaries),
            "success_rate": success_rate,
            "leaked_canaries": leaked_canaries
        }

import math
from collections import Counter
from typing import List, Dict, Tuple, Set, Any, Union, Optional

import numpy as np
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import jensenshannon
import pandas as pd

try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    ROUGE_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UtilityEvaluator:
    """
    Evaluates utility metrics for synthetic text generation.
    """

    def __init__(
        self,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        use_perplexity: bool = True,
        gpt2_model: str = "gpt2",
        device: str = None,
        batch_size: int = 8
    ):
        """
        Initialize the utility evaluator.

        Args:
            embedding_model: SentenceTransformer model to use for embeddings.
            use_perplexity: Whether to calculate perplexity metrics.
            gpt2_model: GPT-2 model to use for perplexity calculation.
            device: Device to use for models (cuda, cpu, mps).
            batch_size: Batch size for computing embeddings and perplexity.
        """
        self.batch_size = batch_size

        # Determine device
        if device is None:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device

        logger.info(f"Using device: {self.device}")

        # Load embedding model
        try:
            logger.info(f"Loading embedding model: {embedding_model}")
            self.embedding_model = SentenceTransformer(embedding_model, device=self.device)
        except Exception as e:
            logger.error(f"Failed to load embedding model: {str(e)}")
            raise

        # Load GPT-2 model if perplexity is requested
        self.use_perplexity = use_perplexity
        if use_perplexity:
            try:
                logger.info(f"Loading GPT-2 model: {gpt2_model}")
                self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(gpt2_model)
                # Set pad token to eos token
                self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token
                self.gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model).to(self.device)
                self.gpt2_model.eval()
            except Exception as e:
                logger.error(f"Failed to load GPT-2 model: {str(e)}")
                self.use_perplexity = False
                logger.warning("Perplexity calculations will be skipped")

    def calculate_perplexity(self, texts: List[str]) -> Dict[str, Any]:
        """
        Calculate perplexity for a list of texts using GPT-2.

        Args:
            texts: List of texts to calculate perplexity for.

        Returns:
            Dictionary with perplexity metrics.
        """
        if not self.use_perplexity:
            return {"error": "Perplexity calculation is disabled"}

        # Calculate perplexity
        perplexities = []

        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i+self.batch_size]

            encodings = self.gpt2_tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=1024
            ).to(self.device)

            with torch.no_grad():
                # We need to provide labels for loss calculation
                input_ids = encodings.input_ids
                outputs = self.gpt2_model(input_ids=input_ids, labels=input_ids)
                if outputs.loss is not None:
                    log_likelihood = outputs.loss.item() * input_ids.size(1)
                else:
                    # Skip this batch if loss is None
                    continue

                # Calculate perplexity for each text in the batch
                for j, text in enumerate(batch_texts):
                    # Tokenize individual text
                    input_ids = self.gpt2_tokenizer.encode(text, return_tensors="pt").to(self.device)

                    # Skip empty texts
                    if input_ids.size(1) <= 1:
                        continue

                    # Calculate perplexity
                    with torch.no_grad():
                        outputs = self.gpt2_model(input_ids, labels=input_ids)
                        neg_log_likelihood = outputs.loss.item()
                        ppl = math.exp(neg_log_likelihood)
                        perplexities.append(ppl)

        # Calculate metrics
        if perplexities:
            result = {
                "mean_perplexity": float(np.mean(perplexities)),
                "median_perplexity": float(np.median(perplexities)),
                "min_perplexity": float(np.min(perplexities)),
                "max_perplexity": float(np.max(perplexities)),
                "std_deviation": float(np.std(perplexities))
            }
        else:
            result = {"error": "Failed to calculate perplexity for any text"}

        return result

    def calculate_semantic_similarity(
        self,
        original_texts: List[str],
        generated_texts: List[str]
    ) -> Dict[str, Any]:
        """
        Calculate semantic similarity between original and generated texts.

        Args:
            original_texts: List of original texts.
            generated_texts: List of generated texts.

        Returns:
            Dictionary with semantic similarity metrics.
        """
        if len(original_texts) != len(generated_texts):
            return {"error": "Number of original and generated texts must match"}

        # Calculate embeddings
        logger.info("Computing embeddings for original texts...")
        original_embeddings = self.embedding_model.encode(
            original_texts,
            batch_size=self.batch_size,
            show_progress_bar=len(original_texts) > 10
        )

        logger.info("Computing embeddings for generated texts...")
        generated_embeddings = self.embedding_model.encode(
            generated_texts,
            batch_size=self.batch_size,
            show_progress_bar=len(generated_texts) > 10
        )

        # Calculate similarities
        similarities = []
        for i in range(len(original_embeddings)):
            similarity = cosine_similarity(
                [original_embeddings[i]],
                [generated_embeddings[i]]
            )[0][0]
            similarities.append(float(similarity))

        # Calculate metrics
        result = {
            "mean_similarity": float(np.mean(similarities)),
            "median_similarity": float(np.median(similarities)),
            "min_similarity": float(np.min(similarities)),
            "max_similarity": float(np.max(similarities)),
            "std_deviation": float(np.std(similarities))
        }

        return result

    def calculate_rouge_scores(
        self,
        original_texts: List[str],
        generated_texts: List[str]
    ) -> Dict[str, Any]:
        """
        Calculate ROUGE scores between original and generated texts.

        Args:
            original_texts: List of original texts.
            generated_texts: List of generated texts.

        Returns:
            Dictionary with ROUGE score metrics.
        """
        if not ROUGE_AVAILABLE:
            return {"error": "rouge_score library not available"}

        if len(original_texts) != len(generated_texts):
            return {"error": "Number of original and generated texts must match"}

        # Initialize ROUGE scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

        # Calculate scores
        scores = []
        for original, generated in zip(original_texts, generated_texts):
            score = scorer.score(original, generated)
            scores.append({
                'rouge1_precision': score['rouge1'].precision,
                'rouge1_recall': score['rouge1'].recall,
                'rouge1_fmeasure': score['rouge1'].fmeasure,
                'rouge2_precision': score['rouge2'].precision,
                'rouge2_recall': score['rouge2'].recall,
                'rouge2_fmeasure': score['rouge2'].fmeasure,
                'rougeL_precision': score['rougeL'].precision,
                'rougeL_recall': score['rougeL'].recall,
                'rougeL_fmeasure': score['rougeL'].fmeasure
            })

        # Calculate averages
        avg_scores = {}
        for metric in scores[0].keys():
            avg_scores[metric] = float(np.mean([s[metric] for s in scores]))

        return {
            "average_scores": avg_scores,
            "all_scores": scores[:10]  # Return first 10 individual scores
        }

    def calculate_n_gram_diversity(
        self,
        original_texts: List[str],
        generated_texts: List[str],
        n_values: List[int] = None
    ) -> Dict[str, Any]:
        """
        Calculate n-gram diversity statistics.

        Args:
            original_texts: List of original texts.
            generated_texts: List of generated texts.
            n_values: List of n-gram sizes to calculate.

        Returns:
            Dictionary with n-gram diversity metrics.
        """
        n_values = n_values or [1, 2, 3]

        def get_n_grams(texts: List[str], n: int) -> Counter:
            """Get n-gram counts for a list of texts."""
            n_grams = Counter()
            for text in texts:
                tokens = text.lower().split()
                if len(tokens) < n:
                    continue
                text_n_grams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
                n_grams.update(text_n_grams)
            return n_grams

        # Calculate n-gram distributions
        result = {}
        for n in n_values:
            original_n_grams = get_n_grams(original_texts, n)
            generated_n_grams = get_n_grams(generated_texts, n)

            # Calculate unique n-grams
            unique_original = len(original_n_grams)
            unique_generated = len(generated_n_grams)
            total_original = sum(original_n_grams.values())
            total_generated = sum(generated_n_grams.values())

            # Calculate probabilities for Jensen-Shannon divergence
            if total_original > 0 and total_generated > 0 and unique_original > 0 and unique_generated > 0:
                # Get sets of n-grams
                all_n_grams = set(original_n_grams.keys()) | set(generated_n_grams.keys())

                # Calculate probability distributions
                orig_probs = np.array([original_n_grams.get(n_gram, 0) / total_original for n_gram in all_n_grams])
                gen_probs = np.array([generated_n_grams.get(n_gram, 0) / total_generated for n_gram in all_n_grams])

                # Add smoothing for zero probabilities
                orig_probs = np.maximum(orig_probs, 1e-10)
                gen_probs = np.maximum(gen_probs, 1e-10)

                # Normalize
                orig_probs = orig_probs / np.sum(orig_probs)
                gen_probs = gen_probs / np.sum(gen_probs)

                # Calculate Jensen-Shannon divergence
                jsd = jensenshannon(orig_probs, gen_probs)

                # Calculate normalized entropy
                generated_entropy = -np.sum(gen_probs * np.log2(gen_probs)) / np.log2(len(gen_probs))
                original_entropy = -np.sum(orig_probs * np.log2(orig_probs)) / np.log2(len(orig_probs))

                result[f"{n}-gram"] = {
                    "unique_original": unique_original,
                    "unique_generated": unique_generated,
                    "total_original": total_original,
                    "total_generated": total_generated,
                    "unique_ratio_original": unique_original / total_original if total_original > 0 else 0,
                    "unique_ratio_generated": unique_generated / total_generated if total_generated > 0 else 0,
                    "jensen_shannon_divergence": float(jsd),
                    "original_entropy_normalized": float(original_entropy),
                    "generated_entropy_normalized": float(generated_entropy)
                }
            else:
                result[f"{n}-gram"] = {
                    "error": "Insufficient data for n-gram diversity calculation"
                }

        return result

    def calculate_output_statistics(
        self,
        original_texts: List[str],
        generated_texts: List[str]
    ) -> Dict[str, Any]:
        """
        Calculate basic output statistics.

        Args:
            original_texts: List of original texts.
            generated_texts: List of generated texts.

        Returns:
            Dictionary with output statistics.
        """
        # Calculate lengths
        original_lengths = [len(text.split()) for text in original_texts]
        generated_lengths = [len(text.split()) for text in generated_texts]

        # Calculate character lengths
        original_char_lengths = [len(text) for text in original_texts]
        generated_char_lengths = [len(text) for text in generated_texts]

        # Calculate lengths of sentences
        sentence_pattern = r'[.!?]+\s+'
        original_sentence_counts = [len(re.split(sentence_pattern, text)) for text in original_texts]
        generated_sentence_counts = [len(re.split(sentence_pattern, text)) for text in generated_texts]

        # Count placeholder tokens
        placeholder_pattern = r'\[(REDACTED|PII|NAME|EMAIL|PHONE|ADDRESS|SSN|CREDIT_CARD|PERSON|ORG|GPE|LOC|\w+_\d+)\]'
        placeholder_counts = [len(re.findall(placeholder_pattern, text)) for text in generated_texts]

        result = {
            "word_count": {
                "original_mean": float(np.mean(original_lengths)),
                "original_median": float(np.median(original_lengths)),
                "original_min": float(np.min(original_lengths)),
                "original_max": float(np.max(original_lengths)),
                "original_std": float(np.std(original_lengths)),
                "generated_mean": float(np.mean(generated_lengths)),
                "generated_median": float(np.median(generated_lengths)),
                "generated_min": float(np.min(generated_lengths)),
                "generated_max": float(np.max(generated_lengths)),
                "generated_std": float(np.std(generated_lengths)),
                "ratio_mean": float(np.mean(generated_lengths) / np.mean(original_lengths)) if np.mean(original_lengths) > 0 else 0
            },
            "char_count": {
                "original_mean": float(np.mean(original_char_lengths)),
                "original_median": float(np.median(original_char_lengths)),
                "original_min": float(np.min(original_char_lengths)),
                "original_max": float(np.max(original_char_lengths)),
                "original_std": float(np.std(original_char_lengths)),
                "generated_mean": float(np.mean(generated_char_lengths)),
                "generated_median": float(np.median(generated_char_lengths)),
                "generated_min": float(np.min(generated_char_lengths)),
                "generated_max": float(np.max(generated_char_lengths)),
                "generated_std": float(np.std(generated_char_lengths)),
                "ratio_mean": float(np.mean(generated_char_lengths) / np.mean(original_char_lengths)) if np.mean(original_char_lengths) > 0 else 0
            },
            "sentence_count": {
                "original_mean": float(np.mean(original_sentence_counts)),
                "original_median": float(np.median(original_sentence_counts)),
                "original_min": float(np.min(original_sentence_counts)),
                "original_max": float(np.max(original_sentence_counts)),
                "original_std": float(np.std(original_sentence_counts)),
                "generated_mean": float(np.mean(generated_sentence_counts)),
                "generated_median": float(np.median(generated_sentence_counts)),
                "generated_min": float(np.min(generated_sentence_counts)),
                "generated_max": float(np.max(generated_sentence_counts)),
                "generated_std": float(np.std(generated_sentence_counts)),
                "ratio_mean": float(np.mean(generated_sentence_counts) / np.mean(original_sentence_counts)) if np.mean(original_sentence_counts) > 0 else 0
            },
            "placeholder_count": {
                "mean": float(np.mean(placeholder_counts)),
                "median": float(np.median(placeholder_counts)),
                "min": float(np.min(placeholder_counts)),
                "max": float(np.max(placeholder_counts)),
                "std": float(np.std(placeholder_counts)),
                "texts_with_placeholders": sum(1 for count in placeholder_counts if count > 0),
                "texts_with_placeholders_rate": sum(1 for count in placeholder_counts if count > 0) / len(generated_texts)
            }
        }

        return result

    def calculate_all_metrics(
        self,
        original_texts: List[str],
        generated_texts: List[str]
    ) -> Dict[str, Any]:
        """
        Calculate all utility metrics.

        Args:
            original_texts: List of original texts.
            generated_texts: List of generated texts.

        Returns:
            Dictionary with all metrics.
        """
        # Ensure the inputs have the same length
        if len(original_texts) != len(generated_texts):
            return {"error": "Number of original and generated texts must match"}

        # Calculate perplexity
        logger.info("Calculating perplexity...")
        perplexity_metrics = {}
        if self.use_perplexity:
            perplexity_metrics = self.calculate_perplexity(generated_texts)

        # Calculate semantic similarity
        logger.info("Calculating semantic similarity...")
        similarity_metrics = self.calculate_semantic_similarity(original_texts, generated_texts)

        # Calculate ROUGE scores
        logger.info("Calculating ROUGE scores...")
        rouge_metrics = {}
        if ROUGE_AVAILABLE:
            rouge_metrics = self.calculate_rouge_scores(original_texts, generated_texts)

        # Calculate n-gram diversity
        logger.info("Calculating n-gram diversity...")
        n_gram_metrics = self.calculate_n_gram_diversity(original_texts, generated_texts)

        # Calculate output statistics
        logger.info("Calculating output statistics...")
        output_stats = self.calculate_output_statistics(original_texts, generated_texts)

        # Combine all metrics
        all_metrics = {
            "perplexity": perplexity_metrics,
            "semantic_similarity": similarity_metrics,
            "rouge_scores": rouge_metrics,
            "n_gram_diversity": n_gram_metrics,
            "output_statistics": output_stats
        }

        return all_metrics



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PrivacyFrameworkEvaluator:
    """
    Main evaluation pipeline for the synthetic data privacy framework.
    """

    def __init__(
        self,
        privacy_evaluator: Optional[PrivacyEvaluator] = None,
        utility_evaluator: Optional[UtilityEvaluator] = None,
        device: str = None,
        results_dir: str = "results"
    ):
        """
        Initialize the privacy framework evaluator.

        Args:
            privacy_evaluator: Preconfigured PrivacyEvaluator instance.
            utility_evaluator: Preconfigured UtilityEvaluator instance.
            device: Device to use for evaluations (cuda, cpu, mps).
            results_dir: Directory to store evaluation results.
        """
        # Initialize evaluators if not provided
        self.privacy_evaluator = privacy_evaluator or PrivacyEvaluator(device=device)
        self.utility_evaluator = utility_evaluator or UtilityEvaluator(device=device)

        # Setup results directory
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)

        # Store experimental configurations
        self.configurations = {}
        self.results = {}

    def add_configuration(
        self,
        config_id: str,
        config_name: str,
        config_description: str,
        configuration: Dict[str, Any]
    ):
        """
        Add a configuration to be evaluated.

        Args:
            config_id: Unique identifier for the configuration.
            config_name: Short name for the configuration.
            config_description: Description of the configuration.
            configuration: Configuration parameters.
        """
        self.configurations[config_id] = {
            "id": config_id,
            "name": config_name,
            "description": config_description,
            "configuration": configuration
        }

        logger.info(f"Added configuration: {config_name} (ID: {config_id})")

    def evaluate_configuration(
        self,
        config_id: str,
        original_data: pd.DataFrame,
        generated_data: Union[pd.DataFrame, List[str]],
        text_column: str,
        pii_metadata_column: str = "pii_metadata",
        canary_metadata_column: str = "canary_metadata",
        adversarial_results: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a specific configuration.

        Args:
            config_id: Identifier of the configuration to evaluate.
            original_data: DataFrame with original data and metadata.
            generated_data: DataFrame or list with generated texts.
            text_column: Column containing the text.
            pii_metadata_column: Column containing PII metadata.
            canary_metadata_column: Column containing canary metadata.
            adversarial_results: Optional results from adversarial testing.

        Returns:
            Dictionary with evaluation results.
        """
        if config_id not in self.configurations:
            logger.error(f"Configuration ID {config_id} not found")
            return {"error": f"Configuration ID {config_id} not found"}

        config = self.configurations[config_id]
        logger.info(f"Evaluating configuration: {config['name']} (ID: {config_id})")

        # Extract original texts
        original_texts = original_data[text_column].tolist()

        # Extract generated texts
        if isinstance(generated_data, pd.DataFrame):
            if text_column not in generated_data.columns:
                logger.error(f"Text column '{text_column}' not found in generated data")
                return {"error": f"Text column '{text_column}' not found in generated data"}

            generated_texts = generated_data[text_column].tolist()
        else:
            generated_texts = generated_data

        # Ensure we have matching counts
        if len(original_texts) != len(generated_texts):
            logger.error(f"Number of original texts ({len(original_texts)}) does not match number of generated texts ({len(generated_texts)})")
            return {"error": "Number of original and generated texts must match"}

        # Evaluate privacy metrics
        logger.info("Evaluating privacy metrics...")

        pii_leakage = self.privacy_evaluator.evaluate_pii_leakage(
            original_data=original_data,
            generated_texts=generated_texts,
            text_column=text_column,
            pii_metadata_column=pii_metadata_column
        )

        canary_leakage = self.privacy_evaluator.evaluate_canary_leakage(
            original_data=original_data,
            generated_texts=generated_texts,
            text_column=text_column,
            canary_metadata_column=canary_metadata_column
        )

        similarity_distribution = self.privacy_evaluator.evaluate_similarity_distribution(
            original_data=original_data,
            generated_texts=generated_texts,
            text_column=text_column
        )

        # Evaluate utility metrics
        logger.info("Evaluating utility metrics...")

        utility_metrics = self.utility_evaluator.calculate_all_metrics(
            original_texts=original_texts,
            generated_texts=generated_texts
        )

        # Combine all results
        results = {
            "config_id": config_id,
            "config_name": config["name"],
            "config_description": config["description"],
            "configuration": config["configuration"],
            "privacy_metrics": {
                "pii_leakage": pii_leakage,
                "canary_leakage": canary_leakage,
                "similarity_distribution": similarity_distribution,
                "adversarial_results": adversarial_results or {}
            },
            "utility_metrics": utility_metrics
        }

        # Store results
        self.results[config_id] = results

        # Save results to file
        self._save_results(config_id, results)

        return results

    def _save_results(self, config_id: str, results: Dict[str, Any]):
        """Save evaluation results to file."""
        results_path = os.path.join(self.results_dir, f"{config_id}_results.json")

        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2)

        logger.info(f"Saved evaluation results to {results_path}")

    def create_comparison_charts(
        self,
        config_ids: List[str] = None,
        output_dir: Optional[str] = None
    ):
        """
        Create comparison charts for multiple configurations.

        Args:
            config_ids: List of configuration IDs to compare. If None, use all.
            output_dir: Directory to save charts. If None, use results_dir.
        """
        # Use all configurations if none specified
        if config_ids is None:
            config_ids = list(self.results.keys())

        # Skip if no results
        if not config_ids or not self.results:
            logger.warning("No results available for comparison")
            return

        # Get results for selected configurations
        configs_to_compare = []
        for config_id in config_ids:
            if config_id in self.results:
                configs_to_compare.append(self.results[config_id])
            else:
                logger.warning(f"Configuration {config_id} not found in results")

        if not configs_to_compare:
            logger.warning("No valid configurations found for comparison")
            return

        # Set output directory
        output_dir = output_dir or self.results_dir
        os.makedirs(output_dir, exist_ok=True)

        # Set plot style
        plt.style.use('seaborn-v0_8-darkgrid')

        # 1. Privacy-Utility Trade-off Chart
        self._create_privacy_utility_tradeoff(configs_to_compare, output_dir)

        # 2. PII Leakage Rate Comparison
        self._create_pii_leakage_comparison(configs_to_compare, output_dir)

        # 3. Semantic Similarity Distribution
        self._create_similarity_distribution_chart(configs_to_compare, output_dir)

        # 4. Perplexity Comparison
        self._create_perplexity_comparison(configs_to_compare, output_dir)

        # 5. Output Length Comparison
        self._create_output_length_comparison(configs_to_compare, output_dir)

    def _create_privacy_utility_tradeoff(
        self,
        configs: List[Dict[str, Any]],
        output_dir: str
    ):
        """Create privacy-utility trade-off chart."""
        # Extract data for the chart
        labels = []
        pii_leakage_rates = []
        semantic_similarities = []
        perplexities = []

        for config in configs:
            config_name = config["config_name"]
            labels.append(config_name)

            # Get PII leakage rate
            pii_leakage = config["privacy_metrics"]["pii_leakage"].get("pii_leakage_rate", 0)
            pii_leakage_rates.append(pii_leakage)

            # Get semantic similarity
            semantic_similarity = config["utility_metrics"]["semantic_similarity"].get("mean_similarity", 0)
            semantic_similarities.append(semantic_similarity)

            # Get perplexity (lower is better)
            perplexity = config["utility_metrics"]["perplexity"].get("mean_perplexity", 0)
            if perplexity > 0:
                # Normalize perplexity to 0-1 range (inverted)
                perplexities.append(1.0 / perplexity)
            else:
                perplexities.append(0)

        # Create the trade-off chart
        fig, ax = plt.subplots(figsize=(10, 6))

        scatter = ax.scatter(
            pii_leakage_rates,
            semantic_similarities,
            s=[p * 200 for p in perplexities],  # Size based on perplexity
            alpha=0.7
        )

        # Add labels for each point
        for i, label in enumerate(labels):
            ax.annotate(
                label,
                (pii_leakage_rates[i], semantic_similarities[i]),
                xytext=(5, 5),
                textcoords='offset points'
            )

        ax.set_xlabel('PII Leakage Rate (lower is better)')
        ax.set_ylabel('Semantic Similarity (higher is better)')
        ax.set_title('Privacy-Utility Trade-off')

        # Set axis ranges
        ax.set_xlim(-0.05, max(pii_leakage_rates) + 0.1)
        ax.set_ylim(min(semantic_similarities) - 0.1, 1.05)

        # Add grid
        ax.grid(True, linestyle='--', alpha=0.7)

        # Save the chart
        plt.tight_layout()
        output_path = os.path.join(output_dir, 'privacy_utility_tradeoff.png')
        plt.savefig(output_path, dpi=300)
        plt.close()

        logger.info(f"Created privacy-utility trade-off chart: {output_path}")

    def _create_pii_leakage_comparison(
        self,
        configs: List[Dict[str, Any]],
        output_dir: str
    ):
        """Create PII leakage comparison chart."""
        # Extract data for the chart
        labels = []
        pii_leakage_rates = []
        canary_leakage_rates = []

        for config in configs:
            config_name = config["config_name"]
            labels.append(config_name)

            # Get PII leakage rate
            pii_leakage = config["privacy_metrics"]["pii_leakage"].get("pii_leakage_rate", 0)
            pii_leakage_rates.append(pii_leakage)

            # Get canary leakage rate
            canary_leakage = config["privacy_metrics"]["canary_leakage"].get("total_leakage_rate", 0)
            canary_leakage_rates.append(canary_leakage)

        # Create the comparison chart
        fig, ax = plt.subplots(figsize=(12, 6))

        x = np.arange(len(labels))
        width = 0.35

        ax.bar(x - width/2, pii_leakage_rates, width, label='PII Leakage Rate')
        ax.bar(x + width/2, canary_leakage_rates, width, label='Canary Leakage Rate')

        ax.set_xlabel('Configuration')
        ax.set_ylabel('Leakage Rate')
        ax.set_title('PII and Canary Leakage Rates by Configuration')
        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=45, ha='right')
        ax.legend()

        # Set y-axis range
        ax.set_ylim(0, max(max(pii_leakage_rates), max(canary_leakage_rates)) + 0.1)

        # Add grid
        ax.grid(True, axis='y', linestyle='--', alpha=0.7)

        # Save the chart
        plt.tight_layout()
        output_path = os.path.join(output_dir, 'pii_leakage_comparison.png')
        plt.savefig(output_path, dpi=300)
        plt.close()

        logger.info(f"Created PII leakage comparison chart: {output_path}")

    def _create_similarity_distribution_chart(
        self,
        configs: List[Dict[str, Any]],
        output_dir: str
    ):
        """Create similarity distribution chart."""
        # Extract data for the chart
        all_similarities = {}

        for config in configs:
            config_name = config["config_name"]

            # Get similarity distribution
            similarities = config["privacy_metrics"]["similarity_distribution"].get("all_similarities", [])
            if similarities:
                all_similarities[config_name] = similarities

        if not all_similarities:
            logger.warning("No similarity data available for distribution chart")
            return

        # Create the distribution chart
        fig, ax = plt.subplots(figsize=(12, 6))

        for config_name, similarities in all_similarities.items():
            sns.kdeplot(similarities, label=config_name, ax=ax)

        ax.set_xlabel('Cosine Similarity')
        ax.set_ylabel('Density')
        ax.set_title('Similarity Distribution by Configuration')
        ax.legend()

        # Set x-axis range
        ax.set_xlim(0, 1)

        # Add grid
        ax.grid(True, linestyle='--', alpha=0.7)

        # Save the chart
        plt.tight_layout()
        output_path = os.path.join(output_dir, 'similarity_distribution.png')
        plt.savefig(output_path, dpi=300)
        plt.close()

        logger.info(f"Created similarity distribution chart: {output_path}")

    def _create_perplexity_comparison(
        self,
        configs: List[Dict[str, Any]],
        output_dir: str
    ):
        """Create perplexity comparison chart."""
        # Extract data for the chart
        labels = []
        perplexities = []

        for config in configs:
            config_name = config["config_name"]

            # Get perplexity
            perplexity = config["utility_metrics"]["perplexity"].get("mean_perplexity")
            if perplexity is not None:
                labels.append(config_name)
                perplexities.append(perplexity)

        if not perplexities:
            logger.warning("No perplexity data available for comparison chart")
            return

        # Create the comparison chart
        fig, ax = plt.subplots(figsize=(12, 6))

        ax.bar(labels, perplexities, alpha=0.7)

        ax.set_xlabel('Configuration')
        ax.set_ylabel('Perplexity (lower is better)')
        ax.set_title('Perplexity by Configuration')
        plt.xticks(rotation=45, ha='right')

        # Add grid
        ax.grid(True, axis='y', linestyle='--', alpha=0.7)

        # Save the chart
        plt.tight_layout()
        output_path = os.path.join(output_dir, 'perplexity_comparison.png')
        plt.savefig(output_path, dpi=300)
        plt.close()

        logger.info(f"Created perplexity comparison chart: {output_path}")

    def _create_output_length_comparison(
        self,
        configs: List[Dict[str, Any]],
        output_dir: str
    ):
        """Create output length comparison chart."""
        # Extract data for the chart
        labels = []
        orig_lengths = []
        gen_lengths = []
        placeholder_rates = []

        for config in configs:
            config_name = config["config_name"]
            labels.append(config_name)

            # Get output statistics
            output_stats = config["utility_metrics"]["output_statistics"]

            # Get word counts
            orig_len = output_stats.get("word_count", {}).get("original_mean", 0)
            gen_len = output_stats.get("word_count", {}).get("generated_mean", 0)
            orig_lengths.append(orig_len)
            gen_lengths.append(gen_len)

            # Get placeholder rate
            placeholder_rate = output_stats.get("placeholder_count", {}).get("texts_with_placeholders_rate", 0)
            placeholder_rates.append(placeholder_rate)

        # Create the comparison chart
        fig, ax1 = plt.subplots(figsize=(12, 6))

        x = np.arange(len(labels))
        width = 0.35

        ax1.bar(x - width/2, orig_lengths, width, label='Original Length', color='tab:blue', alpha=0.7)
        ax1.bar(x + width/2, gen_lengths, width, label='Generated Length', color='tab:orange', alpha=0.7)

        ax1.set_xlabel('Configuration')
        ax1.set_ylabel('Average Word Count')
        ax1.set_title('Text Length and Placeholder Rate by Configuration')
        ax1.set_xticks(x)
        ax1.set_xticklabels(labels, rotation=45, ha='right')
        ax1.legend(loc='upper left')

        # Create second y-axis for placeholder rate
        ax2 = ax1.twinx()
        ax2.plot(x, placeholder_rates, 'r-o', label='Placeholder Rate')
        ax2.set_ylabel('Placeholder Rate')
        ax2.set_ylim(0, max(placeholder_rates) + 0.1)
        ax2.legend(loc='upper right')

        # Add grid
        ax1.grid(True, axis='y', linestyle='--', alpha=0.7)

        # Save the chart
        plt.tight_layout()
        output_path = os.path.join(output_dir, 'output_length_comparison.png')
        plt.savefig(output_path, dpi=300)
        plt.close()

        logger.info(f"Created output length comparison chart: {output_path}")

    def create_summary_report(
        self,
        output_path: Optional[str] = None,
        config_ids: List[str] = None
    ):
        """
        Create a summary report of evaluation results.

        Args:
            output_path: Path to save the report. If None, use default.
            config_ids: List of configuration IDs to include. If None, use all.
        """
        # Use all configurations if none specified
        if config_ids is None:
            config_ids = list(self.results.keys())

        # Skip if no results
        if not config_ids or not self.results:
            logger.warning("No results available for report")
            return

        # Get results for selected configurations
        configs_to_report = []
        for config_id in config_ids:
            if config_id in self.results:
                configs_to_report.append(self.results[config_id])
            else:
                logger.warning(f"Configuration {config_id} not found in results")

        if not configs_to_report:
            logger.warning("No valid configurations found for report")
            return

        # Set output path
        if output_path is None:
            output_path = os.path.join(self.results_dir, 'evaluation_summary.json')

        # Create summary data
        summary = {
            "overview": {
                "num_configurations": len(configs_to_report),
                "configurations": [cfg["config_name"] for cfg in configs_to_report]
            },
            "privacy_metrics": {},
            "utility_metrics": {},
            "configurations": {}
        }

        # Add configuration details
        for config in configs_to_report:
            config_id = config["config_id"]
            config_name = config["config_name"]

            # Extract key metrics
            privacy_metrics = config["privacy_metrics"]
            utility_metrics = config["utility_metrics"]

            pii_leakage_rate = privacy_metrics["pii_leakage"].get("pii_leakage_rate", 0)
            canary_leakage_rate = privacy_metrics["canary_leakage"].get("total_leakage_rate", 0)
            similarity_mean = utility_metrics["semantic_similarity"].get("mean_similarity", 0)
            perplexity = utility_metrics["perplexity"].get("mean_perplexity", 0)

            # Add to summary
            summary["configurations"][config_id] = {
                "name": config_name,
                "description": config["config_description"],
                "key_metrics": {
                    "pii_leakage_rate": pii_leakage_rate,
                    "canary_leakage_rate": canary_leakage_rate,
                    "semantic_similarity": similarity_mean,
                    "perplexity": perplexity
                }
            }

        # Add comparative privacy metrics
        summary["privacy_metrics"] = {
            "pii_leakage_rates": {cfg["config_name"]: cfg["privacy_metrics"]["pii_leakage"].get("pii_leakage_rate", 0) for cfg in configs_to_report},
            "canary_leakage_rates": {cfg["config_name"]: cfg["privacy_metrics"]["canary_leakage"].get("total_leakage_rate", 0) for cfg in configs_to_report},
            "best_privacy_configuration": min([(cfg["config_name"], cfg["privacy_metrics"]["pii_leakage"].get("pii_leakage_rate", 0)) for cfg in configs_to_report], key=lambda x: x[1])[0]
        }

        # Add comparative utility metrics
        summary["utility_metrics"] = {
            "semantic_similarities": {cfg["config_name"]: cfg["utility_metrics"]["semantic_similarity"].get("mean_similarity", 0) for cfg in configs_to_report},
            "perplexities": {cfg["config_name"]: cfg["utility_metrics"]["perplexity"].get("mean_perplexity", 0) for cfg in configs_to_report if "mean_perplexity" in cfg["utility_metrics"]["perplexity"]},
            "best_utility_configuration": max([(cfg["config_name"], cfg["utility_metrics"]["semantic_similarity"].get("mean_similarity", 0)) for cfg in configs_to_report], key=lambda x: x[1])[0]
        }

        # Calculate privacy-utility balance
        privacy_utility_scores = []
        for cfg in configs_to_report:
            # Privacy score (0-1, higher is better)
            privacy_score = 1.0 - cfg["privacy_metrics"]["pii_leakage"].get("pii_leakage_rate", 0)

            # Utility score (0-1, higher is better)
            utility_score = cfg["utility_metrics"]["semantic_similarity"].get("mean_similarity", 0)

            # Combined score (simple average)
            combined_score = (privacy_score + utility_score) / 2

            privacy_utility_scores.append((cfg["config_name"], combined_score))

        # Find best balanced configuration
        best_balanced = max(privacy_utility_scores, key=lambda x: x[1])
        summary["best_balanced_configuration"] = {
            "name": best_balanced[0],
            "score": best_balanced[1]
        }

        # Save summary report
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)

        logger.info(f"Saved evaluation summary report to {output_path}")

        return summary



"""
Script to run the evaluation pipeline on different filter configurations.
"""


from pathlib import Path
from typing import List, Dict, Any

import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('evaluation.log')
    ]
)
logger = logging.getLogger(__name__)

def load_dataset(dataset_type, num_samples, seed, dataset_path=None, text_column="text"):
    """Load the dataset for evaluation."""
    # Load dataset
    if dataset_type == "enron":
        logger.info(f"Loading Enron dataset with {num_samples} samples...")
        data = load_enron_sample(num_samples=num_samples, seed=seed)
        text_column = "body"
    elif dataset_type == "stackoverflow":
        logger.info(f"Loading Stack Overflow dataset with {num_samples} samples...")
        data = load_stackoverflow_sample(num_samples=num_samples, seed=seed)
        text_column = "body"
    else:  # Custom dataset
        if not dataset_path:
            raise ValueError("dataset_path is required for custom dataset")

        logger.info(f"Loading custom dataset from {dataset_path}...")

        file_ext = Path(dataset_path).suffix.lower()
        if file_ext == ".csv":
            data = pd.read_csv(dataset_path)
        elif file_ext == ".json":
            data = pd.read_json(dataset_path)
        elif file_ext in [".pkl", ".pickle"]:
            data = pd.read_pickle(dataset_path)
        else:
            raise ValueError(f"Unsupported file extension: {file_ext}")

    return data, text_column

def inject_sensitive_data(data, text_column, output_dir, seed, names_file=None, pii_rate=0.3, canary_rate=0.1):
    """Inject sensitive data into the dataset."""
    # Set up data injector
    logger.info("Setting up data injector...")
    injector = SensitiveDataInjector(
        seed=seed,
        name_list_path=names_file,
        canary_phrases=None,  # Use default canary phrases
        pii_injection_rate=pii_rate,
        canary_injection_rate=canary_rate
    )

    # Inject sensitive data
    logger.info("Injecting sensitive data...")
    processed_data, stats = injector.process_dataset(
        input_data=data,
        text_column=text_column,
        output_path=os.path.join(output_dir, "processed_dataset.json"),
        output_format="json"
    )

    logger.info(f"Injected PII into {stats['pii_injections']} samples ({stats['pii_injection_rate']:.2%})")
    logger.info(f"Injected canary phrases into {stats['canary_injections']} samples ({stats['canary_injection_rate']:.2%})")

    return processed_data

def create_configurations():
    """Create filter configurations for evaluation."""
    configs = {
        "baseline": {
            "name": "Baseline (No Filters)",
            "description": "No privacy filters applied",
            "pre_filter": None,
            "in_generation": {
                "temperature": 0.8,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.0,
                "blocked_words": []
            },
            "post_filter": None
        },
        "pre_only": {
            "name": "Pre-Generation Filter Only",
            "description": "Only pre-generation filtering applied",
            "pre_filter": {
                "use_token_level": True,
                "type_specific_placeholders": True
            },
            "in_generation": {
                "temperature": 0.8,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.0,
                "blocked_words": []
            },
            "post_filter": None
        },
        "post_only": {
            "name": "Post-Generation Filter Only",
            "description": "Only post-generation filtering applied",
            "pre_filter": None,
            "in_generation": {
                "temperature": 0.8,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.0,
                "blocked_words": []
            },
            "post_filter": {
                "base_threshold": 0.92,
                "reapply_pii_detection": True,
                "use_token_level_redaction": True
            }
        },
        "pre_post": {
            "name": "Pre+Post Filters",
            "description": "Pre-generation and post-generation filters applied",
            "pre_filter": {
                "use_token_level": True,
                "type_specific_placeholders": True
            },
            "in_generation": {
                "temperature": 0.8,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.0,
                "blocked_words": []
            },
            "post_filter": {
                "base_threshold": 0.92,
                "reapply_pii_detection": True,
                "use_token_level_redaction": True
            }
        },
        "all_filters": {
            "name": "All Filters",
            "description": "All privacy filters applied (pre, in, post)",
            "pre_filter": {
                "use_token_level": True,
                "type_specific_placeholders": True
            },
            "in_generation": {
                "temperature": 0.8,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.2,
                "blocked_words": ["password", "social security", "credit card", "address", "phone number", "email"]
            },
            "post_filter": {
                "base_threshold": 0.92,
                "reapply_pii_detection": True,
                "use_token_level_redaction": True
            }
        },
        "all_strict": {
            "name": "All Filters (Strict)",
            "description": "All privacy filters with stricter settings",
            "pre_filter": {
                "use_token_level": True,
                "type_specific_placeholders": True
            },
            "in_generation": {
                "temperature": 0.7,
                "top_k": 40,
                "top_p": 0.9,
                "repetition_penalty": 1.3,
                "blocked_words": ["password", "social security", "credit card", "address", "phone number", "email"]
            },
            "post_filter": {
                "base_threshold": 0.85,  # Lower threshold = more filtering
                "reapply_pii_detection": True,
                "use_token_level_redaction": True
            }
        },
        "adaptive": {
            "name": "Adaptive Thresholds",
            "description": "All filters with content-dependent adaptive thresholds",
            "pre_filter": {
                "use_token_level": True,
                "type_specific_placeholders": True
            },
            "in_generation": {
                "temperature": 0.8,
                "top_k": 50,
                "top_p": 0.95,
                "repetition_penalty": 1.2,
                "blocked_words": ["password", "social security", "credit card", "address", "phone number", "email"]
            },
            "post_filter": {
                "base_threshold": 0.92,
                "adaptive_thresholds": {
                    "technical": 0.95,
                    "general": 0.9,
                    "personal": 0.85
                },
                "reapply_pii_detection": True,
                "use_token_level_redaction": True
            }
        }
    }

    return configs

def setup_filter_pipeline(config, sensitive_items=None):
    """Set up filter pipeline based on configuration."""
    # Set up pre-generation filter
    pre_filter = None
    if config["pre_filter"] is not None:
        pre_filter = PreGenerationFilter(**config["pre_filter"])

    # Set up post-generation filter
    post_filter = None
    if config["post_filter"] is not None:
        post_config = dict(config["post_filter"])
        if sensitive_items:
            post_config["sensitive_items"] = sensitive_items
        post_filter = PostGenerationFilter(**post_config)

    return pre_filter, post_filter

def generate_text(
    config_id,
    config,
    data,
    text_column,
    hf_token='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',
    max_tokens=256,
    num_samples=None,
):
    """Generate text using the specified configuration."""
    logger.info(f"Generating text with configuration: {config['name']} (ID: {config_id})")

    # Set up filters
    pre_filter, post_filter = setup_filter_pipeline(
        config,
        sensitive_items=data.loc[data["has_pii"] == True, text_column].tolist()[:10]
    )

    # Set up LLM
    logger.info("Setting up LLM...")

    in_gen_config = dict(config["in_generation"])
    generator = GemmaTextGenerator(
        model_name="google/gemma-2b-it",
        max_new_tokens=max_tokens,
        **in_gen_config,
        hf_auth_token=hf_token or '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'
    )

    # Process samples
    if num_samples is None:
        num_samples = len(data)
    else:
        num_samples = min(num_samples, len(data))

    samples = data.iloc[:num_samples]
    generated_texts = []
    pre_processed_texts = []

    for i, (_, row) in enumerate(samples.iterrows()):
        original_text = row[text_column]

        # Apply pre-generation filter
        if pre_filter:
            pre_processed, _ = pre_filter.process(original_text)
            prompt = pre_processed
        else:
            prompt = original_text
            pre_processed = original_text

        pre_processed_texts.append(pre_processed)

        # Generate text
        try:
            # Only show progress periodically
            if i % 5 == 0:
                logger.info(f"Generating text for sample {i+1}/{num_samples}...")

            generated_text, _ = generator.generate(
                prompt=f"Based on the following text, generate a variation that preserves the meaning but uses different wording:\n\n{prompt}",
                max_new_tokens=max_tokens
            )

            # Apply post-generation filter
            if post_filter:
                filtered_text, _ = post_filter.process(generated_text)
                generated_text = filtered_text
        except Exception as e:
            logger.error(f"Error generating text for sample {i}: {str(e)}")
            generated_text = "[GENERATION_ERROR]"

        generated_texts.append(generated_text)

    # Create DataFrame with results
    results_df = samples.copy()
    results_df["pre_processed_text"] = pre_processed_texts
    results_df["generated_text"] = generated_texts

    return results_df

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('full_evaluation.log')
    ]
)
logger = logging.getLogger(__name__)

# Set up output directory
os.makedirs('results', exist_ok=True)

data, text_column = load_dataset(
        dataset_type="enron",
        num_samples=100,
        seed=42,
        dataset_path=None,
        text_column='text'
    )

processed_data = inject_sensitive_data(
        data=data,
        text_column=text_column,
        output_dir='results',
        seed=42,
        names_file=None,
        pii_rate=0.3,
        canary_rate=0.1
    )

all_configs = create_configurations()

configs_to_run = None
if configs_to_run:
    configs = {k: v for k, v in all_configs.items() if k in configs_to_run}
    if not configs:
        logger.error(f"None of the specified configurations found: {configs_to_run}")
        logger.info(f"Available configurations: {list(all_configs.keys())}")
        print("done")
else:
    configs = all_configs

logger.info("Setting up evaluators...")
privacy_evaluator = PrivacyEvaluator(device=None)
utility_evaluator = UtilityEvaluator(device=None)
evaluator = PrivacyFrameworkEvaluator(
        privacy_evaluator=privacy_evaluator,
        utility_evaluator=utility_evaluator,
        results_dir='results'
  )
for config_id, config in configs.items():
        evaluator.add_configuration(
            config_id=config_id,
            config_name=config["name"],
            config_description=config["description"],
            configuration=config
        )



for config_id, config in configs.items():
        # Generate text with this configuration
        logger.info(f"Processing configuration: {config['name']} (ID: {config_id})")

        generation_results = generate_text(
            config_id=config_id,
            config=config,
            data=processed_data,
            text_column=text_column,
            hf_token='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',
            max_tokens=256,
            num_samples=100,
        )

        # Save generation results
        output_path = os.path.join('results', f"{config_id}_generated.json")
        generation_results.to_json(output_path, orient="records", indent=2)
        logger.info(f"Saved generation results to {output_path}")

for config_id, config in configs.items():
        # Evaluate configuration
        logger.info(f"Evaluating configuration: {config['name']} (ID: {config_id})")
        evaluation_results = evaluator.evaluate_configuration(
            config_id=config_id,
            original_data=processed_data,
            generated_data=generation_results,
            text_column="generated_text",
            pii_metadata_column="pii_metadata",
            canary_metadata_column="canary_metadata"
        )

logger.info("Creating comparison charts...")
evaluator.create_comparison_charts()

    # Create summary report

logger.info("Creating summary report...")
summary = evaluator.create_summary_report()

logger.info(f"Evaluation complete. Results saved to {'results'}")

    # Print summary of best configurations
print("\n----- Evaluation Summary -----")
print(f"Best privacy configuration: {summary['privacy_metrics']['best_privacy_configuration']}")
print(f"Best utility configuration: {summary['utility_metrics']['best_utility_configuration']}")
print(f"Best balanced configuration: {summary['best_balanced_configuration']['name']} (Score: {summary['best_balanced_configuration']['score']:.4f})")

print(summary)

def run_evaluation(
    dataset_type="enron",
    dataset_path=None,
    text_column="text",
    num_samples=100,
    pii_rate=0.3,
    canary_rate=0.1,
    names_file=None,
    hf_token='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',
    max_tokens=256,
    device=None,
    configs_to_run=None,
    output_dir="results",
    seed=42
):
    """Run the evaluation pipeline with the given parameters."""



    # Load dataset


    # Inject sensitive data


    # Get configurations


    # Filter to selected configurations if specified


    # Set up evaluator


    # Add configurations to evaluator


    # Get HuggingFace token from environment if not provided
    if not hf_token:
        hf_token = os.environ.get("HF_TOKEN")

    # Generate and evaluate each configuration
    for config_id, config in configs.items():
        # Generate text with this configuration
        logger.info(f"Processing configuration: {config['name']} (ID: {config_id})")

        generation_results = generate_text(
            config_id=config_id,
            config=config,
            data=processed_data,
            text_column=text_column,
            hf_token=hf_token,
            max_tokens=max_tokens,
            num_samples=num_samples,
            device=device
        )

        # Save generation results
        output_path = os.path.join(output_dir, f"{config_id}_generated.json")
        generation_results.to_json(output_path, orient="records", indent=2)
        logger.info(f"Saved generation results to {output_path}")

        # Evaluate configuration
        logger.info(f"Evaluating configuration: {config['name']} (ID: {config_id})")
        evaluation_results = evaluator.evaluate_configuration(
            config_id=config_id,
            original_data=processed_data,
            generated_data=generation_results,
            text_column="generated",
            pii_metadata_column="pii_metadata",
            canary_metadata_column="canary_metadata"
        )

    # Create comparison charts
    logger.info("Creating comparison charts...")
    evaluator.create_comparison_charts()

    # Create summary report
    logger.info("Creating summary report...")
    summary = evaluator.create_summary_report()

    logger.info(f"Evaluation complete. Results saved to {output_dir}")

    # Print summary of best configurations
    print("\n----- Evaluation Summary -----")
    print(f"Best privacy configuration: {summary['privacy_metrics']['best_privacy_configuration']}")
    print(f"Best utility configuration: {summary['utility_metrics']['best_utility_configuration']}")
    print(f"Best balanced configuration: {summary['best_balanced_configuration']['name']} (Score: {summary['best_balanced_configuration']['score']:.4f})")

    return summary

"""
Script to run a full evaluation of the synthetic data privacy framework.
"""

import os
import logging


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('full_evaluation.log')
    ]
)
logger = logging.getLogger(__name__)

def run_full_evaluation(
    dataset_type="enron",
    dataset_path=None,
    text_column="text",
    num_samples=100,
    pii_rate=0.3,
    canary_rate=0.1,
    hf_token='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',
    max_tokens=256,
    device=None,
    configs_to_run=None,
    output_dir="results",
    seed=42,
    prepare_only=False
):
    """
    Run a full evaluation of the synthetic data privacy framework.

    Parameters:
    -----------
    dataset_type : str
        Type of dataset to use. Options: "enron", "stackoverflow", "custom".
    dataset_path : str, optional
        Path to custom dataset file. Required if dataset_type is "custom".
    text_column : str, optional
        Column containing text in custom dataset.
    num_samples : int, optional
        Number of samples to use for evaluation.
    pii_rate : float, optional
        PII injection rate (0-1).
    canary_rate : float, optional
        Canary injection rate (0-1).
    hf_token : str, optional
        HuggingFace API token. Will use environment variable if not provided.
    max_tokens : int, optional
        Maximum tokens to generate per sample.
    device : str, optional
        Device to use for models. Options: "cuda", "cpu", "mps".
    configs_to_run : list, optional
        Specific configurations to evaluate. If None, all configurations will be evaluated.
        Options: "baseline", "pre_only", "post_only", "pre_post", "all_filters", "all_strict", "adaptive".
    output_dir : str, optional
        Directory to store results.
    seed : int, optional
        Random seed.
    prepare_only : bool, optional
        Only prepare the dataset, don't run evaluation.

    Returns:
    --------
    dict or None
        Evaluation summary if prepare_only is False, None otherwise.
    """
    # Set up output directory
    os.makedirs(output_dir, exist_ok=True)

    # Prepare dataset
    logger.info(f"Preparing dataset: {dataset_type}")

    # If we're just preparing the dataset, don't continue
    if prepare_only:
        logger.info("Dataset preparation completed. Skipping evaluation as requested.")
        return None

    # Run evaluation
    logger.info("Running evaluation with the following parameters:")
    params = {
        "dataset_type": dataset_type,
        "dataset_path": dataset_path,
        "text_column": text_column,
        "num_samples": num_samples,
        "pii_rate": pii_rate,
        "canary_rate": canary_rate,
        "hf_token": hf_token,
        "max_tokens": max_tokens,
        "device": device,
        "configs_to_run": configs_to_run,
        "output_dir": output_dir,
        "seed": seed
    }

    logger.info(str(params))

    evaluation_results = run_evaluation(**params)

    logger.info("Full evaluation completed!")

    return evaluation_results

def main():
    """
    Main function to run a full evaluation with default parameters.

    To customize parameters, modify the parameters in this function
    or call run_full_evaluation directly with your desired parameters.
    """
    # Default parameters
    params = {
        "dataset_type": "enron",
        "dataset_path": None,
        "text_column": "text",
        "num_samples": 100,
        "pii_rate": 0.3,
        "canary_rate": 0.1,
        "hf_token": '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',
        "max_tokens": 256,
        "device": None,
        "configs_to_run": None,  # Run all configs
        "output_dir": "results",
        "seed": 42,
        "prepare_only": False
    }

    # Run full evaluation with default parameters
    run_full_evaluation(**params)


if __name__ == "__main__":
    main()

